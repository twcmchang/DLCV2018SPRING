{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "class GAN():\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def load(self, npy_path):\n",
    "        self.data_dict = np.load(npy_path, encoding='latin1').item()\n",
    "        print(\"Load %s as self.data_dict\" % npy_path)\n",
    "\n",
    "    def build(self, n_dim=512, shape=(64,64,3)):\n",
    "        \"\"\"\n",
    "        load pre-trained weights from path\n",
    "        :param vgg16_npy_path: file path of vgg16 pre-trained weights\n",
    "        \"\"\"\n",
    "        # input information\n",
    "        self.H, self.W, self.C = shape\n",
    "        self.n_dim = n_dim\n",
    "        \n",
    "        # parameter dictionary\n",
    "        self.para_dict = dict()\n",
    "        self.data_dict = dict()\n",
    "        self.net_shape = dict()\n",
    "\n",
    "        # input placeholder\n",
    "        self.x = tf.placeholder(tf.float32, [None, self.H, self.W, self.C])\n",
    "        self.is_train = tf.placeholder(tf.bool)\n",
    "        self.random_sample = tf.placeholder(tf.float32, [None, self.n_dim])\n",
    "        \n",
    "        # normalize inputs\n",
    "        assert self.x.get_shape().as_list()[1:] == [self.H, self.W, self.C]\n",
    "        \n",
    "\n",
    "        with tf.variable_scope(\"Generator\",reuse=tf.AUTO_REUSE):\n",
    "            self.G_image = self.generator(self.random_sample)\n",
    "            \n",
    "        with tf.variable_scope(\"Discriminator\", reuse=tf.AUTO_REUSE):\n",
    "            self.D_real = self.dense_layer(self.discriminator(self.x), n_hidden=1, name='D_output')\n",
    "            self.D_fake = self.dense_layer(self.discriminator(self.G_image), n_hidden=1, name='D_output')\n",
    "        \n",
    "        real_equality = tf.equal(tf.cast(tf.sigmoid(self.D_real) > 0.5, tf.float32), tf.ones(shape=tf.shape(self.D_real)))\n",
    "        self.D_real_accu = tf.reduce_mean(tf.cast(real_equality, tf.float32))\n",
    "        \n",
    "        fake_equality = tf.equal(tf.cast(tf.sigmoid(self.D_fake) > 0.5, tf.float32), tf.zeros(shape=tf.shape(self.D_fake)))\n",
    "        self.D_fake_accu = tf.reduce_mean(tf.cast(fake_equality, tf.float32))\n",
    "        \n",
    "        # loss of discriminator\n",
    "        self.D_real_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=self.D_real,\n",
    "                                                                                  labels=tf.ones(shape=tf.shape(self.D_real))))\n",
    "        self.D_fake_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=self.D_fake,\n",
    "                                                                                  labels=tf.zeros(shape=tf.shape(self.D_fake))))\n",
    "        \n",
    "        self.D_loss = self.D_real_loss + self.D_fake_loss\n",
    "        \n",
    "        # loss of generator\n",
    "        self.G_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=self.D_fake,labels=tf.ones(shape=tf.shape(self.D_real))))\n",
    "        \n",
    "        print(\"GAN graph built.\")\n",
    "        \n",
    "    def discriminator(self, input_image):\n",
    "        conv1 = self.conv_bn_layer(input_image, shape=(4,4,3,32), stride=2, name=\"conv1\")\n",
    "        conv2 = self.conv_bn_layer(conv1 , shape=(4,4,32,64), stride=2, name=\"conv2\")\n",
    "        conv3 = self.conv_bn_layer(conv2 , shape=(4,4,64,128), stride=2, name=\"conv3\")\n",
    "        conv4 = self.conv_bn_layer(conv3 , shape=(4,4,128,256), stride=2, name=\"conv4\")\n",
    "        flatten = self.flatten_layer(conv4, name='flatten')\n",
    "        #output = self.dense_layer(flatten, n_hidden=1, name='D_output')\n",
    "        return flatten\n",
    "        \n",
    "    def generator(self, sample_input):\n",
    "        deconv_fc1 = self.dense_layer(sample_input, n_hidden=4096, name='deconv_fc1')\n",
    "        deconv_input = tf.reshape(deconv_fc1, shape=[-1, 4, 4, 256])\n",
    "        batch_size = tf.shape(sample_input)[0]\n",
    "        deconv1 = self.trans_conv_layer(bottom=deconv_input, shape=(4,4,128,256),\n",
    "                                        output_shape=[batch_size, 8, 8, 128], stride=2, name='deconv1')\n",
    "        deconv2 = self.trans_conv_layer(bottom=deconv1, shape=(4,4,64,128),\n",
    "                                        output_shape=[batch_size, 16, 16, 64], stride=2, name='deconv2')\n",
    "        deconv3 = self.trans_conv_layer(bottom=deconv2, shape=(4,4,32,64),\n",
    "                                        output_shape=[batch_size, 32, 32, 32], stride=2, name='deconv3')\n",
    "        output = self.trans_conv_layer(bottom=deconv3, shape=(4,4,3,32),\n",
    "                                        output_shape=[batch_size, self.H, self.W, self.C], activation='tanh', stride=2, name='deconv_output')\n",
    "        return (output/2) + 0.5\n",
    "\n",
    "    def dense_layer(self, bottom, n_hidden=None, name=None):\n",
    "        bottom_shape = bottom.get_shape().as_list()\n",
    "        if n_hidden is not None:\n",
    "            W = self.get_weights(shape=(bottom_shape[1], n_hidden), name=name)\n",
    "            b = self.get_bias(shape=n_hidden, name=name)\n",
    "        elif name in self.data_dict.keys():\n",
    "            W = self.get_weights(name=name)\n",
    "            b = self.get_bias(name=name)\n",
    "        else:\n",
    "            print(\"Neither give a shape nor lack a pre-trained layer called %s\" % name)\n",
    "        self.para_dict[name] = [W, b]\n",
    "        fc = tf.nn.bias_add(tf.matmul(bottom, W), b)\n",
    "        self.net_shape[name] = fc.get_shape().as_list()\n",
    "        return fc\n",
    "\n",
    "    def flatten_layer(self, bottom, name):\n",
    "        shape = bottom.get_shape().as_list()\n",
    "        dim = 1\n",
    "        for d in shape[1:]:\n",
    "            dim *= d\n",
    "        flatten = tf.reshape(bottom, [-1, dim])\n",
    "        self.net_shape[name] = flatten.get_shape().as_list()\n",
    "        return flatten\n",
    "\n",
    "    def avg_pool_layer(self, bottom, name):\n",
    "        pool = tf.nn.avg_pool(bottom, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name=name)\n",
    "        self.net_shape[name] = pool.get_shape().as_list()\n",
    "        return pool\n",
    "\n",
    "    def max_pool_layer(self, bottom, name):\n",
    "        pool = tf.nn.max_pool(bottom, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name=name)\n",
    "        self.net_shape[name] = pool.get_shape().as_list()\n",
    "        return pool\n",
    "    \n",
    "    def dropout(self, bottom, keep_prob):\n",
    "        if self.is_train == True:\n",
    "            return tf.nn.dropout(bottom, keep_prob=keep_prob)\n",
    "        else:\n",
    "            return bottom\n",
    "\n",
    "    def trans_conv_layer(self, bottom, output_shape, stride, activation='relu', name=None, shape=None):\n",
    "        if shape is not None:\n",
    "            conv_filter, gamma, beta, bn_mean, bn_variance = self.get_conv_filter(shape=shape, name=name)\n",
    "            conv_bias = self.get_bias(shape=shape[2], name=name)\n",
    "        elif name in self.data_dict.keys():\n",
    "            conv_filter, gamma, beta, bn_mean, bn_variance = self.get_conv_filter(name=name)\n",
    "            conv_bias = self.get_bias(name=name)\n",
    "        else:\n",
    "            print(\"Neither give a shape nor lack a pre-trained layer called %s\" % name)\n",
    "\n",
    "        self.para_dict[name] = [conv_filter, conv_bias]\n",
    "        self.para_dict[name+\"_gamma\"] = gamma\n",
    "        self.para_dict[name+\"_beta\"] = beta\n",
    "        self.para_dict[name+\"_bn_mean\"] = bn_mean\n",
    "        self.para_dict[name+\"_bn_variance\"] = bn_variance\n",
    "\n",
    "        conv = tf.nn.conv2d_transpose(bottom, conv_filter, output_shape, strides=[1, stride, stride, 1], padding=\"SAME\")\n",
    "        conv = tf.nn.bias_add(conv, conv_bias)\n",
    "        \n",
    "        from tensorflow.python.training.moving_averages import assign_moving_average\n",
    "        def mean_var_with_update():\n",
    "            mean, variance = tf.nn.moments(conv, [0,1,2], name='moments')\n",
    "            with tf.control_dependencies([assign_moving_average(bn_mean, mean, 0.99),\n",
    "                                            assign_moving_average(bn_variance, variance, 0.99)]):\n",
    "                return tf.identity(mean), tf.identity(variance)\n",
    "\n",
    "        mean, variance = tf.cond(self.is_train, mean_var_with_update, lambda:(bn_mean, bn_variance))\n",
    "        conv = tf.nn.batch_normalization(conv, mean, variance, beta, gamma, 1e-05)\n",
    "        self.net_shape[name] = conv.get_shape().as_list()\n",
    "\n",
    "        if activation=='tanh':\n",
    "            print('tanh')\n",
    "            tanh = tf.nn.tanh(conv)\n",
    "            return tanh\n",
    "        else:\n",
    "            relu = tf.nn.leaky_relu(conv)\n",
    "            return relu\n",
    "\n",
    "    def conv_bn_layer(self, bottom, stride=1, activation='lrelu', name=None, shape=None):\n",
    "        if shape is not None:\n",
    "            conv_filter, gamma, beta, bn_mean, bn_variance = self.get_conv_filter(shape=shape, name=name)\n",
    "            conv_bias = self.get_bias(shape=shape[3], name=name)\n",
    "        elif name in self.data_dict.keys():\n",
    "            conv_filter, gamma, beta, bn_mean, bn_variance = self.get_conv_filter(name=name)\n",
    "            conv_bias = self.get_bias(name=name)\n",
    "        else:\n",
    "            print(\"Neither give a shape nor lack a pre-trained layer called %s\" % name)\n",
    "\n",
    "        self.para_dict[name] = [conv_filter, conv_bias]\n",
    "        self.para_dict[name+\"_gamma\"] = gamma\n",
    "        self.para_dict[name+\"_beta\"] = beta\n",
    "        self.para_dict[name+\"_bn_mean\"] = bn_mean\n",
    "        self.para_dict[name+\"_bn_variance\"] = bn_variance\n",
    "\n",
    "        conv = tf.nn.conv2d(bottom, conv_filter, [1, stride, stride, 1], padding='SAME')\n",
    "        conv = tf.nn.bias_add(conv, conv_bias)\n",
    "\n",
    "        from tensorflow.python.training.moving_averages import assign_moving_average\n",
    "        def mean_var_with_update():\n",
    "            mean, variance = tf.nn.moments(conv, [0,1,2], name='moments')\n",
    "            with tf.control_dependencies([assign_moving_average(bn_mean, mean, 0.99),\n",
    "                                            assign_moving_average(bn_variance, variance, 0.99)]):\n",
    "                return tf.identity(mean), tf.identity(variance)\n",
    "\n",
    "        mean, variance = tf.cond(self.is_train, mean_var_with_update, lambda:(bn_mean, bn_variance))\n",
    "\n",
    "        conv = tf.nn.batch_normalization(conv, mean, variance, beta, gamma, 1e-05)\n",
    "        self.net_shape[name] = conv.get_shape().as_list()\n",
    "\n",
    "        if activation=='tanh':\n",
    "            tanh = tf.nn.tanh(conv)\n",
    "            return tanh\n",
    "        else:\n",
    "            relu = tf.nn.leaky_relu(conv)\n",
    "            return relu\n",
    "\n",
    "    def get_conv_filter(self, shape=None, name=None, with_bn=True):\n",
    "        if shape is not None:\n",
    "            conv_filter = tf.get_variable(shape=shape, initializer=tf.truncated_normal_initializer(mean=0, stddev=0.1), name=name+\"_W\", dtype=tf.float32)\n",
    "        elif name in self.data_dict.keys():\n",
    "            conv_filter = tf.get_variable(initializer=self.data_dict[name][0], name=name+\"_W\")\n",
    "        else:\n",
    "            print(\"Neither give a shape nor lack a pre-trained layer called %s\" % name)\n",
    "            return None\n",
    "\n",
    "        if with_bn:\n",
    "            if 'deconv' in name:\n",
    "                H,W,O,C = conv_filter.get_shape().as_list()\n",
    "            else:\n",
    "                H,W,C,O = conv_filter.get_shape().as_list()\n",
    "\n",
    "            if name+\"_gamma\" in self.data_dict.keys(): \n",
    "                gamma = tf.get_variable(initializer=self.data_dict[name+\"_gamma\"], name=name+\"_gamma\")\n",
    "            else:\n",
    "                gamma = tf.get_variable(shape=(O,), initializer=tf.ones_initializer(), name=name+\"_gamma\")\n",
    "\n",
    "            if name+\"_beta\" in self.data_dict.keys(): \n",
    "                beta = tf.get_variable(initializer=self.data_dict[name+\"_beta\"], name=name+\"_beta\")\n",
    "            else:\n",
    "                beta = tf.get_variable(shape=(O,), initializer=tf.zeros_initializer(), name=name+'_beta')\n",
    "\n",
    "            if name+\"_bn_mean\" in self.data_dict.keys(): \n",
    "                bn_mean = tf.get_variable(initializer=self.data_dict[name+\"_bn_mean\"], name=name+\"_bn_mean\")\n",
    "            else:\n",
    "                bn_mean = tf.get_variable(shape=(O,), initializer=tf.zeros_initializer(), name=name+'_bn_mean')\n",
    "\n",
    "            if name+\"_bn_variance\" in self.data_dict.keys(): \n",
    "                bn_variance = tf.get_variable(initializer=self.data_dict[name+\"_bn_variance\"], name=name+\"_bn_variance\")\n",
    "            else:\n",
    "                bn_variance = tf.get_variable(shape=(O,), initializer=tf.ones_initializer(), name=name+'_bn_variance')\n",
    "            return conv_filter, gamma, beta, bn_mean, bn_variance\n",
    "        else:\n",
    "            return conv_filter\n",
    "    \n",
    "    def get_weights(self, shape=None, name=None):\n",
    "        if shape is not None:\n",
    "            return tf.get_variable(shape=shape, initializer=tf.truncated_normal_initializer(mean=0, stddev=0.1), name=name+\"_W\", dtype=tf.float32)\n",
    "        elif name in self.data_dict.keys(): \n",
    "            return tf.get_variable(initializer=self.data_dict[name][0], name=name+\"_W\")\n",
    "        else:\n",
    "            print(\"(get_weight) neither give a shape nor lack a pre-trained layer called %s\" % name)\n",
    "            return None\n",
    "            \n",
    "    def get_bias(self, shape=None, name=None):\n",
    "        if shape is not None:\n",
    "            return tf.get_variable(shape=shape, initializer=tf.truncated_normal_initializer(mean=0, stddev=0.1), name=name+\"_b\", dtype=tf.float32)\n",
    "        elif name in self.data_dict.keys(): \n",
    "            return tf.get_variable(initializer=self.data_dict[name][1], name=name+\"_b\")\n",
    "        else:\n",
    "            print(\"(get_bias) neither give a shape nor lack a pre-trained layer called %s\" % name)\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ACGAN(GAN):\n",
    "    def __init_(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def build(self, aux_dim=1, n_dim=512, shape=(64,64,3)):\n",
    "        \"\"\"\n",
    "        load pre-trained weights from path\n",
    "        :param vgg16_npy_path: file path of vgg16 pre-trained weights\n",
    "        \"\"\"\n",
    "        # input information\n",
    "        self.H, self.W, self.C = shape\n",
    "        self.n_dim = n_dim\n",
    "        self.aux_dim = aux_dim\n",
    "        \n",
    "        # parameter dictionary\n",
    "        self.para_dict = dict()\n",
    "        self.data_dict = dict()\n",
    "        self.net_shape = dict()\n",
    "\n",
    "        # input placeholder\n",
    "        self.x = tf.placeholder(tf.float32, [None, self.H, self.W, self.C])\n",
    "        self.random_sample = tf.placeholder(tf.float32, [None, self.n_dim])\n",
    "        self.aux_labels = tf.placeholder(tf.float32, shape=[None, self.aux_dim])\n",
    "        self.is_train = tf.placeholder(tf.bool)\n",
    "        \n",
    "        # normalize inputs\n",
    "        assert self.x.get_shape().as_list()[1:] == [self.H, self.W, self.C]\n",
    "        \n",
    "\n",
    "        with tf.variable_scope(\"Generator\",reuse=tf.AUTO_REUSE):\n",
    "            self.G_image = self.generator(self.random_sample, self.aux_labels)\n",
    "            \n",
    "        with tf.variable_scope(\"Discriminator\", reuse=tf.AUTO_REUSE):\n",
    "            self.D_real = self.dense_layer(self.discriminator(self.x), n_hidden=1, name='D_output')\n",
    "            self.D_fake = self.dense_layer(self.discriminator(self.G_image), n_hidden=1, name='D_output')\n",
    "            \n",
    "            self.D_aux_real = self.dense_layer(self.discriminator(self.x), n_hidden=self.aux_dim, name='aux_output')\n",
    "            self.D_aux_fake = self.dense_layer(self.discriminator(self.G_image), n_hidden=self.aux_dim, name='aux_output')\n",
    "        \n",
    "        real_equality = tf.equal(tf.cast(tf.sigmoid(self.D_real) > 0.5, tf.float32), tf.ones(shape=tf.shape(self.D_real)))\n",
    "        self.D_real_accu = tf.reduce_mean(tf.cast(real_equality, tf.float32))\n",
    "        \n",
    "        fake_equality = tf.equal(tf.cast(tf.sigmoid(self.D_fake) > 0.5, tf.float32), tf.zeros(shape=tf.shape(self.D_fake)))\n",
    "        self.D_fake_accu = tf.reduce_mean(tf.cast(fake_equality, tf.float32))\n",
    "        \n",
    "        # loss of discriminator\n",
    "        self.D_real_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=self.D_real,\n",
    "                                                                                  labels=tf.ones(shape=tf.shape(self.D_real))))\n",
    "        self.D_fake_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=self.D_fake,\n",
    "                                                                                  labels=tf.zeros(shape=tf.shape(self.D_fake))))\n",
    "        self.D_loss = self.D_real_loss + self.D_fake_loss\n",
    "        \n",
    "        # loss of generator\n",
    "        self.G_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=self.D_fake,labels=tf.ones(shape=tf.shape(self.D_real))))\n",
    "        \n",
    "\n",
    "        self.D_aux_real_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=self.D_aux_real,\n",
    "                                                                                    labels=self.aux_labels))\n",
    "        self.D_aux_fake_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=self.D_aux_fake,\n",
    "                                                                                    labels=self.aux_labels))\n",
    "        self.D_aux_total_loss = self.D_aux_real_loss + self.D_aux_fake_loss\n",
    "        \n",
    "        self.D_loss = self.D_loss + self.D_aux_total_loss\n",
    "        self.G_loss = self.G_loss + self.D_aux_total_loss\n",
    "        print(\"GAN graph built.\")\n",
    "        \n",
    "    def discriminator(self, input_image):\n",
    "        conv1 = self.conv_bn_layer(input_image, shape=(4,4,3,32), stride=2, name=\"conv1\")\n",
    "        conv2 = self.conv_bn_layer(conv1 , shape=(4,4,32,64), stride=2, name=\"conv2\")\n",
    "        conv3 = self.conv_bn_layer(conv2 , shape=(4,4,64,128), stride=2, name=\"conv3\")\n",
    "        conv4 = self.conv_bn_layer(conv3 , shape=(4,4,128,256), stride=2, name=\"conv4\")\n",
    "        flatten = self.flatten_layer(conv4, name='flatten')\n",
    "        #output = self.dense_layer(flatten, n_hidden=1, name='D_output')\n",
    "        return flatten\n",
    "    \n",
    "    def generator(self, sample_input, sample_aux_input):\n",
    "        sample_input = tf.concat([sample_input, sample_aux_input], axis=1)\n",
    "        deconv_fc1 = self.dense_layer(sample_input, n_hidden=4096, name='deconv_fc1')\n",
    "        deconv_input = tf.reshape(deconv_fc1, shape=[-1, 4, 4, 256])\n",
    "        batch_size = tf.shape(sample_input)[0]\n",
    "        deconv1 = self.trans_conv_layer(bottom=deconv_input, shape=(4,4,128,256),\n",
    "                                        output_shape=[batch_size, 8, 8, 128], stride=2, name='deconv1')\n",
    "        deconv2 = self.trans_conv_layer(bottom=deconv1, shape=(4,4,64,128),\n",
    "                                        output_shape=[batch_size, 16, 16, 64], stride=2, name='deconv2')\n",
    "        deconv3 = self.trans_conv_layer(bottom=deconv2, shape=(4,4,32,64),\n",
    "                                        output_shape=[batch_size, 32, 32, 32], stride=2, name='deconv3')\n",
    "        output = self.trans_conv_layer(bottom=deconv3, shape=(4,4,3,32),\n",
    "                                        output_shape=[batch_size, self.H, self.W, self.C], activation='tanh', stride=2, name='deconv_output')\n",
    "        return (output/2) + 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating save_acgan/\n",
      "Creating save_acgan/recons\n",
      "Creating save_acgan/samples\n",
      "Reading dataset...\n",
      "tanh\n",
      "GAN graph built.\n"
     ]
    }
   ],
   "source": [
    "# %load train.py\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from progress.bar import Bar\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "import skimage.transform\n",
    "import imageio\n",
    "\n",
    "from utils import read_dataset\n",
    "\n",
    "TRAIN_CSV = \"hw4_data/train.csv\"\n",
    "TRAIN_DIR = \"hw4_data/train/\"\n",
    "TEST_CSV = \"hw4_data/test.csv\"\n",
    "TEST_DIR = \"hw4_data/test/\"\n",
    "\n",
    "FLAG_lr = 1e-3\n",
    "FLAG_save_dir = 'save_acgan/'\n",
    "FLAG_batch_size = 32\n",
    "FLAG_n_dim = 100\n",
    "FLAG_D_G_ratio = 2\n",
    "\n",
    "if not os.path.exists(FLAG_save_dir):\n",
    "    print(\"Creating %s\" % FLAG_save_dir)\n",
    "    os.makedirs(FLAG_save_dir)\n",
    "if not os.path.exists(os.path.join(FLAG_save_dir, \"recons\")):\n",
    "    print(\"Creating %s\" % os.path.join(FLAG_save_dir, \"recons\"))\n",
    "    os.makedirs(os.path.join(FLAG_save_dir, \"recons\"))\n",
    "if not os.path.exists(os.path.join(FLAG_save_dir, \"samples\")):\n",
    "    print(\"Creating %s\" % os.path.join(FLAG_save_dir, \"samples\"))\n",
    "    os.makedirs(os.path.join(FLAG_save_dir, \"samples\"))\n",
    "\n",
    "print(\"Reading dataset...\")\n",
    "# load data\n",
    "Xtrain, df_train = read_dataset(TRAIN_CSV, TRAIN_DIR)\n",
    "Xtest , df_test  = read_dataset(TEST_CSV , TEST_DIR)\n",
    "\n",
    "aux_labels_train = np.expand_dims(df_train['Smiling'].values, axis=1)\n",
    "aux_labels_test  = np.expand_dims(df_test['Smiling'].values, axis=1)\n",
    "\n",
    "gan = ACGAN()\n",
    "gan.build(aux_dim=1, n_dim=FLAG_n_dim, shape=Xtrain.shape[1:])\n",
    "\n",
    "saver = tf.train.Saver(tf.global_variables(), max_to_keep=None)\n",
    "checkpoint_path = os.path.join(FLAG_save_dir, 'model.ckpt')\n",
    "\n",
    "def initialize_uninitialized(sess):\n",
    "    global_vars = tf.global_variables()\n",
    "    is_not_initialized = sess.run([tf.is_variable_initialized(var) for var in global_vars])\n",
    "    not_initialized_vars = [v for (v,f) in zip(global_vars, is_not_initialized) if not f]\n",
    "    if len(not_initialized_vars): \n",
    "        sess.run(tf.variables_initializer(not_initialized_vars))\n",
    "\n",
    "def res_plot(samples, n_row, n_col):     \n",
    "    fig = plt.figure(figsize=(n_col*2, n_row*2))\n",
    "    gs = gridspec.GridSpec(n_row, n_col)\n",
    "    gs.update(wspace=0.05, hspace=0.05)\n",
    "    for i, sample in enumerate(samples):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "        plt.imshow(sample.reshape(64, 64, 3))\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvars = tf.trainable_variables()\n",
    "D_vars, G_vars = [],[]\n",
    "for v in tvars:\n",
    "    if 'Generator' in v.name:\n",
    "        G_vars.append(v)\n",
    "    if 'Discriminator' in v.name:\n",
    "        D_vars.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bde4937a54c84502aec600db36801579",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>IntProgress</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "IntProgress(value=0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, 133.02 sec >> D real accu: 0.6725, D fake accu: 0.6894, D real aux loss: 0.5822, D fake aux loss: 0.2262, G loss: 12.0813\n",
      "Epoch 1, 119.53 sec >> D real accu: 0.6833, D fake accu: 0.7175, D real aux loss: 0.4597, D fake aux loss: 0.1043, G loss: 6.2408\n",
      "Epoch 2, 120.74 sec >> D real accu: 0.6673, D fake accu: 0.7043, D real aux loss: 0.4127, D fake aux loss: 0.0705, G loss: 4.6420\n",
      "INFO:tensorflow:save_acgan/model.ckpt-3 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Epoch 3, 119.81 sec >> D real accu: 0.6963, D fake accu: 0.7393, D real aux loss: 0.3901, D fake aux loss: 0.0539, G loss: 4.6562\n",
      "Epoch 4, 120.38 sec >> D real accu: 0.7597, D fake accu: 0.8076, D real aux loss: 0.3678, D fake aux loss: 0.0403, G loss: 5.1642\n",
      "Epoch 5, 119.29 sec >> D real accu: 0.7902, D fake accu: 0.8384, D real aux loss: 0.3529, D fake aux loss: 0.0387, G loss: 5.5918\n",
      "INFO:tensorflow:save_acgan/model.ckpt-6 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Epoch 6, 119.28 sec >> D real accu: 0.8182, D fake accu: 0.8577, D real aux loss: 0.3380, D fake aux loss: 0.0312, G loss: 5.9045\n",
      "Epoch 7, 120.84 sec >> D real accu: 0.8438, D fake accu: 0.8801, D real aux loss: 0.3213, D fake aux loss: 0.0381, G loss: 6.6114\n",
      "Epoch 8, 119.81 sec >> D real accu: 0.8623, D fake accu: 0.8992, D real aux loss: 0.2978, D fake aux loss: 0.0340, G loss: 7.0018\n",
      "INFO:tensorflow:save_acgan/model.ckpt-9 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Epoch 9, 124.31 sec >> D real accu: 0.8838, D fake accu: 0.9145, D real aux loss: 0.2710, D fake aux loss: 0.0267, G loss: 7.4720\n",
      "Epoch 10, 126.7 sec >> D real accu: 0.9110, D fake accu: 0.9355, D real aux loss: 0.2564, D fake aux loss: 0.0260, G loss: 8.0978\n",
      "Epoch 11, 126.1 sec >> D real accu: 0.9263, D fake accu: 0.9441, D real aux loss: 0.2411, D fake aux loss: 0.0262, G loss: 8.9507\n",
      "INFO:tensorflow:save_acgan/model.ckpt-12 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Epoch 12, 126.69 sec >> D real accu: 0.9288, D fake accu: 0.9459, D real aux loss: 0.2315, D fake aux loss: 0.0220, G loss: 9.1071\n",
      "Epoch 13, 126.87 sec >> D real accu: 0.9298, D fake accu: 0.9455, D real aux loss: 0.2262, D fake aux loss: 0.0220, G loss: 9.3707\n",
      "Epoch 14, 129.1 sec >> D real accu: 0.9207, D fake accu: 0.9374, D real aux loss: 0.2135, D fake aux loss: 0.0191, G loss: 8.7492\n",
      "INFO:tensorflow:save_acgan/model.ckpt-15 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Epoch 15, 127.44 sec >> D real accu: 0.9151, D fake accu: 0.9301, D real aux loss: 0.2069, D fake aux loss: 0.0197, G loss: 8.5738\n",
      "Epoch 16, 127.28 sec >> D real accu: 0.9138, D fake accu: 0.9291, D real aux loss: 0.1966, D fake aux loss: 0.0167, G loss: 8.2348\n",
      "Epoch 17, 126.81 sec >> D real accu: 0.9083, D fake accu: 0.9235, D real aux loss: 0.1908, D fake aux loss: 0.0188, G loss: 7.9250\n",
      "INFO:tensorflow:save_acgan/model.ckpt-18 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Epoch 18, 126.78 sec >> D real accu: 0.9022, D fake accu: 0.9137, D real aux loss: 0.1867, D fake aux loss: 0.0191, G loss: 7.7669\n",
      "Epoch 19, 126.71 sec >> D real accu: 0.8971, D fake accu: 0.9090, D real aux loss: 0.1807, D fake aux loss: 0.0175, G loss: 7.3425\n",
      "Epoch 20, 127.49 sec >> D real accu: 0.9052, D fake accu: 0.9149, D real aux loss: 0.1730, D fake aux loss: 0.0196, G loss: 7.3838\n",
      "INFO:tensorflow:save_acgan/model.ckpt-21 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Epoch 21, 126.45 sec >> D real accu: 0.9034, D fake accu: 0.9133, D real aux loss: 0.1679, D fake aux loss: 0.0188, G loss: 7.3787\n",
      "Epoch 22, 117.9 sec >> D real accu: 0.9100, D fake accu: 0.9184, D real aux loss: 0.1638, D fake aux loss: 0.0200, G loss: 7.5700\n",
      "Epoch 23, 114.73 sec >> D real accu: 0.9151, D fake accu: 0.9202, D real aux loss: 0.1543, D fake aux loss: 0.0185, G loss: 7.6955\n",
      "INFO:tensorflow:save_acgan/model.ckpt-24 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Epoch 24, 103.86 sec >> D real accu: 0.9184, D fake accu: 0.9271, D real aux loss: 0.1487, D fake aux loss: 0.0171, G loss: 7.6615\n",
      "Epoch 25, 103.36 sec >> D real accu: 0.9165, D fake accu: 0.9234, D real aux loss: 0.1450, D fake aux loss: 0.0160, G loss: 7.7960\n",
      "Epoch 26, 104.44 sec >> D real accu: 0.9207, D fake accu: 0.9260, D real aux loss: 0.1355, D fake aux loss: 0.0166, G loss: 7.9477\n",
      "INFO:tensorflow:save_acgan/model.ckpt-27 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Epoch 27, 103.73 sec >> D real accu: 0.9281, D fake accu: 0.9349, D real aux loss: 0.1293, D fake aux loss: 0.0152, G loss: 8.1474\n",
      "Epoch 28, 104.02 sec >> D real accu: 0.9248, D fake accu: 0.9296, D real aux loss: 0.1237, D fake aux loss: 0.0152, G loss: 8.2627\n",
      "Epoch 29, 103.72 sec >> D real accu: 0.9375, D fake accu: 0.9444, D real aux loss: 0.1187, D fake aux loss: 0.0162, G loss: 8.5594\n",
      "INFO:tensorflow:save_acgan/model.ckpt-30 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Epoch 30, 104.13 sec >> D real accu: 0.9308, D fake accu: 0.9351, D real aux loss: 0.1123, D fake aux loss: 0.0168, G loss: 8.6181\n",
      "Epoch 31, 104.65 sec >> D real accu: 0.9391, D fake accu: 0.9449, D real aux loss: 0.1043, D fake aux loss: 0.0148, G loss: 8.8645\n",
      "Epoch 32, 105.69 sec >> D real accu: 0.9395, D fake accu: 0.9446, D real aux loss: 0.1016, D fake aux loss: 0.0137, G loss: 9.2009\n",
      "INFO:tensorflow:save_acgan/model.ckpt-33 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Epoch 33, 105.54 sec >> D real accu: 0.9398, D fake accu: 0.9454, D real aux loss: 0.0965, D fake aux loss: 0.0140, G loss: 9.1573\n",
      "Epoch 34, 105.18 sec >> D real accu: 0.9350, D fake accu: 0.9414, D real aux loss: 0.0903, D fake aux loss: 0.0146, G loss: 9.2014\n",
      "Epoch 35, 104.87 sec >> D real accu: 0.9421, D fake accu: 0.9457, D real aux loss: 0.0877, D fake aux loss: 0.0162, G loss: 9.7284\n",
      "INFO:tensorflow:save_acgan/model.ckpt-36 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Epoch 36, 102.68 sec >> D real accu: 0.9476, D fake accu: 0.9513, D real aux loss: 0.0841, D fake aux loss: 0.0133, G loss: 9.6837\n",
      "Epoch 37, 104.59 sec >> D real accu: 0.9443, D fake accu: 0.9475, D real aux loss: 0.0778, D fake aux loss: 0.0154, G loss: 9.8478\n",
      "Epoch 38, 103.62 sec >> D real accu: 0.9486, D fake accu: 0.9526, D real aux loss: 0.0741, D fake aux loss: 0.0129, G loss: 9.8351\n",
      "INFO:tensorflow:save_acgan/model.ckpt-39 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Epoch 39, 102.7 sec >> D real accu: 0.9520, D fake accu: 0.9563, D real aux loss: 0.0686, D fake aux loss: 0.0120, G loss: 10.1010\n",
      "Epoch 40, 103.38 sec >> D real accu: 0.9490, D fake accu: 0.9517, D real aux loss: 0.0654, D fake aux loss: 0.0138, G loss: 10.2886\n",
      "Epoch 41, 102.85 sec >> D real accu: 0.9563, D fake accu: 0.9587, D real aux loss: 0.0625, D fake aux loss: 0.0146, G loss: 10.4281\n",
      "INFO:tensorflow:save_acgan/model.ckpt-42 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Epoch 42, 103.92 sec >> D real accu: 0.9531, D fake accu: 0.9553, D real aux loss: 0.0636, D fake aux loss: 0.0134, G loss: 10.5406\n",
      "Epoch 43, 102.55 sec >> D real accu: 0.9513, D fake accu: 0.9537, D real aux loss: 0.0625, D fake aux loss: 0.0156, G loss: 10.5887\n",
      "Epoch 44, 103.06 sec >> D real accu: 0.9547, D fake accu: 0.9573, D real aux loss: 0.0556, D fake aux loss: 0.0143, G loss: 10.8524\n",
      "INFO:tensorflow:save_acgan/model.ckpt-45 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Epoch 45, 104.74 sec >> D real accu: 0.9487, D fake accu: 0.9506, D real aux loss: 0.0549, D fake aux loss: 0.0121, G loss: 10.7101\n",
      "Epoch 46, 104.86 sec >> D real accu: 0.9525, D fake accu: 0.9569, D real aux loss: 0.0516, D fake aux loss: 0.0155, G loss: 10.9760\n",
      "Epoch 47, 104.6 sec >> D real accu: 0.9536, D fake accu: 0.9571, D real aux loss: 0.0527, D fake aux loss: 0.0140, G loss: 11.2035\n",
      "INFO:tensorflow:save_acgan/model.ckpt-48 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Epoch 48, 123.18 sec >> D real accu: 0.9573, D fake accu: 0.9604, D real aux loss: 0.0474, D fake aux loss: 0.0130, G loss: 11.2437\n",
      "Epoch 49, 122.8 sec >> D real accu: 0.9509, D fake accu: 0.9559, D real aux loss: 0.0478, D fake aux loss: 0.0136, G loss: 11.0292\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # hyper parameters\n",
    "    batch_size = FLAG_batch_size\n",
    "    epoch = 50\n",
    "    min_delta = 0.0001\n",
    "\n",
    "    # recorder\n",
    "    epoch_counter = 0\n",
    "\n",
    "    # optimizer\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "    # Passing global_step to minimize() will increment it at each step.\n",
    "    #start_learning_rate = FLAG_lr\n",
    "    #half_cycle = 20000\n",
    "    #learning_rate = tf.train.exponential_decay(start_learning_rate, global_step, half_cycle, 0.5, staircase=True)\n",
    "    \n",
    "    # define D and G operation, with respect to their own variabls\n",
    "    D_train_op = tf.train.AdamOptimizer(learning_rate=FLAG_lr).minimize(gan.D_loss, global_step=global_step, var_list=D_vars)\n",
    "    G_train_op = tf.train.AdamOptimizer(learning_rate=FLAG_lr).minimize(gan.G_loss, global_step=global_step, var_list=G_vars)    \n",
    "\n",
    "    # progress bar\n",
    "    ptrain = IntProgress()\n",
    "    display(ptrain)\n",
    "    ptrain.max = int(Xtrain.shape[0]/batch_size)\n",
    "\n",
    "    # re-initialize\n",
    "    initialize_uninitialized(sess)\n",
    "\n",
    "    # reset due to adding a new task\n",
    "    patience_counter = 0\n",
    "    current_best_val_loss = np.float('Inf')\n",
    "\n",
    "    loss_dict = dict({'D_real_accu':[],\n",
    "                      'D_fake_accu':[], \n",
    "                      'G_loss':[],\n",
    "                      'D_aux_real_loss':[],\n",
    "                      'D_aux_fake_loss':[]\n",
    "                     })\n",
    "    \n",
    "    \n",
    "    # optimize when the aggregated obj\n",
    "    while(epoch_counter < epoch):\n",
    "\n",
    "        # start training\n",
    "        stime = time.time()\n",
    "        bar_train = Bar('Training', max=int(Xtrain.shape[0]/batch_size), suffix='%(index)d/%(max)d - %(percent).1f%% - %(eta)ds')\n",
    "        bar_val =  Bar('Validation', max=int(Xtest.shape[0]/batch_size), suffix='%(index)d/%(max)d - %(percent).1f%% - %(eta)ds')\n",
    "\n",
    "        epoch_D_real_accu = 0.0\n",
    "        epoch_D_fake_accu = 0.0\n",
    "        epoch_D_aux_real_loss = 0.0\n",
    "        epoch_D_aux_fake_loss = 0.0\n",
    "        epoch_G_loss = 0.0\n",
    "        \n",
    "        for i in range(int(Xtrain.shape[0]/batch_size)):\n",
    "            # Update D\n",
    "            st = i*batch_size\n",
    "            ed = (i+1)*batch_size\n",
    "            real_accu, fake_accu, aux_real_loss, aux_fake_loss, _ = sess.run([gan.D_real_accu,\n",
    "                                                                              gan.D_fake_accu,\n",
    "                                                                              gan.D_aux_real_loss,\n",
    "                                                                              gan.D_aux_fake_loss,\n",
    "                                                                              D_train_op],\n",
    "                                                                             feed_dict={gan.x: Xtrain[st:ed,:],\n",
    "                                                                                        gan.aux_labels: aux_labels_train[st:ed,:],\n",
    "                                                                                        gan.random_sample: np.random.uniform(-1, 1, [batch_size, gan.n_dim]).astype(np.float32),\n",
    "                                                                                        gan.is_train: True})\n",
    "            epoch_D_real_accu += np.mean(real_accu)\n",
    "            epoch_D_fake_accu += np.mean(fake_accu)\n",
    "            epoch_D_aux_real_loss += np.mean(aux_real_loss)\n",
    "            epoch_D_aux_fake_loss += np.mean(aux_fake_loss)\n",
    "\n",
    "            # Update G\n",
    "            for _ in range(FLAG_D_G_ratio):\n",
    "                loss, _ = sess.run([gan.G_loss,\n",
    "                                    G_train_op],\n",
    "                                   feed_dict={gan.x: Xtrain[st:ed,:],\n",
    "                                              gan.aux_labels: aux_labels_train[st:ed,:],\n",
    "                                              gan.random_sample: np.random.uniform(-1, 1, [batch_size, gan.n_dim]).astype(np.float32),\n",
    "                                              gan.is_train: True})\n",
    "                epoch_G_loss += np.mean(loss)\n",
    "                \n",
    "            ptrain.value += 1\n",
    "            ptrain.description = \"Training %s/%s\" % (ptrain.value, ptrain.max)\n",
    "\n",
    "        print(\"Epoch %s, %s sec >> D real accu: %.4f, D fake accu: %.4f, D real aux loss: %.4f, D fake aux loss: %.4f, G loss: %.4f\" % (epoch_counter, round(time.time()-stime,2), epoch_D_real_accu/ptrain.value, epoch_D_fake_accu/ptrain.value, epoch_D_aux_real_loss/ptrain.value, epoch_D_aux_fake_loss/ptrain.value, epoch_G_loss/ptrain.value))\n",
    "        loss_dict['D_real_accu'].append(epoch_D_real_accu/ptrain.value)\n",
    "        loss_dict['D_fake_accu'].append(epoch_D_fake_accu/ptrain.value)\n",
    "        loss_dict['D_aux_real_loss'].append(epoch_D_aux_real_loss/ptrain.value)\n",
    "        loss_dict['D_aux_fake_loss'].append(epoch_D_aux_fake_loss/ptrain.value)\n",
    "        loss_dict['G_loss'].append(epoch_G_loss/ptrain.value)\n",
    "        \n",
    "        # epoch end\n",
    "        # shuffle Xtrain and Ytrain in the next epoch\n",
    "        idx = np.random.permutation(Xtrain.shape[0])\n",
    "        Xtrain= Xtrain[idx,:,:,:]\n",
    "        aux_labels_train = aux_labels_train[idx,:]\n",
    "        ptrain.value = 0\n",
    "        bar_train.finish()\n",
    "        epoch_counter += 1\n",
    "        \n",
    "            \n",
    "        # plot\n",
    "        Xplot = sess.run(gan.G_image,\n",
    "                feed_dict={gan.random_sample: np.random.uniform(-1, 1, [batch_size, gan.n_dim]).astype(np.float32),\n",
    "                           gan.aux_labels: np.expand_dims(np.repeat([0,1], int(batch_size/2)),axis=1),\n",
    "                           gan.is_train: False})\n",
    "        fig = res_plot(Xplot, int(batch_size/8), 8)\n",
    "        plt.savefig(os.path.join(FLAG_save_dir, 'samples', '{}.png'.format(str(epoch_counter).zfill(3))), \n",
    "                    bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "        if epoch_counter%3==0:\n",
    "            saver.save(sess,os.path.join(FLAG_save_dir,'model.ckpt'),global_step=epoch_counter)\n",
    "np.save(os.path.join(FLAG_save_dir, \"history_dict.npy\"), loss_dict)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
