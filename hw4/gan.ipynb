{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "class GAN():\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def load(self, npy_path):\n",
    "        self.data_dict = np.load(npy_path, encoding='latin1').item()\n",
    "        print(\"Load %s as self.data_dict\" % npy_path)\n",
    "\n",
    "    def build(self, n_dim=512, shape=(64,64,3)):\n",
    "        \"\"\"\n",
    "        load pre-trained weights from path\n",
    "        :param vgg16_npy_path: file path of vgg16 pre-trained weights\n",
    "        \"\"\"\n",
    "        # input information\n",
    "        self.H, self.W, self.C = shape\n",
    "        self.n_dim = n_dim\n",
    "        \n",
    "        # parameter dictionary\n",
    "        self.para_dict = dict()\n",
    "        self.data_dict = dict()\n",
    "        self.net_shape = dict()\n",
    "\n",
    "        # input placeholder\n",
    "        self.x = tf.placeholder(tf.float32, [None, self.H, self.W, self.C])\n",
    "        self.is_train = tf.placeholder(tf.bool)\n",
    "        self.random_sample = tf.placeholder(tf.float32, [None, self.n_dim])\n",
    "        \n",
    "        # normalize inputs\n",
    "        assert self.x.get_shape().as_list()[1:] == [self.H, self.W, self.C]\n",
    "        \n",
    "\n",
    "        with tf.variable_scope(\"Generator\",reuse=tf.AUTO_REUSE):\n",
    "            self.G_image = self.generator(self.random_sample)\n",
    "            \n",
    "        with tf.variable_scope(\"Discriminator\", reuse=tf.AUTO_REUSE):\n",
    "            self.D_real = self.discriminator(self.x)\n",
    "            self.D_fake = self.discriminator(self.G_image)\n",
    "    \n",
    "        self.real_equality = tf.equal(tf.cast(tf.sigmoid(self.D_real) > 0.5, tf.float32), tf.ones(shape=tf.shape(self.D_real)))\n",
    "        self.D_real_accu = tf.reduce_mean(tf.cast(self.real_equality, tf.float32))\n",
    "        \n",
    "        self.fake_equality = tf.equal(tf.cast(tf.sigmoid(self.D_fake) > 0.5, tf.float32), tf.zeros(shape=tf.shape(self.D_fake)))\n",
    "        self.D_fake_accu = tf.reduce_mean(tf.cast(self.fake_equality, tf.float32))\n",
    "        \n",
    "        # loss of discriminator\n",
    "        self.D_real_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=self.D_real,\n",
    "                                                                                  labels=tf.ones(shape=tf.shape(self.D_real))))\n",
    "        self.D_fake_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=self.D_fake,\n",
    "                                                                                  labels=tf.zeros(shape=tf.shape(self.D_fake))))\n",
    "        \n",
    "        self.D_loss = self.D_real_loss + self.D_fake_loss\n",
    "        \n",
    "        # loss of generator\n",
    "        self.G_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=self.D_fake,\n",
    "                                                                             labels=tf.ones(shape=tf.shape(self.D_fake))))\n",
    "    \n",
    "    def discriminator(self, input_image):\n",
    "        conv1 = self.conv_bn_layer(input_image, shape=(4,4,3,32), stride=2, name=\"conv1\")\n",
    "        conv2 = self.conv_bn_layer(conv1 , shape=(4,4,32,64), stride=2, name=\"conv2\")\n",
    "        conv3 = self.conv_bn_layer(conv2 , shape=(4,4,64,128), stride=2, name=\"conv3\")\n",
    "        conv4 = self.conv_bn_layer(conv3 , shape=(4,4,128,256), stride=2, name=\"conv4\")\n",
    "        flatten = self.flatten_layer(conv4, name='flatten')\n",
    "        output = self.dense_layer(flatten, n_hidden=1, name='D_output')\n",
    "        return output\n",
    "        \n",
    "    def generator(self, sample_input):\n",
    "        deconv_fc1 = self.dense_layer(sample_input, n_hidden=4096, name='deconv_fc1')\n",
    "        deconv_input = tf.reshape(deconv_fc1, shape=[-1, 4, 4, 256])\n",
    "        batch_size = tf.shape(sample_input)[0]\n",
    "        deconv1 = self.trans_conv_layer(bottom=deconv_input, shape=(4,4,128,256),\n",
    "                                        output_shape=[batch_size, 8, 8, 128], stride=2, name='deconv1')\n",
    "        deconv2 = self.trans_conv_layer(bottom=deconv1, shape=(4,4,64,128),\n",
    "                                        output_shape=[batch_size, 16, 16, 64], stride=2, name='deconv2')\n",
    "        deconv3 = self.trans_conv_layer(bottom=deconv2, shape=(4,4,32,64),\n",
    "                                        output_shape=[batch_size, 32, 32, 32], stride=2, name='deconv3')\n",
    "        output = self.trans_conv_layer(bottom=deconv3, shape=(4,4,3,32),\n",
    "                                        output_shape=[batch_size, self.H, self.W, self.C], activation='tanh', stride=2, name='deconv_output')\n",
    "        return (output/2) + 0.5\n",
    "\n",
    "    def dense_layer(self, bottom, n_hidden=None, name=None):\n",
    "        bottom_shape = bottom.get_shape().as_list()\n",
    "        if n_hidden is not None:\n",
    "            W = self.get_weights(shape=(bottom_shape[1], n_hidden), name=name)\n",
    "            b = self.get_bias(shape=n_hidden, name=name)\n",
    "        elif name in self.data_dict.keys():\n",
    "            W = self.get_weights(name=name)\n",
    "            b = self.get_bias(name=name)\n",
    "        else:\n",
    "            print(\"Neither give a shape nor lack a pre-trained layer called %s\" % name)\n",
    "        self.para_dict[name] = [W, b]\n",
    "        fc = tf.nn.bias_add(tf.matmul(bottom, W), b)\n",
    "        self.net_shape[name] = fc.get_shape().as_list()\n",
    "        return fc\n",
    "\n",
    "    def flatten_layer(self, bottom, name):\n",
    "        shape = bottom.get_shape().as_list()\n",
    "        dim = 1\n",
    "        for d in shape[1:]:\n",
    "            dim *= d\n",
    "        flatten = tf.reshape(bottom, [-1, dim])\n",
    "        self.net_shape[name] = flatten.get_shape().as_list()\n",
    "        return flatten\n",
    "\n",
    "    def avg_pool_layer(self, bottom, name):\n",
    "        pool = tf.nn.avg_pool(bottom, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name=name)\n",
    "        self.net_shape[name] = pool.get_shape().as_list()\n",
    "        return pool\n",
    "\n",
    "    def max_pool_layer(self, bottom, name):\n",
    "        pool = tf.nn.max_pool(bottom, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name=name)\n",
    "        self.net_shape[name] = pool.get_shape().as_list()\n",
    "        return pool\n",
    "    \n",
    "    def dropout(self, bottom, keep_prob):\n",
    "        if self.is_train == True:\n",
    "            return tf.nn.dropout(bottom, keep_prob=keep_prob)\n",
    "        else:\n",
    "            return bottom\n",
    "\n",
    "    def trans_conv_layer(self, bottom, output_shape, stride, activation='relu', name=None, shape=None):\n",
    "        if shape is not None:\n",
    "            conv_filter, gamma, beta, bn_mean, bn_variance = self.get_conv_filter(shape=shape, name=name)\n",
    "            conv_bias = self.get_bias(shape=shape[2], name=name)\n",
    "        elif name in self.data_dict.keys():\n",
    "            conv_filter, gamma, beta, bn_mean, bn_variance = self.get_conv_filter(name=name)\n",
    "            conv_bias = self.get_bias(name=name)\n",
    "        else:\n",
    "            print(\"Neither give a shape nor lack a pre-trained layer called %s\" % name)\n",
    "\n",
    "        self.para_dict[name] = [conv_filter, conv_bias]\n",
    "        self.para_dict[name+\"_gamma\"] = gamma\n",
    "        self.para_dict[name+\"_beta\"] = beta\n",
    "        self.para_dict[name+\"_bn_mean\"] = bn_mean\n",
    "        self.para_dict[name+\"_bn_variance\"] = bn_variance\n",
    "\n",
    "        conv = tf.nn.conv2d_transpose(bottom, conv_filter, output_shape, strides=[1, stride, stride, 1], padding=\"SAME\")\n",
    "        conv = tf.nn.bias_add(conv, conv_bias)\n",
    "        \n",
    "        from tensorflow.python.training.moving_averages import assign_moving_average\n",
    "        def mean_var_with_update():\n",
    "            mean, variance = tf.nn.moments(conv, [0,1,2], name='moments')\n",
    "            with tf.control_dependencies([assign_moving_average(bn_mean, mean, 0.99),\n",
    "                                            assign_moving_average(bn_variance, variance, 0.99)]):\n",
    "                return tf.identity(mean), tf.identity(variance)\n",
    "\n",
    "        mean, variance = tf.cond(self.is_train, mean_var_with_update, lambda:(bn_mean, bn_variance))\n",
    "        conv = tf.nn.batch_normalization(conv, mean, variance, beta, gamma, 1e-05)\n",
    "        self.net_shape[name] = conv.get_shape().as_list()\n",
    "\n",
    "        if activation=='tanh':\n",
    "            print('tanh')\n",
    "            tanh = tf.nn.tanh(conv)\n",
    "            return tanh\n",
    "        else:\n",
    "            relu = tf.nn.leaky_relu(conv)\n",
    "            return relu\n",
    "\n",
    "    def conv_bn_layer(self, bottom, stride=1, activation='lrelu', name=None, shape=None):\n",
    "        if shape is not None:\n",
    "            conv_filter, gamma, beta, bn_mean, bn_variance = self.get_conv_filter(shape=shape, name=name)\n",
    "            conv_bias = self.get_bias(shape=shape[3], name=name)\n",
    "        elif name in self.data_dict.keys():\n",
    "            conv_filter, gamma, beta, bn_mean, bn_variance = self.get_conv_filter(name=name)\n",
    "            conv_bias = self.get_bias(name=name)\n",
    "        else:\n",
    "            print(\"Neither give a shape nor lack a pre-trained layer called %s\" % name)\n",
    "\n",
    "        self.para_dict[name] = [conv_filter, conv_bias]\n",
    "        self.para_dict[name+\"_gamma\"] = gamma\n",
    "        self.para_dict[name+\"_beta\"] = beta\n",
    "        self.para_dict[name+\"_bn_mean\"] = bn_mean\n",
    "        self.para_dict[name+\"_bn_variance\"] = bn_variance\n",
    "\n",
    "        conv = tf.nn.conv2d(bottom, conv_filter, [1, stride, stride, 1], padding='SAME')\n",
    "        conv = tf.nn.bias_add(conv, conv_bias)\n",
    "\n",
    "        from tensorflow.python.training.moving_averages import assign_moving_average\n",
    "        def mean_var_with_update():\n",
    "            mean, variance = tf.nn.moments(conv, [0,1,2], name='moments')\n",
    "            with tf.control_dependencies([assign_moving_average(bn_mean, mean, 0.99),\n",
    "                                            assign_moving_average(bn_variance, variance, 0.99)]):\n",
    "                return tf.identity(mean), tf.identity(variance)\n",
    "\n",
    "        mean, variance = tf.cond(self.is_train, mean_var_with_update, lambda:(bn_mean, bn_variance))\n",
    "\n",
    "        conv = tf.nn.batch_normalization(conv, mean, variance, beta, gamma, 1e-05)\n",
    "        self.net_shape[name] = conv.get_shape().as_list()\n",
    "\n",
    "        if activation=='tanh':\n",
    "            tanh = tf.nn.tanh(conv)\n",
    "            return tanh\n",
    "        else:\n",
    "            relu = tf.nn.leaky_relu(conv)\n",
    "            return relu\n",
    "\n",
    "    def get_conv_filter(self, shape=None, name=None, with_bn=True):\n",
    "        if shape is not None:\n",
    "            conv_filter = tf.get_variable(shape=shape, initializer=tf.truncated_normal_initializer(mean=0, stddev=0.1), name=name+\"_W\", dtype=tf.float32)\n",
    "        elif name in self.data_dict.keys():\n",
    "            conv_filter = tf.get_variable(initializer=self.data_dict[name][0], name=name+\"_W\")\n",
    "        else:\n",
    "            print(\"Neither give a shape nor lack a pre-trained layer called %s\" % name)\n",
    "            return None\n",
    "\n",
    "        if with_bn:\n",
    "            if 'deconv' in name:\n",
    "                H,W,O,C = conv_filter.get_shape().as_list()\n",
    "            else:\n",
    "                H,W,C,O = conv_filter.get_shape().as_list()\n",
    "\n",
    "            if name+\"_gamma\" in self.data_dict.keys(): \n",
    "                gamma = tf.get_variable(initializer=self.data_dict[name+\"_gamma\"], name=name+\"_gamma\")\n",
    "            else:\n",
    "                gamma = tf.get_variable(shape=(O,), initializer=tf.ones_initializer(), name=name+\"_gamma\")\n",
    "\n",
    "            if name+\"_beta\" in self.data_dict.keys(): \n",
    "                beta = tf.get_variable(initializer=self.data_dict[name+\"_beta\"], name=name+\"_beta\")\n",
    "            else:\n",
    "                beta = tf.get_variable(shape=(O,), initializer=tf.zeros_initializer(), name=name+'_beta')\n",
    "\n",
    "            if name+\"_bn_mean\" in self.data_dict.keys(): \n",
    "                bn_mean = tf.get_variable(initializer=self.data_dict[name+\"_bn_mean\"], name=name+\"_bn_mean\")\n",
    "            else:\n",
    "                bn_mean = tf.get_variable(shape=(O,), initializer=tf.zeros_initializer(), name=name+'_bn_mean')\n",
    "\n",
    "            if name+\"_bn_variance\" in self.data_dict.keys(): \n",
    "                bn_variance = tf.get_variable(initializer=self.data_dict[name+\"_bn_variance\"], name=name+\"_bn_variance\")\n",
    "            else:\n",
    "                bn_variance = tf.get_variable(shape=(O,), initializer=tf.ones_initializer(), name=name+'_bn_variance')\n",
    "            return conv_filter, gamma, beta, bn_mean, bn_variance\n",
    "        else:\n",
    "            return conv_filter\n",
    "    \n",
    "    def get_weights(self, shape=None, name=None):\n",
    "        if shape is not None:\n",
    "            return tf.get_variable(shape=shape, initializer=tf.truncated_normal_initializer(mean=0, stddev=0.1), name=name+\"_W\", dtype=tf.float32)\n",
    "        elif name in self.data_dict.keys(): \n",
    "            return tf.get_variable(initializer=self.data_dict[name][0], name=name+\"_W\")\n",
    "        else:\n",
    "            print(\"(get_weight) neither give a shape nor lack a pre-trained layer called %s\" % name)\n",
    "            return None\n",
    "            \n",
    "    def get_bias(self, shape=None, name=None):\n",
    "        if shape is not None:\n",
    "            return tf.get_variable(shape=shape, initializer=tf.truncated_normal_initializer(mean=0, stddev=0.1), name=name+\"_b\", dtype=tf.float32)\n",
    "        elif name in self.data_dict.keys(): \n",
    "            return tf.get_variable(initializer=self.data_dict[name][1], name=name+\"_b\")\n",
    "        else:\n",
    "            print(\"(get_bias) neither give a shape nor lack a pre-trained layer called %s\" % name)\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating save_gan_n1/\n",
      "Creating save_gan_n1/recons\n",
      "Creating save_gan_n1/samples\n",
      "Reading dataset...\n",
      "tanh\n"
     ]
    }
   ],
   "source": [
    "# %load train.py\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from progress.bar import Bar\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "import skimage.transform\n",
    "import imageio\n",
    "\n",
    "from utils import read_dataset\n",
    "\n",
    "TRAIN_CSV = \"hw4_data/train.csv\"\n",
    "TRAIN_DIR = \"hw4_data/train/\"\n",
    "TEST_CSV = \"hw4_data/test.csv\"\n",
    "TEST_DIR = \"hw4_data/test/\"\n",
    "\n",
    "FLAG_lr = 1e-4\n",
    "FLAG_save_dir = 'save_gan_n1/'\n",
    "FLAG_batch_size = 32\n",
    "FLAG_n_dim = 100\n",
    "FLAG_D_G_ratio = 2\n",
    "\n",
    "if not os.path.exists(FLAG_save_dir):\n",
    "    print(\"Creating %s\" % FLAG_save_dir)\n",
    "    os.makedirs(FLAG_save_dir)\n",
    "if not os.path.exists(os.path.join(FLAG_save_dir, \"recons\")):\n",
    "    print(\"Creating %s\" % os.path.join(FLAG_save_dir, \"recons\"))\n",
    "    os.makedirs(os.path.join(FLAG_save_dir, \"recons\"))\n",
    "if not os.path.exists(os.path.join(FLAG_save_dir, \"samples\")):\n",
    "    print(\"Creating %s\" % os.path.join(FLAG_save_dir, \"samples\"))\n",
    "    os.makedirs(os.path.join(FLAG_save_dir, \"samples\"))\n",
    "\n",
    "print(\"Reading dataset...\")\n",
    "# load data\n",
    "Xtrain, df_train = read_dataset(TRAIN_CSV, TRAIN_DIR)\n",
    "Xtest , df_test  = read_dataset(TEST_CSV , TEST_DIR)\n",
    "\n",
    "gan = GAN()\n",
    "gan.build(n_dim=FLAG_n_dim, shape=Xtrain.shape[1:])\n",
    "\n",
    "saver = tf.train.Saver(tf.global_variables(), max_to_keep=None)\n",
    "\n",
    "def initialize_uninitialized(sess):\n",
    "    global_vars = tf.global_variables()\n",
    "    is_not_initialized = sess.run([tf.is_variable_initialized(var) for var in global_vars])\n",
    "    not_initialized_vars = [v for (v,f) in zip(global_vars, is_not_initialized) if not f]\n",
    "    if len(not_initialized_vars): \n",
    "        sess.run(tf.variables_initializer(not_initialized_vars))\n",
    "\n",
    "def res_plot(samples, n_row, n_col):     \n",
    "    fig = plt.figure(figsize=(n_col*2, n_row*2))\n",
    "    gs = gridspec.GridSpec(n_row, n_col)\n",
    "    gs.update(wspace=0.05, hspace=0.05)\n",
    "    for i, sample in enumerate(samples):\n",
    "        ax = plt.subplot(gs[i])\n",
    "        plt.axis('off')\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_aspect('equal')\n",
    "        plt.imshow(sample.reshape(64, 64, 3))\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvars = tf.trainable_variables()\n",
    "D_vars, G_vars = [],[]\n",
    "for v in tvars:\n",
    "    if 'Generator' in v.name:\n",
    "        G_vars.append(v)\n",
    "    if 'Discriminator' in v.name:\n",
    "        D_vars.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "003ebce9d77e43f8b7faa21214de54fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>IntProgress</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "IntProgress(value=0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, 78.32 sec >> train D real accu: 0.9048, train D fake accu: 0.9226, train G loss: 8.3313\n",
      "Epoch 1, 75.78 sec >> train D real accu: 0.8028, train D fake accu: 0.8327, train G loss: 5.6123\n",
      "Epoch 2, 76.18 sec >> train D real accu: 0.7083, train D fake accu: 0.7452, train G loss: 4.2717\n",
      "Epoch 3, 76.68 sec >> train D real accu: 0.7111, train D fake accu: 0.7560, train G loss: 3.6873\n",
      "Epoch 4, 76.68 sec >> train D real accu: 0.6883, train D fake accu: 0.7319, train G loss: 3.0329\n",
      "Epoch 5, 76.37 sec >> train D real accu: 0.6448, train D fake accu: 0.6792, train G loss: 2.5456\n",
      "Epoch 6, 76.43 sec >> train D real accu: 0.6163, train D fake accu: 0.6420, train G loss: 2.2537\n",
      "Epoch 7, 76.01 sec >> train D real accu: 0.6109, train D fake accu: 0.6353, train G loss: 1.9851\n",
      "Epoch 8, 76.25 sec >> train D real accu: 0.5842, train D fake accu: 0.6020, train G loss: 1.8384\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # hyper parameters\n",
    "    batch_size = FLAG_batch_size\n",
    "    epoch = 50\n",
    "    min_delta = 0.0001\n",
    "\n",
    "    # recorder\n",
    "    epoch_counter = 0\n",
    "\n",
    "    # optimizer\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "    # Passing global_step to minimize() will increment it at each step.\n",
    "    start_learning_rate = FLAG_lr\n",
    "    half_cycle = 12500\n",
    "    learning_rate = tf.train.exponential_decay(start_learning_rate, global_step, half_cycle, 0.5, staircase=True)\n",
    "    \n",
    "    # define D and G operation, with respect to their own variabls\n",
    "    D_train_op = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(gan.D_loss, global_step=global_step, var_list=D_vars)\n",
    "    G_train_op = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(gan.G_loss, global_step=global_step, var_list=G_vars)    \n",
    "\n",
    "    # progress bar\n",
    "    ptrain = IntProgress()\n",
    "    display(ptrain)\n",
    "    ptrain.max = int(Xtrain.shape[0]/batch_size)\n",
    "\n",
    "    # re-initialize\n",
    "    initialize_uninitialized(sess)\n",
    "\n",
    "    loss_dict = dict({'D_fake_accu':[], \n",
    "                      'D_real_accu':[],\n",
    "                      'G_loss':[]\n",
    "                     })\n",
    "    \n",
    "    \n",
    "    # optimize when the aggregated obj\n",
    "    while(epoch_counter < epoch):\n",
    "\n",
    "        # start training\n",
    "        stime = time.time()\n",
    "        bar_train = Bar('Training', max=int(Xtrain.shape[0]/batch_size), suffix='%(index)d/%(max)d - %(percent).1f%% - %(eta)ds')\n",
    "        bar_val =  Bar('Validation', max=int(Xtest.shape[0]/batch_size), suffix='%(index)d/%(max)d - %(percent).1f%% - %(eta)ds')\n",
    "\n",
    "        epoch_D_real_accu = 0.0\n",
    "        epoch_D_fake_accu = 0.0\n",
    "        epoch_G_loss = 0.0\n",
    "        stime = time.time()\n",
    "        for i in range(int(Xtrain.shape[0]/batch_size)):\n",
    "            # Update D\n",
    "            st = i*batch_size\n",
    "            ed = (i+1)*batch_size\n",
    "            real_accu, fake_accu, _ = sess.run([gan.D_real_accu,gan.D_fake_accu,D_train_op],\n",
    "                               feed_dict={gan.x: Xtrain[st:ed,:],\n",
    "                                          gan.random_sample: np.random.uniform(-1, 1, [batch_size, gan.n_dim]).astype(np.float32),\n",
    "                                          gan.is_train: True}\n",
    "                              )\n",
    "            epoch_D_real_accu += np.mean(real_accu)\n",
    "            epoch_D_fake_accu += np.mean(fake_accu)\n",
    "        \n",
    "            # Update G\n",
    "            for _ in range(FLAG_D_G_ratio):\n",
    "                loss, _ = sess.run([gan.G_loss,G_train_op],\n",
    "                                   feed_dict={gan.x: Xtrain[st:ed,:],\n",
    "                                              gan.random_sample: np.random.uniform(-1, 1, [batch_size, gan.n_dim]).astype(np.float32),\n",
    "                                              gan.is_train: True})\n",
    "                epoch_G_loss += np.mean(loss)\n",
    "            ptrain.value += 1\n",
    "            ptrain.description = \"G-Training %s/%s\" % (ptrain.value, ptrain.max)\n",
    "            \n",
    "        print(\"Epoch %s, %s sec >> train D real accu: %.4f, train D fake accu: %.4f, train G loss: %.4f\" % (epoch_counter, round(time.time()-stime,2), epoch_D_real_accu/ptrain.value, epoch_D_fake_accu/ptrain.value, epoch_G_loss/ptrain.value))    \n",
    "        loss_dict['D_real_accu'].append(epoch_D_real_accu/ptrain.value)\n",
    "        loss_dict['D_fake_accu'].append(epoch_D_fake_accu/ptrain.value)\n",
    "        loss_dict['G_loss'].append(epoch_G_loss/ptrain.value)\n",
    "        \n",
    "        # epoch end\n",
    "        # shuffle Xtrain and Ytrain in the next epoch\n",
    "        idx = np.random.permutation(Xtrain.shape[0])\n",
    "        Xtrain= Xtrain[idx,:,:,:]\n",
    "        ptrain.value = 0\n",
    "        bar_train.finish()\n",
    "        epoch_counter += 1\n",
    "\n",
    "            \n",
    "        # plot\n",
    "        Xplot = sess.run(gan.G_image,\n",
    "                feed_dict={gan.random_sample: np.random.uniform(-1, 1, [batch_size, gan.n_dim]).astype(np.float32),\n",
    "                           gan.is_train: False})\n",
    "        fig = res_plot(Xplot, int(batch_size/8), 8)\n",
    "        plt.savefig(os.path.join(FLAG_save_dir, 'samples', '{}.png'.format(str(epoch_counter).zfill(3))), \n",
    "                    bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "        \n",
    "        if epoch_counter%3==0 and epoch_counter>20:\n",
    "            saver.save(sess,os.path.join(FLAG_save_dir,'model.ckpt'),global_step=epoch_counter)\n",
    "np.save(os.path.join(FLAG_save_dir, \"history_dict.npy\"), loss_dict)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = np.load(\"save_gan_v2/history_dict.npy\", encoding='latin1').item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a= pd.DataFrame.from_dict(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = a[['D_fake_accu', 'D_real_accu']].plot()\n",
    "ax.set_xlabel('epoch')\n",
    "ax.set_ylabel('accuarcy')\n",
    "ax.set_title('GAN Discriminator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = a['G_loss'].plot()\n",
    "ax.set_xlabel('epoch')\n",
    "ax.set_ylabel('loss')\n",
    "ax.set_title('GAN Generator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
