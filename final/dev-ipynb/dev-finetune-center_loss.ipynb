{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='4'\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from VGG16_GAP import VGG16_GAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import imgaug as ia\n",
    "from imgaug import augmenters as iaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import skimage.io as imageio\n",
    "\n",
    "def readImgList(file_list):\n",
    "    images = list()\n",
    "    for i, file in enumerate(file_list):\n",
    "        print(i, end=\"\\r\")\n",
    "        img = imageio.imread(file)\n",
    "        img = img.astype(int)\n",
    "        images.append(img)\n",
    "    return np.array(images)\n",
    "\n",
    "def transformLabel(id_list, y_dict):\n",
    "    label = list()\n",
    "    for uid in list(id_list):\n",
    "        label.append(y_dict[uid])\n",
    "    return np.array(label)\n",
    "\n",
    "def one_hot_encoding(class_numbers, num_classes):\n",
    "    return np.eye(num_classes, dtype=float)[class_numbers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('save/label_dict.pkl', 'rb') as f:\n",
    "    y_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOME_DIR = \"/home/cmchang/DLCV2018SPRING/final/\"\n",
    "TRAIN_DIR = HOME_DIR+\"dlcv_final_2_dataset/train/\"\n",
    "VALID_DIR = HOME_DIR+\"dlcv_final_2_dataset/val/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = pd.read_csv(HOME_DIR+\"dlcv_final_2_dataset/train_id.txt\", header=None,sep=\" \", names=[\"img\", \"id\"])\n",
    "dvalid = pd.read_csv(HOME_DIR+\"dlcv_final_2_dataset/val_id.txt\", header=None,sep=\" \", names=[\"img\", \"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list = list(TRAIN_DIR+dtrain.img)\n",
    "valid_list = list(VALID_DIR+dvalid.img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_uninitialized(sess):\n",
    "    global_vars = tf.global_variables()\n",
    "    is_not_initialized = sess.run([tf.is_variable_initialized(var) for var in global_vars])\n",
    "    not_initialized_vars = [v for (v,f) in zip(global_vars, is_not_initialized) if not f]\n",
    "    if len(not_initialized_vars): \n",
    "            sess.run(tf.variables_initializer(not_initialized_vars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = readImgList(train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xvalid = readImgList(valid_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain = transformLabel(list(dtrain.id), y_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yvalid = transformLabel(list(dvalid.id), y_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ytrain = one_hot_encoding(ytrain, len(y_dict))\n",
    "Yvalid = one_hot_encoding(yvalid, len(y_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scope_name = \"Model22\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGG16_GAP(scope_name=scope_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SELF_DIR = \"newCL_v5_lambda-1e-1_dynamic_gap_L5_v3_rescale0-1_save_linear/\"\n",
    "FLAG_init_from = HOME_DIR+SELF_DIR+\"para_dict.npy\"\n",
    "FLAG_prof_type = \"linear\"\n",
    "FLAG_lambda_s = 4e-3\n",
    "FLAG_lambda_m = 2e-5\n",
    "FLAG_decay = 1e-5\n",
    "FLAG_lr = 2e-6\n",
    "FLAG_keep_prob = 1.0\n",
    "FLAG_lambda_c = 1e-3\n",
    "FLAG_save_dir = HOME_DIR+\"jus_test/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build(vgg16_npy_path=FLAG_init_from,\n",
    "            shape=Xtrain.shape[1:],\n",
    "            classes=len(y_dict),\n",
    "            prof_type=FLAG_prof_type,\n",
    "            conv_pre_training=True,\n",
    "            fc_pre_training=True,\n",
    "            new_bn=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers = np.load(HOME_DIR+SELF_DIR+\"centers.npy\")\n",
    "model.add_centers(centers.astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp = [1.0, 0.75, 0.5]\n",
    "tasks = [str(int(p*100)) for p in dp]\n",
    "model.set_idp_operation(dp=dp, decay=FLAG_decay, keep_prob=FLAG_keep_prob, lambda_c = FLAG_lambda_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = 0.0\n",
    "for cur_task in tasks:\n",
    "    print(cur_task)\n",
    "    obj += model.loss_dict[cur_task]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracking = list()\n",
    "for cur_task in tasks:\n",
    "    tracking.append(model.accu_dict[cur_task])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sometimes = lambda aug: iaa.Sometimes(0.5, aug)\n",
    "transform = iaa.Sequential([\n",
    "    sometimes(iaa.Affine(translate_percent={\"x\": (-0.15, 0.15), \"y\": (-0.15, 0.15)})),\n",
    "    sometimes(iaa.Affine(scale={\"x\": (0.85, 1.15), \"y\":(0.85, 1.15)})),\n",
    "    sometimes(iaa.Affine(rotate=(-45, 45))),\n",
    "    sometimes(iaa.Fliplr(0.5))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"===== create directory =====\")\n",
    "if not os.path.exists(FLAG_save_dir):\n",
    "    os.makedirs(FLAG_save_dir)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    augment = True\n",
    "\n",
    "   # hyper parameters\n",
    "    batch_size = 32\n",
    "    epoch = 100\n",
    "    early_stop_patience = 10\n",
    "    min_delta = 0.0001\n",
    "\n",
    "    # recorder\n",
    "    epoch_counter = 0\n",
    "    history = list()\n",
    "\n",
    "    # Passing global_step to minimize() will increment it at each step.\n",
    "    learning_rate = FLAG_lr\n",
    "    opt = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.5)\n",
    "\n",
    "    checkpoint_path = os.path.join(FLAG_save_dir, 'model.ckpt')\n",
    "    \n",
    "    # trainable variables\n",
    "    train_vars = list()\n",
    "    for var in tf.trainable_variables():\n",
    "        if model.scope_name in var.name:\n",
    "            train_vars.append(var)\n",
    "    \n",
    "    for var in train_vars:\n",
    "        if '_mean' in var.name:\n",
    "            train_vars.remove(var)\n",
    "            print('%s is not trainable.'% var)\n",
    "    \n",
    "    for var in train_vars:\n",
    "        if '_variance' in var.name:\n",
    "            train_vars.remove(var)\n",
    "            print('%s is not trainable.'% var)\n",
    "    \n",
    "    print(train_vars)\n",
    "            \n",
    "    train_op = opt.minimize(obj, var_list=train_vars)\n",
    "    \n",
    "    saver = tf.train.Saver(tf.global_variables(), max_to_keep=len(tasks))\n",
    "\n",
    "    # max step in a epoch\n",
    "    ptrain_max = int(Xtrain.shape[0]/batch_size)\n",
    "    pval_max = int(Xvalid.shape[0]/batch_size)\n",
    "\n",
    "    # re-initialize\n",
    "    initialize_uninitialized(sess)\n",
    "\n",
    "    # reset due to adding a new task\n",
    "    patience_counter = 0\n",
    "    current_best_val_accu = 0\n",
    "\n",
    "    # optimize when the aggregated obj\n",
    "    while(patience_counter < early_stop_patience and epoch_counter < epoch):\n",
    "        \n",
    "        # start training\n",
    "        stime = time.time()\n",
    "        train_loss, train_accu = 0.0, 0.0\n",
    "        \n",
    "        if augment:\n",
    "            def load_batches():\n",
    "                for i in range(int(Xtrain.shape[0]/batch_size)):\n",
    "                    print(\"Training: {0}/{1}\".format(i,ptrain_max), end='\\r')\n",
    "                    st = i*batch_size\n",
    "                    ed = (i+1)*batch_size\n",
    "                    batch = ia.Batch(images=Xtrain[st:ed,:,:,:], data=Ytrain[st:ed,:])\n",
    "                    yield batch\n",
    "\n",
    "            batch_loader = ia.BatchLoader(load_batches)\n",
    "            bg_augmenter = ia.BackgroundAugmenter(batch_loader=batch_loader, augseq=transform, nb_workers=1)\n",
    "\n",
    "            while True:\n",
    "                batch = bg_augmenter.get_batch()\n",
    "                if batch is None:\n",
    "                    print(\"Finished epoch.\")\n",
    "                    break\n",
    "                x_images_aug = batch.images_aug\n",
    "                y_images = batch.data\n",
    "                loss, accu, _, _ = sess.run([obj, model.accu_dict[cur_task], train_op, model.centers_update_op], \n",
    "                                         feed_dict={model.x: x_images_aug,\n",
    "                                                    model.y: y_images,\n",
    "                                                    model.is_train: True,\n",
    "                                                    model.bn_train: False})\n",
    "                train_loss += loss\n",
    "                train_accu += accu\n",
    "            batch_loader.terminate()\n",
    "            bg_augmenter.terminate()\n",
    "        else:\n",
    "            for i in range(int(Xtrain.shape[0]/batch_size)):\n",
    "                print(\"Training: {0}/{1}\".format(i,ptrain_max), end='\\r')\n",
    "                st = i*batch_size\n",
    "                ed = (i+1)*batch_size\n",
    "                loss, accu, _, _ = sess.run([obj, model.accu_dict[tasks[0]], train_op, model.centers_update_op],\n",
    "                                                    feed_dict={model.x: Xtrain[st:ed,:],\n",
    "                                                               model.y: Ytrain[st:ed,:],\n",
    "                                                               model.is_train: True,\n",
    "                                                               model.bn_train: False})\n",
    "                train_loss += loss\n",
    "                train_accu += accu\n",
    "\n",
    "        train_loss = train_loss/ptrain_max\n",
    "        train_accu = train_accu/ptrain_max\n",
    "\n",
    "\n",
    "        # validation\n",
    "        val_loss, val_accu1, val_accu2 = 0.0, 0.0, 0.0\n",
    "        val_accu_dp = list()\n",
    "        for i in range(int(Xvalid.shape[0]/batch_size)):\n",
    "            print(\"Validating: {0}/{1}\".format(i,pval_max), end='\\r')\n",
    "            st = i*batch_size\n",
    "            ed = (i+1)*batch_size\n",
    "            loss, accu1, accu2, accu_dp = sess.run([obj, model.accu_dict[tasks[0]], model.accu_dict[tasks[-1]], tracking],\n",
    "                                                feed_dict={model.x: Xvalid[st:ed,:],\n",
    "                                                           model.y: Yvalid[st:ed,:],\n",
    "                                                           model.is_train: False,\n",
    "                                                           model.bn_train: False})\n",
    "            val_loss += loss\n",
    "            val_accu1 += accu1\n",
    "            val_accu2 += accu2\n",
    "            val_accu_dp.append(accu_dp)\n",
    "            \n",
    "        val_accu_dp = np.mean(val_accu_dp, axis=0).tolist()\n",
    "        dp_str = \"\"\n",
    "        for i in range(len(tasks)):\n",
    "            dp_str += \"{0}%:{1}, \".format(tasks[i], np.round(val_accu_dp[i],4))\n",
    "        \n",
    "        print(dp_str)\n",
    "        val_loss = val_loss/pval_max\n",
    "        val_accu1 = val_accu1/pval_max\n",
    "        val_accu2 = val_accu2/pval_max\n",
    "        val_accu = val_accu1 # used for early stopping\n",
    "        \n",
    "        # early stopping check\n",
    "        if (val_accu - current_best_val_accu) > min_delta:\n",
    "            current_best_val_accu = val_accu\n",
    "            patience_counter = 0\n",
    "\n",
    "            para_dict = sess.run(model.para_dict)\n",
    "            np.save(os.path.join(FLAG_save_dir, \"para_dict.npy\"), para_dict)\n",
    "            print(\"save in %s\" % os.path.join(FLAG_save_dir, \"para_dict.npy\"))\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        # shuffle Xtrain and Ytrain in the next epoch\n",
    "        idx = np.random.permutation(Xtrain.shape[0])\n",
    "        Xtrain, Ytrain = Xtrain[idx,:,:,:], Ytrain[idx,:]\n",
    "\n",
    "        # epoch end\n",
    "        epoch_counter += 1\n",
    "\n",
    "        print(\"Epoch %s (%s), %s sec >> train loss: %.4f, train accu: %.4f, val loss: %.4f, val accu at %s: %.4f, val accu at %s: %.4f\" % (epoch_counter, patience_counter, round(time.time()-stime,2), train_loss, train_accu, val_loss, tasks[0], val_accu1, tasks[-1], val_accu2))\n",
    "        history.append([train_loss, train_accu, val_loss, val_accu ])\n",
    "        \n",
    "        if epoch_counter % 10 == 0:\n",
    "            import matplotlib.pyplot as plt\n",
    "            df = pd.DataFrame(history)\n",
    "            df.columns = ['train_loss', 'train_accu', 'val_loss', 'val_accu']\n",
    "            df[['train_loss', 'val_loss']].plot()\n",
    "            plt.savefig(os.path.join(FLAG_save_dir, 'loss.png'))\n",
    "            plt.close()\n",
    "            df[['train_accu', 'val_accu']].plot()\n",
    "            plt.savefig(os.path.join(FLAG_save_dir, 'accu.png'))\n",
    "            plt.close()\n",
    "            \n",
    "    saver.save(sess, checkpoint_path, global_step=epoch_counter)\n",
    "    \n",
    "    # extract features and calculate center\n",
    "\n",
    "    output = []\n",
    "    for i in range(int(Xtrain.shape[0]/200+1)):\n",
    "        print(i, end=\"\\r\")\n",
    "        st = i*200\n",
    "        ed = min((i+1)*200, Xtrain.shape[0])\n",
    "        prob = sess.run(model.features, feed_dict={model.x: Xtrain[st:ed,:], \n",
    "                                                   model.is_train: False,\n",
    "                                                   model.bn_train: False})\n",
    "        output.append(prob)\n",
    "\n",
    "    for i in range(int(Xvalid.shape[0]/200+1)):\n",
    "        print(i, end=\"\\r\")\n",
    "        st = i*200\n",
    "        ed = min((i+1)*200, Xvalid.shape[0])\n",
    "        prob = sess.run(model.features, feed_dict={model.x: Xvalid[st:ed,:], \n",
    "                                                   model.is_train: False,\n",
    "                                                   model.bn_train: False})\n",
    "        output.append(prob)\n",
    "\n",
    "    EX = np.concatenate(output)\n",
    "    print(EX.shape)\n",
    "    EY = np.concatenate([ytrain, yvalid])\n",
    "    print(EY.shape)\n",
    "    centers = np.zeros((len(y_dict), EX.shape[1]))\n",
    "    for i in range(len(y_dict)):\n",
    "        centers[i,:] = np.mean(EX[EY==i,:], axis=0)\n",
    "        np.save(arr=centers,file=os.path.join(FLAG_save_dir,\"centers.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
