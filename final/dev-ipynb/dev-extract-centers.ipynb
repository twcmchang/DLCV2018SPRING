{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='6'\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from VGG16_GAP import VGG16_GAP\n",
    "from VGG16_flatten import VGG16_flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import skimage.io as imageio\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from progress.bar import Bar\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('save/label_dict.pkl', 'rb') as f:\n",
    "    y_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('save/inv_label_dict.pkl', 'rb') as f:\n",
    "    inv_y_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOME_DIR = \"/home/cmchang/DLCV2018SPRING/final/\"\n",
    "TRAIN_DIR = HOME_DIR+\"dlcv_final_2_dataset/train/\"\n",
    "VALID_DIR = HOME_DIR+\"dlcv_final_2_dataset/val/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = pd.read_csv(HOME_DIR+\"dlcv_final_2_dataset/train_id.txt\", header=None,sep=\" \", names=[\"img\", \"id\"])\n",
    "dvalid = pd.read_csv(HOME_DIR+\"dlcv_final_2_dataset/val_id.txt\", header=None,sep=\" \", names=[\"img\", \"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list = list(TRAIN_DIR+dtrain.img)\n",
    "valid_list = list(VALID_DIR+dvalid.img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readImgList(file_list):\n",
    "    images = list()\n",
    "    for i, file in enumerate(file_list):\n",
    "        print(i, end=\"\\r\")\n",
    "        img = imageio.imread(file)\n",
    "        img = img.astype(int)\n",
    "        images.append(img)\n",
    "    return np.array(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformLabel(id_list, y_dict):\n",
    "    label = list()\n",
    "    for uid in list(id_list):\n",
    "        label.append(y_dict[uid])\n",
    "    return np.array(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(class_numbers, num_classes):\n",
    "    return np.eye(num_classes, dtype=float)[class_numbers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_uninitialized(sess):\n",
    "    global_vars = tf.global_variables()\n",
    "    is_not_initialized = sess.run([tf.is_variable_initialized(var) for var in global_vars])\n",
    "    not_initialized_vars = [v for (v,f) in zip(global_vars, is_not_initialized) if not f]\n",
    "    if len(not_initialized_vars): \n",
    "            sess.run(tf.variables_initializer(not_initialized_vars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = readImgList(train_list)\n",
    "print(\"train:\", Xtrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xvalid = readImgList(valid_list)\n",
    "print(\"valid:\", Xtrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain = transformLabel(list(dtrain.id), y_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yvalid = transformLabel(list(dvalid.id), y_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ytrain = one_hot_encoding(ytrain, len(y_dict))\n",
    "Yvalid = one_hot_encoding(yvalid, len(y_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scope_name = \"Model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGG16_GAP(scope_name=scope_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLAG_save_dir = \"/home/cmchang/DLCV2018SPRING/final/newCL_lambda-1e-1_dynamic_gap_L5_v3_rescale0-1_save_linear/\"\n",
    "FLAG_init_from = FLAG_save_dir + \"para_dict.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build(vgg16_npy_path=FLAG_init_from,\n",
    "            shape=Xtrain.shape[1:],\n",
    "            classes=len(y_dict),\n",
    "            conv_pre_training=True,\n",
    "            fc_pre_training=True,\n",
    "            new_bn=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp = [1.0]\n",
    "model.set_idp_operation(dp=dp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_number_params(para_dict):\n",
    "    n = 0\n",
    "    for k,v in sorted(para_dict.items()):\n",
    "        if 'bn_mean' in k:\n",
    "            continue\n",
    "        elif 'bn_variance' in k:\n",
    "            continue\n",
    "        elif 'gamma' in k:\n",
    "            continue\n",
    "        elif 'beta' in k:\n",
    "            continue\n",
    "        elif 'conv' in k or 'fc' in k:\n",
    "            n += get_params_shape(v[0].shape.as_list())\n",
    "            n += get_params_shape(v[1].shape.as_list())\n",
    "    return n\n",
    "\n",
    "def get_params_shape(shape):\n",
    "    n = 1\n",
    "    for dim in shape:\n",
    "        n = n*dim\n",
    "    return n\n",
    "\n",
    "def count_flops(para_dict, net_shape):\n",
    "    input_shape = (3 ,32 ,32) # Format:(channels, rows,cols)\n",
    "    total_flops_per_layer = 0\n",
    "    input_count = 0\n",
    "    for k,v in sorted(para_dict.items()):\n",
    "        if 'bn_mean' in k:\n",
    "            continue\n",
    "        elif 'bn_variance' in k:\n",
    "            continue\n",
    "        elif 'gamma' in k:\n",
    "            continue\n",
    "        elif 'beta' in k:\n",
    "            continue\n",
    "        elif 'fc' in k:\n",
    "            continue\n",
    "        elif 'conv' in k:\n",
    "            conv_filter = v[0].shape.as_list()[3::-1] # (64 ,3 ,3 ,3)  # Format: (num_filters, channels, rows, cols)\n",
    "            stride = 1\n",
    "            padding = 1\n",
    "\n",
    "            if conv_filter[1] == 0:\n",
    "                n = conv_filter[2] * conv_filter[3] # vector_length\n",
    "            else:\n",
    "                n = conv_filter[1] * conv_filter[2] * conv_filter[3]  # vector_length\n",
    "\n",
    "            flops_per_instance = n + ( n -1)    # general defination for number of flops (n: multiplications and n-1: additions)\n",
    "\n",
    "            num_instances_per_filter = (( input_shape[1] - conv_filter[2] + 2 * padding) / stride) + 1  # for rows\n",
    "            num_instances_per_filter *= ((input_shape[1] - conv_filter[2] + 2 * padding) / stride) + 1  # multiplying with cols\n",
    "\n",
    "            flops_per_filter = num_instances_per_filter * flops_per_instance\n",
    "            total_flops_per_layer += flops_per_filter * conv_filter[0]  # multiply with number of filters\n",
    "\n",
    "            total_flops_per_layer += conv_filter[0] * input_shape[1] * input_shape[2]\n",
    "\n",
    "            input_shape = net_shape[input_count].as_list()[3:0:-1]\n",
    "            input_count +=1\n",
    "\n",
    "    total_flops_per_layer += net_shape[-1].as_list()[1] *2360*2\n",
    "    return total_flops_per_layer\n",
    "\n",
    "def countFlopsParas(net):\n",
    "    total_flops = count_flops(net.para_dict, net.net_shape)\n",
    "    if total_flops / 1e9 > 1:   # for Giga Flops\n",
    "        print(total_flops/ 1e9 ,'{}'.format('GFlops'))\n",
    "    else:\n",
    "        print(total_flops / 1e6 ,'{}'.format('MFlops'))\n",
    "\n",
    "    total_params = count_number_params(net.para_dict)\n",
    "\n",
    "    if total_params / 1e9 > 1:   # for Giga Flops\n",
    "        print(total_params/ 1e9 ,'{}'.format('G'))\n",
    "    else:\n",
    "        print(total_params / 1e6 ,'{}'.format('M'))\n",
    "    \n",
    "    return total_flops, total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flops, params = countFlopsParas(model)\n",
    "FLAG_flops_M = flops/1e6\n",
    "FLAG_params_M = params/1e6\n",
    "print(\"Flops: %3f M, Paras: %3f M\" % (flops/1e6, params/1e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.global_variables())\n",
    "    print(\"Initialized\")\n",
    "    output = []\n",
    "    for i in range(int(Xtrain.shape[0]/200+1)):\n",
    "        print(i, end=\"\\r\")\n",
    "        st = i*200\n",
    "        ed = min((i+1)*200, Xtrain.shape[0])\n",
    "        prob = sess.run(model.features, feed_dict={model.x: Xtrain[st:ed,:], \n",
    "                                                    model.is_train: False})\n",
    "        output.append(prob)\n",
    "\n",
    "    for i in range(int(Xvalid.shape[0]/200+1)):\n",
    "        print(i, end=\"\\r\")\n",
    "        st = i*200\n",
    "        ed = min((i+1)*200, Xvalid.shape[0])\n",
    "        prob = sess.run(model.features, feed_dict={model.x: Xvalid[st:ed,:], \n",
    "                                                    model.is_train: False})\n",
    "        output.append(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EX = np.concatenate(output,)\n",
    "print(EX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.concatenate([ytrain, yvalid])\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers = np.zeros((len(y_dict), EX.shape[1]))\n",
    "for i in range(len(y_dict)):\n",
    "    centers[i,:] = np.mean(EX[Y==i,:], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(arr=centers,file=FLAG_save_dir+\"centers.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
