{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='1'\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from VGG16_GAP import VGG16_GAP\n",
    "from VGG16_flatten import VGG16_flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import skimage.io as imageio\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from progress.bar import Bar\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('save/label_dict.pkl', 'rb') as f:\n",
    "    y_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOME_DIR = \"/home/cmchang/DLCV2018SPRING/final/\"\n",
    "TRAIN_DIR = HOME_DIR+\"dlcv_final_2_dataset/train/\"\n",
    "VALID_DIR = HOME_DIR+\"dlcv_final_2_dataset/val/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = pd.read_csv(HOME_DIR+\"dlcv_final_2_dataset/train_id.txt\", header=None,sep=\" \", names=[\"img\", \"id\"])\n",
    "dvalid = pd.read_csv(HOME_DIR+\"dlcv_final_2_dataset/val_id.txt\", header=None,sep=\" \", names=[\"img\", \"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list = list(TRAIN_DIR+dtrain.img)\n",
    "valid_list = list(VALID_DIR+dvalid.img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readImgList(file_list):\n",
    "    images = list()\n",
    "    for i, file in enumerate(file_list):\n",
    "        print(i, end=\"\\r\")\n",
    "        img = imageio.imread(file)\n",
    "        img = img.astype(int)\n",
    "        images.append(img)\n",
    "    return np.array(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformLabel(id_list, y_dict):\n",
    "    label = list()\n",
    "    for uid in list(id_list):\n",
    "        label.append(y_dict[uid])\n",
    "    return np.array(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(class_numbers, num_classes):\n",
    "    return np.eye(num_classes, dtype=float)[class_numbers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_uninitialized(sess):\n",
    "    global_vars = tf.global_variables()\n",
    "    is_not_initialized = sess.run([tf.is_variable_initialized(var) for var in global_vars])\n",
    "    not_initialized_vars = [v for (v,f) in zip(global_vars, is_not_initialized) if not f]\n",
    "    if len(not_initialized_vars): \n",
    "            sess.run(tf.variables_initializer(not_initialized_vars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56474\r"
     ]
    }
   ],
   "source": [
    "Xtrain = readImgList(train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7210\r"
     ]
    }
   ],
   "source": [
    "Xvalid = readImgList(valid_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56475, 218, 178, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain = transformLabel(list(dtrain.id), y_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "yvalid = transformLabel(list(dvalid.id), y_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ytrain = one_hot_encoding(ytrain, len(y_dict))\n",
    "Yvalid = one_hot_encoding(yvalid, len(y_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "scope_name = \"Model2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGG16_GAP(scope_name=scope_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLAG_init_from = HOME_DIR+\"finetune_gap_v3_rescale0-1_save_linear/sparse_dict.npy\"\n",
    "FLAG_prof_type = \"linear\"\n",
    "FLAG_lambda_s = 4e-3\n",
    "FLAG_lambda_m = 2e-5\n",
    "FLAG_decay = 1e-5\n",
    "FLAG_lr = 1e-4\n",
    "FLAG_keep_prob = 1.0\n",
    "FLAG_save_dir = HOME_DIR+\"dynamic_gap_v3_rescale0-1_save_linear/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build model started\n",
      "npy file loaded\n",
      "build model finished: 0s\n"
     ]
    }
   ],
   "source": [
    "model.build(vgg16_npy_path=FLAG_init_from,\n",
    "            shape=Xtrain.shape[1:],\n",
    "            classes=len(y_dict),\n",
    "            prof_type=FLAG_prof_type,\n",
    "            conv_pre_training=True,\n",
    "            fc_pre_training=True,\n",
    "            new_bn=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DP under test: [1.  0.5]\n",
      "[None, 218, 178, 3]\n",
      "[None, 218, 178, 29]\n",
      "AFTER [None, 218, 178, 29]\n",
      "[None, 109, 89, 32]\n",
      "AFTER [None, 109, 89, 32]\n",
      "[None, 109, 89, 39]\n",
      "AFTER [None, 109, 89, 39]\n",
      "[None, 55, 45, 66]\n",
      "AFTER [None, 55, 45, 66]\n",
      "[None, 55, 45, 79]\n",
      "AFTER [None, 55, 45, 79]\n",
      "[None, 55, 45, 99]\n",
      "AFTER [None, 55, 45, 99]\n",
      "[None, 28, 23, 85]\n",
      "AFTER [None, 28, 23, 85]\n",
      "[None, 28, 23, 88]\n",
      "AFTER [None, 28, 23, 88]\n",
      "[None, 28, 23, 80]\n",
      "AFTER [None, 28, 23, 80]\n",
      "[None, 14, 12, 100]\n",
      "AFTER [None, 14, 12, 100]\n",
      "[None, 14, 12, 85]\n",
      "AFTER [None, 14, 12, 85]\n",
      "[None, 14, 12, 108]\n",
      "AFTER [None, 14, 12, 108]\n",
      "[None, 218, 178, 3]\n",
      "[None, 218, 178, 29]\n",
      "AFTER [None, 218, 178, 14]\n",
      "[None, 109, 89, 32]\n",
      "AFTER [None, 109, 89, 16]\n",
      "[None, 109, 89, 39]\n",
      "AFTER [None, 109, 89, 19]\n",
      "[None, 55, 45, 66]\n",
      "AFTER [None, 55, 45, 33]\n",
      "[None, 55, 45, 79]\n",
      "AFTER [None, 55, 45, 39]\n",
      "[None, 55, 45, 99]\n",
      "AFTER [None, 55, 45, 49]\n",
      "[None, 28, 23, 85]\n",
      "AFTER [None, 28, 23, 42]\n",
      "[None, 28, 23, 88]\n",
      "AFTER [None, 28, 23, 44]\n",
      "[None, 28, 23, 80]\n",
      "AFTER [None, 28, 23, 40]\n",
      "[None, 14, 12, 100]\n",
      "AFTER [None, 14, 12, 50]\n",
      "[None, 14, 12, 85]\n",
      "AFTER [None, 14, 12, 42]\n",
      "[None, 14, 12, 108]\n",
      "AFTER [None, 14, 12, 54]\n",
      "Set dp operations finished: 1s\n"
     ]
    }
   ],
   "source": [
    "dp = [1.0, 0.5]\n",
    "tasks = [str(int(p*100)) for p in dp]\n",
    "model.set_idp_operation(dp=dp, decay=FLAG_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "obj = 0.0\n",
    "for cur_task in tasks:\n",
    "    print(cur_task)\n",
    "    obj += model.loss_dict[cur_task]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imgaug as ia\n",
    "from imgaug import augmenters as iaa\n",
    "sometimes = lambda aug: iaa.Sometimes(0.5, aug)\n",
    "transform = iaa.Sequential([\n",
    "    sometimes(iaa.Affine(translate_percent={\"x\": (-0.15, 0.15), \"y\": (-0.15, 0.15)})),\n",
    "    sometimes(iaa.Affine(scale={\"x\": (0.85, 1.15), \"y\":(0.85, 1.15)})),\n",
    "    sometimes(iaa.Affine(rotate=(-45, 45))),\n",
    "    sometimes(iaa.Fliplr(0.5))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== create directory =====\n",
      "<tf.Variable 'Model2/conv1_1_gamma:0' shape=(29,) dtype=float32_ref> is not trainable.\n",
      "<tf.Variable 'Model2/conv1_2_gamma:0' shape=(32,) dtype=float32_ref> is not trainable.\n",
      "<tf.Variable 'Model2/conv2_1_gamma:0' shape=(39,) dtype=float32_ref> is not trainable.\n",
      "<tf.Variable 'Model2/conv2_2_gamma:0' shape=(66,) dtype=float32_ref> is not trainable.\n",
      "<tf.Variable 'Model2/conv3_1_gamma:0' shape=(79,) dtype=float32_ref> is not trainable.\n",
      "<tf.Variable 'Model2/conv3_2_gamma:0' shape=(99,) dtype=float32_ref> is not trainable.\n",
      "<tf.Variable 'Model2/conv3_3_gamma:0' shape=(85,) dtype=float32_ref> is not trainable.\n",
      "<tf.Variable 'Model2/conv4_1_gamma:0' shape=(88,) dtype=float32_ref> is not trainable.\n",
      "<tf.Variable 'Model2/conv4_2_gamma:0' shape=(80,) dtype=float32_ref> is not trainable.\n",
      "<tf.Variable 'Model2/conv4_3_gamma:0' shape=(100,) dtype=float32_ref> is not trainable.\n",
      "<tf.Variable 'Model2/conv5_1_gamma:0' shape=(85,) dtype=float32_ref> is not trainable.\n",
      "<tf.Variable 'Model2/conv5_2_gamma:0' shape=(108,) dtype=float32_ref> is not trainable.\n",
      "<tf.Variable 'Model2/conv5_3_gamma:0' shape=(413,) dtype=float32_ref> is not trainable.\n",
      "<tf.Variable 'Model2/conv1_1_bn_mean:0' shape=(29,) dtype=float32_ref> is not trainable.\n",
      "<tf.Variable 'Model2/conv1_2_bn_mean:0' shape=(32,) dtype=float32_ref> is not trainable.\n",
      "<tf.Variable 'Model2/conv2_1_bn_mean:0' shape=(39,) dtype=float32_ref> is not trainable.\n",
      "<tf.Variable 'Model2/conv2_2_bn_mean:0' shape=(66,) dtype=float32_ref> is not trainable.\n",
      "<tf.Variable 'Model2/conv3_1_bn_mean:0' shape=(79,) dtype=float32_ref> is not trainable.\n",
      "<tf.Variable 'Model2/conv3_2_bn_mean:0' shape=(99,) dtype=float32_ref> is not trainable.\n",
      "<tf.Variable 'Model2/conv3_3_bn_mean:0' shape=(85,) dtype=float32_ref> is not trainable.\n",
      "<tf.Variable 'Model2/conv4_1_bn_mean:0' shape=(88,) dtype=float32_ref> is not trainable.\n",
      "<tf.Variable 'Model2/conv4_2_bn_mean:0' shape=(80,) dtype=float32_ref> is not trainable.\n",
      "<tf.Variable 'Model2/conv4_3_bn_mean:0' shape=(100,) dtype=float32_ref> is not trainable.\n",
      "<tf.Variable 'Model2/conv5_1_bn_mean:0' shape=(85,) dtype=float32_ref> is not trainable.\n",
      "<tf.Variable 'Model2/conv5_2_bn_mean:0' shape=(108,) dtype=float32_ref> is not trainable.\n",
      "<tf.Variable 'Model2/conv5_3_bn_mean:0' shape=(413,) dtype=float32_ref> is not trainable.\n",
      "<tf.Variable 'Model2/conv1_1_beta:0' shape=(29,) dtype=float32_ref> is not trainable.\n",
      "<tf.Variable 'Model2/conv1_2_beta:0' shape=(32,) dtype=float32_ref> is not trainable.\n",
      "<tf.Variable 'Model2/conv2_1_beta:0' shape=(39,) dtype=float32_ref> is not trainable.\n",
      "<tf.Variable 'Model2/conv2_2_beta:0' shape=(66,) dtype=float32_ref> is not trainable.\n",
      "<tf.Variable 'Model2/conv3_1_beta:0' shape=(79,) dtype=float32_ref> is not trainable.\n",
      "<tf.Variable 'Model2/conv3_2_beta:0' shape=(99,) dtype=float32_ref> is not trainable.\n",
      "<tf.Variable 'Model2/conv3_3_beta:0' shape=(85,) dtype=float32_ref> is not trainable.\n",
      "<tf.Variable 'Model2/conv4_1_beta:0' shape=(88,) dtype=float32_ref> is not trainable.\n",
      "<tf.Variable 'Model2/conv4_2_beta:0' shape=(80,) dtype=float32_ref> is not trainable.\n",
      "<tf.Variable 'Model2/conv4_3_beta:0' shape=(100,) dtype=float32_ref> is not trainable.\n",
      "<tf.Variable 'Model2/conv5_1_beta:0' shape=(85,) dtype=float32_ref> is not trainable.\n",
      "<tf.Variable 'Model2/conv5_2_beta:0' shape=(108,) dtype=float32_ref> is not trainable.\n",
      "<tf.Variable 'Model2/conv5_3_beta:0' shape=(413,) dtype=float32_ref> is not trainable.\n",
      "<tf.Variable 'Model2/conv1_1_bn_variance:0' shape=(29,) dtype=float32_ref> is not trainable.\n",
      "<tf.Variable 'Model2/conv1_2_bn_variance:0' shape=(32,) dtype=float32_ref> is not trainable.\n",
      "<tf.Variable 'Model2/conv2_1_bn_variance:0' shape=(39,) dtype=float32_ref> is not trainable.\n",
      "<tf.Variable 'Model2/conv2_2_bn_variance:0' shape=(66,) dtype=float32_ref> is not trainable.\n",
      "<tf.Variable 'Model2/conv3_1_bn_variance:0' shape=(79,) dtype=float32_ref> is not trainable.\n",
      "<tf.Variable 'Model2/conv3_2_bn_variance:0' shape=(99,) dtype=float32_ref> is not trainable.\n",
      "<tf.Variable 'Model2/conv3_3_bn_variance:0' shape=(85,) dtype=float32_ref> is not trainable.\n",
      "<tf.Variable 'Model2/conv4_1_bn_variance:0' shape=(88,) dtype=float32_ref> is not trainable.\n",
      "<tf.Variable 'Model2/conv4_2_bn_variance:0' shape=(80,) dtype=float32_ref> is not trainable.\n",
      "<tf.Variable 'Model2/conv4_3_bn_variance:0' shape=(100,) dtype=float32_ref> is not trainable.\n",
      "<tf.Variable 'Model2/conv5_1_bn_variance:0' shape=(85,) dtype=float32_ref> is not trainable.\n",
      "<tf.Variable 'Model2/conv5_2_bn_variance:0' shape=(108,) dtype=float32_ref> is not trainable.\n",
      "<tf.Variable 'Model2/conv5_3_bn_variance:0' shape=(413,) dtype=float32_ref> is not trainable.\n",
      "[<tf.Variable 'Model2/conv1_1_W:0' shape=(3, 3, 3, 29) dtype=float32_ref>, <tf.Variable 'Model2/conv1_1_b:0' shape=(29,) dtype=float32_ref>, <tf.Variable 'Model2/conv1_2_W:0' shape=(3, 3, 29, 32) dtype=float32_ref>, <tf.Variable 'Model2/conv1_2_b:0' shape=(32,) dtype=float32_ref>, <tf.Variable 'Model2/conv2_1_W:0' shape=(3, 3, 32, 39) dtype=float32_ref>, <tf.Variable 'Model2/conv2_1_b:0' shape=(39,) dtype=float32_ref>, <tf.Variable 'Model2/conv2_2_W:0' shape=(3, 3, 39, 66) dtype=float32_ref>, <tf.Variable 'Model2/conv2_2_b:0' shape=(66,) dtype=float32_ref>, <tf.Variable 'Model2/conv3_1_W:0' shape=(3, 3, 66, 79) dtype=float32_ref>, <tf.Variable 'Model2/conv3_1_b:0' shape=(79,) dtype=float32_ref>, <tf.Variable 'Model2/conv3_2_W:0' shape=(3, 3, 79, 99) dtype=float32_ref>, <tf.Variable 'Model2/conv3_2_b:0' shape=(99,) dtype=float32_ref>, <tf.Variable 'Model2/conv3_3_W:0' shape=(3, 3, 99, 85) dtype=float32_ref>, <tf.Variable 'Model2/conv3_3_b:0' shape=(85,) dtype=float32_ref>, <tf.Variable 'Model2/conv4_1_W:0' shape=(3, 3, 85, 88) dtype=float32_ref>, <tf.Variable 'Model2/conv4_1_b:0' shape=(88,) dtype=float32_ref>, <tf.Variable 'Model2/conv4_2_W:0' shape=(3, 3, 88, 80) dtype=float32_ref>, <tf.Variable 'Model2/conv4_2_b:0' shape=(80,) dtype=float32_ref>, <tf.Variable 'Model2/conv4_3_W:0' shape=(3, 3, 80, 100) dtype=float32_ref>, <tf.Variable 'Model2/conv4_3_b:0' shape=(100,) dtype=float32_ref>, <tf.Variable 'Model2/conv5_1_W:0' shape=(3, 3, 100, 85) dtype=float32_ref>, <tf.Variable 'Model2/conv5_1_b:0' shape=(85,) dtype=float32_ref>, <tf.Variable 'Model2/conv5_2_W:0' shape=(3, 3, 85, 108) dtype=float32_ref>, <tf.Variable 'Model2/conv5_2_b:0' shape=(108,) dtype=float32_ref>, <tf.Variable 'Model2/conv5_3_W:0' shape=(3, 3, 108, 413) dtype=float32_ref>, <tf.Variable 'Model2/conv5_3_b:0' shape=(413,) dtype=float32_ref>, <tf.Variable 'Model2/fc_2_W:0' shape=(413, 2360) dtype=float32_ref>, <tf.Variable 'Model2/fc_2_b:0' shape=(2360,) dtype=float32_ref>]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb1fb8154ee2463589919b6cdfa00c6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>IntProgress</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "IntProgress(value=0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "461acfcc20ed462da9b33b63bf5c8c40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>IntProgress</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "IntProgress(value=0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial spareness: 0.0\n",
      "\n",
      "spareness: 0.0\n",
      "save in /home/cmchang/DLCV2018SPRING/final/dynamic_gap_v3_rescale0-1_save_linear/para_dict.npy\n",
      "Epoch 1 (0), 475.36 sec >> train loss: 14.0934, train accu: 0.0820, val loss: 11.8437, val accu at 100: 0.2526, val accu at 50: 0.0003\n",
      "\n",
      "spareness: 0.0\n",
      "save in /home/cmchang/DLCV2018SPRING/final/dynamic_gap_v3_rescale0-1_save_linear/para_dict.npy\n",
      "Epoch 2 (0), 472.22 sec >> train loss: 10.6258, train accu: 0.4138, val loss: 10.6546, val accu at 100: 0.4381, val accu at 50: 0.0003\n",
      "\n",
      "spareness: 0.0\n",
      "save in /home/cmchang/DLCV2018SPRING/final/dynamic_gap_v3_rescale0-1_save_linear/para_dict.npy\n",
      "Epoch 3 (0), 472.51 sec >> train loss: 9.6416, train accu: 0.5762, val loss: 10.0296, val accu at 100: 0.5312, val accu at 50: 0.0011\n",
      "\n",
      "spareness: 0.0\n",
      "save in /home/cmchang/DLCV2018SPRING/final/dynamic_gap_v3_rescale0-1_save_linear/para_dict.npy\n",
      "Epoch 4 (0), 473.02 sec >> train loss: 9.0392, train accu: 0.6548, val loss: 9.6569, val accu at 100: 0.5683, val accu at 50: 0.0029\n",
      "\n",
      "spareness: 0.0\n",
      "Epoch 5 (1), 473.74 sec >> train loss: 8.5634, train accu: 0.7035, val loss: 9.6332, val accu at 100: 0.5333, val accu at 50: 0.0042\n",
      "\n",
      "spareness: 0.0\n",
      "save in /home/cmchang/DLCV2018SPRING/final/dynamic_gap_v3_rescale0-1_save_linear/para_dict.npy\n",
      "Epoch 6 (0), 473.39 sec >> train loss: 8.1845, train accu: 0.7376, val loss: 9.0291, val accu at 100: 0.5968, val accu at 50: 0.0082\n",
      "\n",
      "spareness: 0.0\n",
      "save in /home/cmchang/DLCV2018SPRING/final/dynamic_gap_v3_rescale0-1_save_linear/para_dict.npy\n",
      "Epoch 7 (0), 474.63 sec >> train loss: 7.8670, train accu: 0.7685, val loss: 8.9842, val accu at 100: 0.6038, val accu at 50: 0.0104\n",
      "\n",
      "spareness: 0.0\n",
      "save in /home/cmchang/DLCV2018SPRING/final/dynamic_gap_v3_rescale0-1_save_linear/para_dict.npy\n",
      "Epoch 8 (0), 471.77 sec >> train loss: 7.5935, train accu: 0.7897, val loss: 8.5150, val accu at 100: 0.6479, val accu at 50: 0.0144\n",
      "\n",
      "spareness: 0.0\n",
      "save in /home/cmchang/DLCV2018SPRING/final/dynamic_gap_v3_rescale0-1_save_linear/para_dict.npy\n",
      "Epoch 9 (0), 473.67 sec >> train loss: 7.3318, train accu: 0.8047, val loss: 8.4340, val accu at 100: 0.6433, val accu at 50: 0.0221\n",
      "\n",
      "spareness: 0.0\n",
      "save in /home/cmchang/DLCV2018SPRING/final/dynamic_gap_v3_rescale0-1_save_linear/para_dict.npy\n",
      "Epoch 10 (0), 470.02 sec >> train loss: 7.0631, train accu: 0.8219, val loss: 8.2354, val accu at 100: 0.6640, val accu at 50: 0.0244\n",
      "\n",
      "spareness: 0.0\n",
      "save in /home/cmchang/DLCV2018SPRING/final/dynamic_gap_v3_rescale0-1_save_linear/para_dict.npy\n",
      "Epoch 11 (0), 470.45 sec >> train loss: 6.8001, train accu: 0.8338, val loss: 7.9756, val accu at 100: 0.6633, val accu at 50: 0.0392\n",
      "\n",
      "spareness: 0.0\n",
      "save in /home/cmchang/DLCV2018SPRING/final/dynamic_gap_v3_rescale0-1_save_linear/para_dict.npy\n",
      "Epoch 12 (0), 472.69 sec >> train loss: 6.5129, train accu: 0.8478, val loss: 7.6689, val accu at 100: 0.6894, val accu at 50: 0.0483\n",
      "\n",
      "spareness: 0.0\n",
      "save in /home/cmchang/DLCV2018SPRING/final/dynamic_gap_v3_rescale0-1_save_linear/para_dict.npy\n",
      "Epoch 13 (0), 472.73 sec >> train loss: 6.2315, train accu: 0.8602, val loss: 7.5955, val accu at 100: 0.6796, val accu at 50: 0.0632\n",
      "\n",
      "spareness: 0.0\n",
      "Epoch 14 (1), 473.0 sec >> train loss: 5.9685, train accu: 0.8659, val loss: 7.5186, val accu at 100: 0.6663, val accu at 50: 0.0747\n",
      "\n",
      "spareness: 0.0\n",
      "save in /home/cmchang/DLCV2018SPRING/final/dynamic_gap_v3_rescale0-1_save_linear/para_dict.npy\n",
      "Epoch 15 (0), 472.55 sec >> train loss: 5.7093, train accu: 0.8776, val loss: 7.4282, val accu at 100: 0.6687, val accu at 50: 0.0883\n",
      "\n",
      "spareness: 0.0\n",
      "save in /home/cmchang/DLCV2018SPRING/final/dynamic_gap_v3_rescale0-1_save_linear/para_dict.npy\n",
      "Epoch 16 (0), 473.43 sec >> train loss: 5.4598, train accu: 0.8852, val loss: 7.1143, val accu at 100: 0.6769, val accu at 50: 0.1104\n",
      "\n",
      "spareness: 0.0\n",
      "Epoch 17 (1), 473.97 sec >> train loss: 5.2301, train accu: 0.8928, val loss: 7.3357, val accu at 100: 0.6101, val accu at 50: 0.1389\n",
      "\n",
      "spareness: 0.0\n",
      "save in /home/cmchang/DLCV2018SPRING/final/dynamic_gap_v3_rescale0-1_save_linear/para_dict.npy\n",
      "Epoch 18 (0), 473.07 sec >> train loss: 5.0241, train accu: 0.8982, val loss: 7.0756, val accu at 100: 0.6653, val accu at 50: 0.1410\n",
      "\n",
      "spareness: 0.0\n",
      "save in /home/cmchang/DLCV2018SPRING/final/dynamic_gap_v3_rescale0-1_save_linear/para_dict.npy\n",
      "Epoch 19 (0), 472.49 sec >> train loss: 4.8264, train accu: 0.9061, val loss: 6.8235, val accu at 100: 0.6682, val accu at 50: 0.1722\n",
      "\n",
      "spareness: 0.0\n",
      "save in /home/cmchang/DLCV2018SPRING/final/dynamic_gap_v3_rescale0-1_save_linear/para_dict.npy\n",
      "Epoch 20 (0), 472.8 sec >> train loss: 4.6405, train accu: 0.9103, val loss: 6.7502, val accu at 100: 0.6633, val accu at 50: 0.1782\n",
      "\n",
      "spareness: 0.0\n",
      "save in /home/cmchang/DLCV2018SPRING/final/dynamic_gap_v3_rescale0-1_save_linear/para_dict.npy\n",
      "Epoch 21 (0), 471.85 sec >> train loss: 4.4600, train accu: 0.9168, val loss: 6.6851, val accu at 100: 0.6767, val accu at 50: 0.1846\n",
      "\n",
      "spareness: 0.0\n",
      "save in /home/cmchang/DLCV2018SPRING/final/dynamic_gap_v3_rescale0-1_save_linear/para_dict.npy\n",
      "Epoch 22 (0), 474.44 sec >> train loss: 4.2953, train accu: 0.9198, val loss: 6.4420, val accu at 100: 0.6958, val accu at 50: 0.2049\n",
      "\n",
      "spareness: 0.0\n",
      "save in /home/cmchang/DLCV2018SPRING/final/dynamic_gap_v3_rescale0-1_save_linear/para_dict.npy\n",
      "Epoch 23 (0), 474.54 sec >> train loss: 4.1272, train accu: 0.9254, val loss: 6.3197, val accu at 100: 0.6935, val accu at 50: 0.2347\n",
      "\n",
      "spareness: 0.0\n",
      "save in /home/cmchang/DLCV2018SPRING/final/dynamic_gap_v3_rescale0-1_save_linear/para_dict.npy\n",
      "Epoch 24 (0), 473.92 sec >> train loss: 3.9775, train accu: 0.9271, val loss: 6.2843, val accu at 100: 0.6892, val accu at 50: 0.2537\n",
      "\n",
      "spareness: 0.0\n",
      "Epoch 25 (1), 471.93 sec >> train loss: 3.8218, train accu: 0.9340, val loss: 6.4543, val accu at 100: 0.6729, val accu at 50: 0.2594\n",
      "\n",
      "spareness: 0.0\n",
      "save in /home/cmchang/DLCV2018SPRING/final/dynamic_gap_v3_rescale0-1_save_linear/para_dict.npy\n",
      "Epoch 26 (0), 471.56 sec >> train loss: 3.6805, train accu: 0.9345, val loss: 6.1312, val accu at 100: 0.6886, val accu at 50: 0.2822\n",
      "\n",
      "spareness: 0.0\n",
      "save in /home/cmchang/DLCV2018SPRING/final/dynamic_gap_v3_rescale0-1_save_linear/para_dict.npy\n",
      "Epoch 27 (0), 472.42 sec >> train loss: 3.5452, train accu: 0.9382, val loss: 6.1479, val accu at 100: 0.6885, val accu at 50: 0.2940\n",
      "\n",
      "spareness: 0.0\n",
      "Epoch 28 (1), 472.77 sec >> train loss: 3.4252, train accu: 0.9407, val loss: 6.0698, val accu at 100: 0.6763, val accu at 50: 0.3028\n",
      "\n",
      "spareness: 0.0\n",
      "save in /home/cmchang/DLCV2018SPRING/final/dynamic_gap_v3_rescale0-1_save_linear/para_dict.npy\n",
      "Epoch 29 (0), 471.39 sec >> train loss: 3.2964, train accu: 0.9447, val loss: 5.9195, val accu at 100: 0.6950, val accu at 50: 0.3226\n",
      "\n",
      "spareness: 0.0\n",
      "Epoch 30 (1), 472.13 sec >> train loss: 3.1852, train accu: 0.9469, val loss: 6.2345, val accu at 100: 0.6533, val accu at 50: 0.3392\n",
      "\n",
      "spareness: 0.0\n",
      "Epoch 31 (2), 472.95 sec >> train loss: 3.0773, train accu: 0.9480, val loss: 6.2631, val accu at 100: 0.6696, val accu at 50: 0.3397\n",
      "\n",
      "spareness: 0.0\n",
      "save in /home/cmchang/DLCV2018SPRING/final/dynamic_gap_v3_rescale0-1_save_linear/para_dict.npy\n",
      "Epoch 32 (0), 473.81 sec >> train loss: 2.9716, train accu: 0.9491, val loss: 6.0047, val accu at 100: 0.6756, val accu at 50: 0.3656\n",
      "\n",
      "spareness: 0.0\n",
      "Epoch 33 (1), 472.33 sec >> train loss: 2.8756, train accu: 0.9526, val loss: 6.0578, val accu at 100: 0.6736, val accu at 50: 0.3618\n",
      "\n",
      "spareness: 0.0\n",
      "Epoch 34 (2), 474.05 sec >> train loss: 2.7892, train accu: 0.9540, val loss: 6.0258, val accu at 100: 0.6687, val accu at 50: 0.3694\n",
      "\n",
      "spareness: 0.0\n",
      "save in /home/cmchang/DLCV2018SPRING/final/dynamic_gap_v3_rescale0-1_save_linear/para_dict.npy\n",
      "Epoch 35 (0), 471.83 sec >> train loss: 2.7025, train accu: 0.9573, val loss: 5.9665, val accu at 100: 0.6768, val accu at 50: 0.3832\n",
      "\n",
      "spareness: 0.0\n",
      "save in /home/cmchang/DLCV2018SPRING/final/dynamic_gap_v3_rescale0-1_save_linear/para_dict.npy\n",
      "Epoch 36 (0), 474.01 sec >> train loss: 2.6213, train accu: 0.9583, val loss: 5.9278, val accu at 100: 0.6839, val accu at 50: 0.3979\n",
      "\n",
      "spareness: 0.0\n",
      "Epoch 37 (1), 471.14 sec >> train loss: 2.5511, train accu: 0.9575, val loss: 5.9787, val accu at 100: 0.6672, val accu at 50: 0.4128\n",
      "\n",
      "spareness: 0.0\n",
      "Epoch 38 (2), 474.28 sec >> train loss: 2.4729, train accu: 0.9605, val loss: 6.3056, val accu at 100: 0.6693, val accu at 50: 0.3692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "spareness: 0.0\n",
      "save in /home/cmchang/DLCV2018SPRING/final/dynamic_gap_v3_rescale0-1_save_linear/para_dict.npy\n",
      "Epoch 39 (0), 471.49 sec >> train loss: 2.4026, train accu: 0.9621, val loss: 5.8466, val accu at 100: 0.6833, val accu at 50: 0.4336\n",
      "\n",
      "spareness: 0.0\n",
      "save in /home/cmchang/DLCV2018SPRING/final/dynamic_gap_v3_rescale0-1_save_linear/para_dict.npy\n",
      "Epoch 40 (0), 473.94 sec >> train loss: 2.3350, train accu: 0.9622, val loss: 5.7228, val accu at 100: 0.6906, val accu at 50: 0.4457\n",
      "\n",
      "spareness: 0.0\n",
      "Epoch 41 (1), 471.18 sec >> train loss: 2.2709, train accu: 0.9638, val loss: 5.7588, val accu at 100: 0.6781, val accu at 50: 0.4432\n",
      "\n",
      "spareness: 0.0\n",
      "Epoch 42 (2), 471.49 sec >> train loss: 2.2127, train accu: 0.9651, val loss: 6.0077, val accu at 100: 0.6840, val accu at 50: 0.3974\n",
      "\n",
      "spareness: 0.0\n",
      "Epoch 43 (3), 471.66 sec >> train loss: 2.1572, train accu: 0.9648, val loss: 6.0978, val accu at 100: 0.6833, val accu at 50: 0.4256\n",
      "\n",
      "spareness: 0.0\n",
      "Epoch 44 (4), 473.49 sec >> train loss: 2.0966, train accu: 0.9666, val loss: 5.9692, val accu at 100: 0.6783, val accu at 50: 0.4390\n",
      "\n",
      "spareness: 0.0\n",
      "save in /home/cmchang/DLCV2018SPRING/final/dynamic_gap_v3_rescale0-1_save_linear/para_dict.npy\n",
      "Epoch 45 (0), 472.93 sec >> train loss: 2.0482, train accu: 0.9678, val loss: 5.7286, val accu at 100: 0.6785, val accu at 50: 0.4671\n",
      "\n",
      "spareness: 0.0\n",
      "Epoch 46 (1), 472.84 sec >> train loss: 1.9947, train accu: 0.9683, val loss: 5.9338, val accu at 100: 0.6760, val accu at 50: 0.4629\n",
      "\n",
      "spareness: 0.0\n",
      "save in /home/cmchang/DLCV2018SPRING/final/dynamic_gap_v3_rescale0-1_save_linear/para_dict.npy\n",
      "Epoch 47 (0), 472.43 sec >> train loss: 1.9502, train accu: 0.9680, val loss: 5.8636, val accu at 100: 0.6689, val accu at 50: 0.4821\n",
      "\n",
      "spareness: 0.0\n",
      "Epoch 48 (1), 472.97 sec >> train loss: 1.8957, train accu: 0.9699, val loss: 5.9304, val accu at 100: 0.6707, val accu at 50: 0.4788\n",
      "\n",
      "spareness: 0.0\n",
      "save in /home/cmchang/DLCV2018SPRING/final/dynamic_gap_v3_rescale0-1_save_linear/para_dict.npy\n",
      "Epoch 49 (0), 472.99 sec >> train loss: 1.8567, train accu: 0.9685, val loss: 6.0002, val accu at 100: 0.6850, val accu at 50: 0.4750\n",
      "\n",
      "spareness: 0.0\n",
      "Epoch 50 (1), 475.04 sec >> train loss: 1.8073, train accu: 0.9707, val loss: 5.9332, val accu at 100: 0.6646, val accu at 50: 0.4813\n",
      "\n",
      "spareness: 0.0\n",
      "save in /home/cmchang/DLCV2018SPRING/final/dynamic_gap_v3_rescale0-1_save_linear/para_dict.npy\n",
      "Epoch 51 (0), 470.13 sec >> train loss: 1.7609, train accu: 0.9736, val loss: 5.8145, val accu at 100: 0.6901, val accu at 50: 0.4892\n",
      "\n",
      "spareness: 0.0\n",
      "save in /home/cmchang/DLCV2018SPRING/final/dynamic_gap_v3_rescale0-1_save_linear/para_dict.npy\n",
      "Epoch 52 (0), 474.21 sec >> train loss: 1.7282, train accu: 0.9720, val loss: 5.6882, val accu at 100: 0.6800, val accu at 50: 0.5003\n",
      "\n",
      "spareness: 0.0\n",
      "save in /home/cmchang/DLCV2018SPRING/final/dynamic_gap_v3_rescale0-1_save_linear/para_dict.npy\n",
      "Epoch 53 (0), 488.58 sec >> train loss: 1.6814, train accu: 0.9727, val loss: 5.8862, val accu at 100: 0.6825, val accu at 50: 0.5056\n",
      "\n",
      "spareness: 0.0\n",
      "save in /home/cmchang/DLCV2018SPRING/final/dynamic_gap_v3_rescale0-1_save_linear/para_dict.npy\n",
      "Epoch 54 (0), 471.12 sec >> train loss: 1.6443, train accu: 0.9737, val loss: 5.8711, val accu at 100: 0.6907, val accu at 50: 0.5067\n",
      "\n",
      "spareness: 0.0\n",
      "Epoch 55 (1), 471.35 sec >> train loss: 1.6096, train accu: 0.9734, val loss: 5.8343, val accu at 100: 0.6840, val accu at 50: 0.4965\n",
      "\n",
      "spareness: 0.0\n",
      "save in /home/cmchang/DLCV2018SPRING/final/dynamic_gap_v3_rescale0-1_save_linear/para_dict.npy\n",
      "Epoch 56 (0), 470.64 sec >> train loss: 1.5686, train accu: 0.9747, val loss: 5.7745, val accu at 100: 0.6914, val accu at 50: 0.5114\n",
      "\n",
      "spareness: 0.0\n",
      "Epoch 57 (1), 470.76 sec >> train loss: 1.5395, train accu: 0.9728, val loss: 6.1907, val accu at 100: 0.6647, val accu at 50: 0.5085\n",
      "\n",
      "spareness: 0.0\n",
      "Epoch 58 (2), 469.16 sec >> train loss: 1.5053, train accu: 0.9748, val loss: 6.0163, val accu at 100: 0.6779, val accu at 50: 0.5126\n",
      "\n",
      "spareness: 0.0\n",
      "Epoch 59 (3), 471.11 sec >> train loss: 1.4685, train accu: 0.9748, val loss: 6.1492, val accu at 100: 0.6651, val accu at 50: 0.5183\n",
      "\n",
      "spareness: 0.0\n",
      "Epoch 60 (4), 469.8 sec >> train loss: 1.4348, train accu: 0.9763, val loss: 5.8776, val accu at 100: 0.6918, val accu at 50: 0.5081\n",
      "\n",
      "spareness: 0.0\n",
      "save in /home/cmchang/DLCV2018SPRING/final/dynamic_gap_v3_rescale0-1_save_linear/para_dict.npy\n",
      "Epoch 61 (0), 471.05 sec >> train loss: 1.4029, train accu: 0.9768, val loss: 5.8975, val accu at 100: 0.6932, val accu at 50: 0.5211\n",
      "\n",
      "spareness: 0.0\n",
      "save in /home/cmchang/DLCV2018SPRING/final/dynamic_gap_v3_rescale0-1_save_linear/para_dict.npy\n",
      "Epoch 62 (0), 469.1 sec >> train loss: 1.3744, train accu: 0.9767, val loss: 5.7555, val accu at 100: 0.7000, val accu at 50: 0.5160\n",
      "\n",
      "spareness: 0.0\n",
      "save in /home/cmchang/DLCV2018SPRING/final/dynamic_gap_v3_rescale0-1_save_linear/para_dict.npy\n",
      "Epoch 63 (0), 472.12 sec >> train loss: 1.3431, train accu: 0.9778, val loss: 6.0439, val accu at 100: 0.6893, val accu at 50: 0.5315\n",
      "\n",
      "spareness: 0.0\n",
      "Epoch 64 (1), 471.02 sec >> train loss: 1.3172, train accu: 0.9763, val loss: 6.2461, val accu at 100: 0.6708, val accu at 50: 0.5142\n",
      "\n",
      "spareness: 0.0\n",
      "save in /home/cmchang/DLCV2018SPRING/final/dynamic_gap_v3_rescale0-1_save_linear/para_dict.npy\n",
      "Epoch 65 (0), 472.24 sec >> train loss: 1.2835, train accu: 0.9784, val loss: 5.8027, val accu at 100: 0.6931, val accu at 50: 0.5324\n"
     ]
    }
   ],
   "source": [
    "print(\"===== create directory =====\")\n",
    "if not os.path.exists(FLAG_save_dir):\n",
    "    os.makedirs(FLAG_save_dir)\n",
    "\n",
    "arr_spareness = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    augment = False\n",
    "\n",
    "   # hyper parameters\n",
    "    batch_size = 32\n",
    "    epoch = 100\n",
    "    early_stop_patience = 10\n",
    "    min_delta = 0.0001\n",
    "    opt_type = 'adam'\n",
    "\n",
    "    # recorder\n",
    "    epoch_counter = 0\n",
    "    history = list()\n",
    "\n",
    "    # optimizer\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "    # Passing global_step to minimize() will increment it at each step.\n",
    "    learning_rate = FLAG_lr\n",
    "    opt = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.5)\n",
    "\n",
    "    checkpoint_path = os.path.join(FLAG_save_dir, 'model.ckpt')\n",
    "    \n",
    "    # trainable variables\n",
    "    train_vars = list()\n",
    "    for var in tf.trainable_variables():\n",
    "        if model.scope_name in var.name:\n",
    "            train_vars.append(var)\n",
    "\n",
    "    for rm in model.gamma_var:\n",
    "        train_vars.remove(rm)\n",
    "        print('%s is not trainable.'% rm)\n",
    "    \n",
    "    for var in train_vars:\n",
    "        if '_mean' in var.name:\n",
    "            train_vars.remove(var)\n",
    "            print('%s is not trainable.'% var)\n",
    "    \n",
    "    for var in train_vars:\n",
    "        if '_beta' in var.name:\n",
    "            train_vars.remove(var)\n",
    "            print('%s is not trainable.'% var)\n",
    "    \n",
    "    for var in train_vars:\n",
    "        if '_variance' in var.name:\n",
    "            train_vars.remove(var)\n",
    "            print('%s is not trainable.'% var)\n",
    "    \n",
    "    print(train_vars)\n",
    "            \n",
    "    train_op = opt.minimize(obj, global_step=global_step, var_list=train_vars)\n",
    "    \n",
    "    saver = tf.train.Saver(tf.global_variables(), max_to_keep=len(tasks))\n",
    "\n",
    "    # progress bar\n",
    "    ptrain = IntProgress()\n",
    "    pval = IntProgress()\n",
    "    display(ptrain)\n",
    "    display(pval)\n",
    "    ptrain.max = int(Xtrain.shape[0]/batch_size)\n",
    "    pval.max = int(Xvalid.shape[0]/batch_size)\n",
    "\n",
    "    spareness = model.spareness(thresh=0.05)\n",
    "    print(\"initial spareness: %s\" % sess.run(spareness))\n",
    "\n",
    "    # re-initialize\n",
    "    initialize_uninitialized(sess)\n",
    "\n",
    "    # reset due to adding a new task\n",
    "    patience_counter = 0\n",
    "    current_best_val_accu = 0\n",
    "\n",
    "    # optimize when the aggregated obj\n",
    "    while(patience_counter < early_stop_patience and epoch_counter < epoch):\n",
    "        \n",
    "        # start training\n",
    "        stime = time.time()\n",
    "        bar_train = Bar('Training', max=int(Xtrain.shape[0]/batch_size), suffix='%(index)d/%(max)d - %(percent).1f%% - %(eta)ds')\n",
    "        bar_val =  Bar('Validation', max=int(Xvalid.shape[0]/batch_size), suffix='%(index)d/%(max)d - %(percent).1f%% - %(eta)ds')\n",
    "        train_loss, train_accu = 0.0, 0.0\n",
    "        \n",
    "        if augment:\n",
    "            def load_batches():\n",
    "                for i in range(int(Xtrain.shape[0]/batch_size)):\n",
    "                    st = i*batch_size\n",
    "                    ed = (i+1)*batch_size\n",
    "                    batch = ia.Batch(images=Xtrain[st:ed,:,:,:], data=Ytrain[st:ed,:])\n",
    "                    yield batch\n",
    "\n",
    "            batch_loader = ia.BatchLoader(load_batches)\n",
    "            bg_augmenter = ia.BackgroundAugmenter(batch_loader=batch_loader, augseq=transform, nb_workers=1)\n",
    "\n",
    "            while True:\n",
    "                batch = bg_augmenter.get_batch()\n",
    "                if batch is None:\n",
    "                    print(\"Finished epoch.\")\n",
    "                    break\n",
    "                x_images_aug = batch.images_aug\n",
    "                y_images = batch.data\n",
    "                loss, accu, _ = sess.run([obj, model.accu_dict[cur_task], train_op], feed_dict={model.x: x_images_aug,\n",
    "                                model.y: y_images,\n",
    "                                model.is_train: True})\n",
    "                bar_train.next()\n",
    "                train_loss += loss\n",
    "                train_accu += accu\n",
    "                ptrain.value +=1\n",
    "                ptrain.description = \"Training %s/%s\" % (ptrain.value, ptrain.max)\n",
    "            batch_loader.terminate()\n",
    "            bg_augmenter.terminate()\n",
    "        else:\n",
    "            for i in range(int(Xtrain.shape[0]/batch_size)):\n",
    "                st = i*batch_size\n",
    "                ed = (i+1)*batch_size\n",
    "                loss, accu, _ = sess.run([obj, model.accu_dict[tasks[0]], train_op],\n",
    "                                                    feed_dict={model.x: Xtrain[st:ed,:],\n",
    "                                                               model.y: Ytrain[st:ed,:],\n",
    "                                                               model.is_train: False})\n",
    "                train_loss += loss\n",
    "                train_accu += accu\n",
    "                ptrain.value +=1\n",
    "                ptrain.description = \"Training %s/%s\" % (ptrain.value, ptrain.max)\n",
    "        \n",
    "        train_loss = train_loss/ptrain.value\n",
    "        train_accu = train_accu/ptrain.value\n",
    "\n",
    "\n",
    "        # validation\n",
    "        val_loss = 0\n",
    "        val_accu1 = 0\n",
    "        val_accu2 = 0\n",
    "        for i in range(int(Xvalid.shape[0]/batch_size)):\n",
    "            st = i*batch_size\n",
    "            ed = (i+1)*batch_size\n",
    "            loss, accu1, accu2 = sess.run([obj, model.accu_dict[tasks[0]], model.accu_dict[tasks[-1]]],\n",
    "                                                feed_dict={model.x: Xvalid[st:ed,:],\n",
    "                                                           model.y: Yvalid[st:ed,:],\n",
    "                                                           model.is_train: False})\n",
    "            val_loss += loss\n",
    "            val_accu1 += accu1\n",
    "            val_accu2 += accu2\n",
    "            pval.value += 1\n",
    "            pval.description = \"Testing %s/%s\" % (pval.value, pval.value)\n",
    "        val_loss = val_loss/pval.value\n",
    "        val_accu1 = val_accu1/pval.value\n",
    "        val_accu2 = val_accu2/pval.value\n",
    "        val_accu = (val_accu1+val_accu2)/2\n",
    "        \n",
    "        print(\"\\nspareness: %s\" % sess.run(spareness))\n",
    "        # early stopping check\n",
    "        if (val_accu - current_best_val_accu) > min_delta:\n",
    "            current_best_val_accu = val_accu\n",
    "            patience_counter = 0\n",
    "\n",
    "            para_dict = sess.run(model.para_dict)\n",
    "            np.save(os.path.join(FLAG_save_dir, \"para_dict.npy\"), para_dict)\n",
    "            print(\"save in %s\" % os.path.join(FLAG_save_dir, \"para_dict.npy\"))\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        # shuffle Xtrain and Ytrain in the next epoch\n",
    "        idx = np.random.permutation(Xtrain.shape[0])\n",
    "        Xtrain, Ytrain = Xtrain[idx,:,:,:], Ytrain[idx,:]\n",
    "\n",
    "        # epoch end\n",
    "        # writer.add_summary(epoch_summary, epoch_counter)\n",
    "        epoch_counter += 1\n",
    "\n",
    "        ptrain.value = 0\n",
    "        pval.value = 0\n",
    "        bar_train.finish()\n",
    "        bar_val.finish()\n",
    "\n",
    "        print(\"Epoch %s (%s), %s sec >> train loss: %.4f, train accu: %.4f, val loss: %.4f, val accu at %s: %.4f, val accu at %s: %.4f\" % (epoch_counter, patience_counter, round(time.time()-stime,2), train_loss, train_accu, val_loss, tasks[0], val_accu1, tasks[-1], val_accu2))\n",
    "        history.append([train_loss, train_accu, val_loss, val_accu ])\n",
    "        \n",
    "        if epoch_counter % 10 == 0:\n",
    "            import matplotlib.pyplot as plt\n",
    "            df = pd.DataFrame(history)\n",
    "            df.columns = ['train_loss', 'train_accu', 'val_loss', 'val_accu']\n",
    "            df[['train_loss', 'val_loss']].plot()\n",
    "            plt.savefig(os.path.join(FLAG_save_dir, 'loss.png'))\n",
    "            df[['train_accu', 'val_accu']].plot()\n",
    "            plt.savefig(os.path.join(FLAG_save_dir, 'accu.png'))\n",
    "            \n",
    "    saver.save(sess, checkpoint_path, global_step=epoch_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
