{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from VGG16_flatten import VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import skimage.io as imageio\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from progress.bar import Bar\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('save/label_dict.pkl', 'rb') as f:\n",
    "    y_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOME_DIR = \"/home/cmchang/DLCV2018SPRING/final/\"\n",
    "TRAIN_DIR = HOME_DIR+\"dlcv_final_2_dataset/train/\"\n",
    "VALID_DIR = HOME_DIR+\"dlcv_final_2_dataset/val/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = pd.read_csv(HOME_DIR+\"dlcv_final_2_dataset/train_id.txt\", header=None,sep=\" \", names=[\"img\", \"id\"])\n",
    "dvalid = pd.read_csv(HOME_DIR+\"dlcv_final_2_dataset/val_id.txt\", header=None,sep=\" \", names=[\"img\", \"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list = list(TRAIN_DIR+dtrain.img)\n",
    "valid_list = list(VALID_DIR+dvalid.img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readImgList(file_list):\n",
    "    images = list()\n",
    "    for i, file in enumerate(file_list):\n",
    "        print(i, end=\"\\r\")\n",
    "        img = imageio.imread(file)\n",
    "        img = img.astype(int)\n",
    "        images.append(img)\n",
    "    return np.array(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformLabel(id_list, y_dict):\n",
    "    label = list()\n",
    "    for uid in list(id_list):\n",
    "        label.append(y_dict[uid])\n",
    "    return np.array(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(class_numbers, num_classes):\n",
    "    return np.eye(num_classes, dtype=float)[class_numbers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_uninitialized(sess):\n",
    "    global_vars = tf.global_variables()\n",
    "    is_not_initialized = sess.run([tf.is_variable_initialized(var) for var in global_vars])\n",
    "    not_initialized_vars = [v for (v,f) in zip(global_vars, is_not_initialized) if not f]\n",
    "    if len(not_initialized_vars): \n",
    "            sess.run(tf.variables_initializer(not_initialized_vars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56474\r"
     ]
    }
   ],
   "source": [
    "Xtrain = readImgList(train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7210\r"
     ]
    }
   ],
   "source": [
    "Xvalid = readImgList(valid_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56475, 218, 178, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain = transformLabel(list(dtrain.id), y_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "yvalid = transformLabel(list(dvalid.id), y_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ytrain = one_hot_encoding(ytrain, len(y_dict))\n",
    "Yvalid = one_hot_encoding(yvalid, len(y_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "scope_name = \"Model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGG16(scope_name=scope_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLAG_init_from = \"updated_keras_vgg16.npy\"\n",
    "FLAG_prof_type = \"all-one\"\n",
    "FLAG_lambda_s = 2e-7\n",
    "FLAG_lambda_m = 2e-7\n",
    "FLAG_decay = 1e-5\n",
    "FLAG_lr = 4e-4\n",
    "FLAG_keep_prob = 1.0\n",
    "FLAG_save_dir = HOME_DIR+\"rescale0-1_v2_save_all-one/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build model started\n",
      "npy file loaded\n",
      "build model finished: 1s\n"
     ]
    }
   ],
   "source": [
    "model.build(vgg16_npy_path=FLAG_init_from,\n",
    "            shape=Xtrain.shape[1:],\n",
    "            classes=len(y_dict),\n",
    "            prof_type=FLAG_prof_type,\n",
    "            conv_pre_training=False,\n",
    "            fc_pre_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 218, 178, 3]\n",
      "[None, 218, 178, 64]\n",
      "AFTER [None, 218, 178, 64]\n",
      "[None, 109, 89, 64]\n",
      "AFTER [None, 109, 89, 64]\n",
      "[None, 109, 89, 128]\n",
      "AFTER [None, 109, 89, 128]\n",
      "[None, 55, 45, 128]\n",
      "AFTER [None, 55, 45, 128]\n",
      "[None, 55, 45, 256]\n",
      "AFTER [None, 55, 45, 256]\n",
      "[None, 55, 45, 256]\n",
      "AFTER [None, 55, 45, 256]\n",
      "[None, 28, 23, 256]\n",
      "AFTER [None, 28, 23, 256]\n",
      "[None, 28, 23, 512]\n",
      "AFTER [None, 28, 23, 512]\n",
      "[None, 28, 23, 512]\n",
      "AFTER [None, 28, 23, 512]\n",
      "[None, 14, 12, 512]\n",
      "AFTER [None, 14, 12, 512]\n",
      "[None, 14, 12, 512]\n",
      "AFTER [None, 14, 12, 512]\n",
      "[None, 14, 12, 512]\n",
      "AFTER [None, 14, 12, 512]\n",
      "sparsity train operation setup: 1s\n"
     ]
    }
   ],
   "source": [
    "model.sparsity_train(l1_gamma=FLAG_lambda_s, l1_gamma_diff=FLAG_lambda_m, decay=FLAG_decay, keep_prob=FLAG_keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imgaug as ia\n",
    "from imgaug import augmenters as iaa\n",
    "sometimes = lambda aug: iaa.Sometimes(0.5, aug)\n",
    "transform = iaa.Sequential([\n",
    "    sometimes(iaa.Affine(translate_percent={\"x\": (-0.15, 0.15), \"y\": (-0.15, 0.15)})),\n",
    "    sometimes(iaa.Affine(scale={\"x\": (0.85, 1.15), \"y\":(0.85, 1.15)})),\n",
    "    sometimes(iaa.Affine(rotate=(-45, 45))),\n",
    "    sometimes(iaa.Fliplr(0.5))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== create directory =====\n",
      "['var_dp']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e090ab7bb9d443f4a2afba46faa23ec0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>IntProgress</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "IntProgress(value=0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e936efb5af10420c833e4686d8374439",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>IntProgress</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in Jupyter Notebook or JupyterLab, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another notebook frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "IntProgress(value=0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial spareness: 0.0\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.0\n",
      "save in /home/cmchang/DLCV2018SPRING/final/rescale0-1_v2_save_all-one/para_dict.npy\n",
      "Epoch 1 (0), 891.2 sec >> train loss: 9.8467, train accu: 0.0015, val loss: 13.4347, val accu at var_dp: 0.0004\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.0\n",
      "Epoch 2 (1), 842.62 sec >> train loss: 9.1615, train accu: 0.0005, val loss: 9.1121, val accu at var_dp: 0.0000\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.0\n",
      "save in /home/cmchang/DLCV2018SPRING/final/rescale0-1_v2_save_all-one/para_dict.npy\n",
      "Epoch 3 (0), 847.87 sec >> train loss: 9.0509, train accu: 0.0005, val loss: 8.9923, val accu at var_dp: 0.0010\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.0\n",
      "save in /home/cmchang/DLCV2018SPRING/final/rescale0-1_v2_save_all-one/para_dict.npy\n",
      "Epoch 4 (0), 847.33 sec >> train loss: 8.8815, train accu: 0.0011, val loss: 8.7208, val accu at var_dp: 0.0018\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.0\n",
      "save in /home/cmchang/DLCV2018SPRING/final/rescale0-1_v2_save_all-one/para_dict.npy\n",
      "Epoch 5 (0), 849.97 sec >> train loss: 8.2654, train accu: 0.0058, val loss: 7.9132, val accu at var_dp: 0.0089\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.0\n",
      "save in /home/cmchang/DLCV2018SPRING/final/rescale0-1_v2_save_all-one/para_dict.npy\n",
      "Epoch 6 (0), 846.28 sec >> train loss: 7.4553, train accu: 0.0195, val loss: 7.2827, val accu at var_dp: 0.0268\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.0\n",
      "save in /home/cmchang/DLCV2018SPRING/final/rescale0-1_v2_save_all-one/para_dict.npy\n",
      "Epoch 7 (0), 850.89 sec >> train loss: 6.7448, train accu: 0.0427, val loss: 6.4704, val accu at var_dp: 0.0645\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.0\n",
      "save in /home/cmchang/DLCV2018SPRING/final/rescale0-1_v2_save_all-one/para_dict.npy\n",
      "Epoch 8 (0), 849.31 sec >> train loss: 6.0678, train accu: 0.0829, val loss: 6.3169, val accu at var_dp: 0.0801\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.0\n",
      "save in /home/cmchang/DLCV2018SPRING/final/rescale0-1_v2_save_all-one/para_dict.npy\n",
      "Epoch 9 (0), 849.92 sec >> train loss: 5.3496, train accu: 0.1427, val loss: 5.6396, val accu at var_dp: 0.1279\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.0\n",
      "save in /home/cmchang/DLCV2018SPRING/final/rescale0-1_v2_save_all-one/para_dict.npy\n",
      "Epoch 10 (0), 847.04 sec >> train loss: 4.7944, train accu: 0.2063, val loss: 4.9556, val accu at var_dp: 0.2130\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.0\n",
      "save in /home/cmchang/DLCV2018SPRING/final/rescale0-1_v2_save_all-one/para_dict.npy\n",
      "Epoch 11 (0), 846.99 sec >> train loss: 4.0962, train accu: 0.3003, val loss: 4.5517, val accu at var_dp: 0.2712\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.0\n",
      "save in /home/cmchang/DLCV2018SPRING/final/rescale0-1_v2_save_all-one/para_dict.npy\n",
      "Epoch 12 (0), 847.6 sec >> train loss: 3.5301, train accu: 0.3945, val loss: 4.5205, val accu at var_dp: 0.2924\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.0\n",
      "save in /home/cmchang/DLCV2018SPRING/final/rescale0-1_v2_save_all-one/para_dict.npy\n",
      "Epoch 13 (0), 850.44 sec >> train loss: 3.1235, train accu: 0.4654, val loss: 4.1311, val accu at var_dp: 0.3708\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.0\n",
      "Epoch 14 (1), 843.59 sec >> train loss: 2.7549, train accu: 0.5407, val loss: 4.3482, val accu at var_dp: 0.3613\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.0\n",
      "save in /home/cmchang/DLCV2018SPRING/final/rescale0-1_v2_save_all-one/para_dict.npy\n",
      "Epoch 15 (0), 851.44 sec >> train loss: 2.4556, train accu: 0.6017, val loss: 4.2721, val accu at var_dp: 0.3878\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.0\n",
      "save in /home/cmchang/DLCV2018SPRING/final/rescale0-1_v2_save_all-one/para_dict.npy\n",
      "Epoch 16 (0), 844.56 sec >> train loss: 2.2596, train accu: 0.6497, val loss: 4.2147, val accu at var_dp: 0.4113\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.0\n",
      "save in /home/cmchang/DLCV2018SPRING/final/rescale0-1_v2_save_all-one/para_dict.npy\n",
      "Epoch 17 (0), 850.55 sec >> train loss: 2.0719, train accu: 0.6930, val loss: 4.4326, val accu at var_dp: 0.4169\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.0\n",
      "save in /home/cmchang/DLCV2018SPRING/final/rescale0-1_v2_save_all-one/para_dict.npy\n",
      "Epoch 18 (0), 849.27 sec >> train loss: 1.8862, train accu: 0.7356, val loss: 4.3611, val accu at var_dp: 0.4231\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.0\n",
      "Epoch 19 (1), 850.56 sec >> train loss: 1.7342, train accu: 0.7745, val loss: 4.7833, val accu at var_dp: 0.4069\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.0\n",
      "save in /home/cmchang/DLCV2018SPRING/final/rescale0-1_v2_save_all-one/para_dict.npy\n",
      "Epoch 20 (0), 847.54 sec >> train loss: 1.6463, train accu: 0.7945, val loss: 4.6034, val accu at var_dp: 0.4312\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.0\n",
      "save in /home/cmchang/DLCV2018SPRING/final/rescale0-1_v2_save_all-one/para_dict.npy\n",
      "Epoch 21 (0), 851.83 sec >> train loss: 1.5663, train accu: 0.8142, val loss: 4.4752, val accu at var_dp: 0.4568\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.0\n",
      "save in /home/cmchang/DLCV2018SPRING/final/rescale0-1_v2_save_all-one/para_dict.npy\n",
      "Epoch 22 (0), 847.4 sec >> train loss: 1.5251, train accu: 0.8244, val loss: 4.3212, val accu at var_dp: 0.4647\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.0\n",
      "Epoch 23 (1), 845.79 sec >> train loss: 1.4278, train accu: 0.8509, val loss: 4.5275, val accu at var_dp: 0.4594\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.0\n",
      "save in /home/cmchang/DLCV2018SPRING/final/rescale0-1_v2_save_all-one/para_dict.npy\n",
      "Epoch 24 (0), 846.48 sec >> train loss: 1.4040, train accu: 0.8553, val loss: 4.4887, val accu at var_dp: 0.4782\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.0\n",
      "save in /home/cmchang/DLCV2018SPRING/final/rescale0-1_v2_save_all-one/para_dict.npy\n",
      "Epoch 25 (0), 849.17 sec >> train loss: 1.3530, train accu: 0.8683, val loss: 4.3510, val accu at var_dp: 0.5032\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.0\n",
      "Epoch 26 (1), 847.93 sec >> train loss: 1.3194, train accu: 0.8758, val loss: 4.6757, val accu at var_dp: 0.4648\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.0\n",
      "Epoch 27 (2), 845.71 sec >> train loss: 1.2975, train accu: 0.8797, val loss: 4.8383, val accu at var_dp: 0.4605\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.0\n",
      "Epoch 28 (3), 846.88 sec >> train loss: 1.2530, train accu: 0.8902, val loss: 4.5277, val accu at var_dp: 0.4947\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.0\n",
      "Epoch 29 (4), 846.28 sec >> train loss: 1.2168, train accu: 0.8987, val loss: 4.5205, val accu at var_dp: 0.4954\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.0\n",
      "Epoch 30 (5), 845.62 sec >> train loss: 1.1744, train accu: 0.9063, val loss: 4.9229, val accu at var_dp: 0.4867\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.0\n",
      "save in /home/cmchang/DLCV2018SPRING/final/rescale0-1_v2_save_all-one/para_dict.npy\n",
      "Epoch 31 (0), 850.28 sec >> train loss: 1.1601, train accu: 0.9082, val loss: 4.3225, val accu at var_dp: 0.5063\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.0\n",
      "Epoch 32 (1), 843.45 sec >> train loss: 1.1418, train accu: 0.9106, val loss: 4.7501, val accu at var_dp: 0.4858\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.0\n",
      "Epoch 33 (2), 849.99 sec >> train loss: 1.1207, train accu: 0.9142, val loss: 4.5702, val accu at var_dp: 0.4845\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.0\n",
      "Epoch 34 (3), 847.4 sec >> train loss: 1.0981, train accu: 0.9185, val loss: 4.6305, val accu at var_dp: 0.4909\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.0\n",
      "save in /home/cmchang/DLCV2018SPRING/final/rescale0-1_v2_save_all-one/para_dict.npy\n",
      "Epoch 35 (0), 850.4 sec >> train loss: 1.0626, train accu: 0.9245, val loss: 4.2836, val accu at var_dp: 0.5336\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.0\n",
      "Epoch 36 (1), 846.83 sec >> train loss: 1.0678, train accu: 0.9217, val loss: 4.5311, val accu at var_dp: 0.5292\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.0\n",
      "Epoch 37 (2), 844.43 sec >> train loss: 1.0430, train accu: 0.9264, val loss: 4.9624, val accu at var_dp: 0.4904\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.0\n",
      "Epoch 38 (3), 847.67 sec >> train loss: 1.0052, train accu: 0.9343, val loss: 4.6506, val accu at var_dp: 0.5278\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.00023674243\n",
      "Epoch 39 (4), 845.44 sec >> train loss: 1.0158, train accu: 0.9292, val loss: 4.5785, val accu at var_dp: 0.5112\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.00023674243\n",
      "Epoch 40 (5), 844.05 sec >> train loss: 0.9782, train accu: 0.9365, val loss: 4.7753, val accu at var_dp: 0.4914\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.00023674243\n",
      "Epoch 41 (6), 846.03 sec >> train loss: 0.9708, train accu: 0.9355, val loss: 4.3361, val accu at var_dp: 0.5279\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.00023674243\n",
      "Epoch 42 (7), 849.62 sec >> train loss: 0.9544, train accu: 0.9385, val loss: 4.4077, val accu at var_dp: 0.5153\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.00023674243\n",
      "Epoch 43 (8), 849.44 sec >> train loss: 0.9408, train accu: 0.9397, val loss: 5.1919, val accu at var_dp: 0.4967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch.\n",
      "\n",
      "spareness: 0.00023674243\n",
      "Epoch 44 (9), 849.25 sec >> train loss: 0.9200, train accu: 0.9423, val loss: 4.3818, val accu at var_dp: 0.5197\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.00023674243\n",
      "Epoch 45 (10), 850.97 sec >> train loss: 0.9088, train accu: 0.9419, val loss: 4.4647, val accu at var_dp: 0.5215\n"
     ]
    }
   ],
   "source": [
    "print(\"===== create directory =====\")\n",
    "if not os.path.exists(FLAG_save_dir):\n",
    "    os.makedirs(FLAG_save_dir)\n",
    "\n",
    "arr_spareness = []\n",
    "\n",
    "# define tasks\n",
    "tasks = ['var_dp']\n",
    "print(tasks)\n",
    "\n",
    "# initial task\n",
    "cur_task = tasks[0]\n",
    "obj = model.loss_dict[tasks[0]]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "   # hyper parameters\n",
    "    batch_size = 64\n",
    "    epoch = 100\n",
    "    early_stop_patience = 10\n",
    "    min_delta = 0.0001\n",
    "    opt_type = 'adam'\n",
    "\n",
    "    # recorder\n",
    "    epoch_counter = 0\n",
    "    history = list()\n",
    "\n",
    "    # optimizer\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "    # Passing global_step to minimize() will increment it at each step.\n",
    "    learning_rate = FLAG_lr # adam # 4e-3 #sgd\n",
    "    opt = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.5)\n",
    "\n",
    "    checkpoint_path = os.path.join(FLAG_save_dir, 'model.ckpt')\n",
    "    tvars_trainable = tf.trainable_variables()\n",
    "\n",
    "    train_vars = list()\n",
    "    for var in tf.trainable_variables():\n",
    "        if model.scope_name in var.name:\n",
    "            train_vars.append(var)\n",
    "            \n",
    "    train_op = opt.minimize(obj, global_step=global_step, var_list=tvars_trainable)\n",
    "    \n",
    "    saver = tf.train.Saver(tf.global_variables(), max_to_keep=len(tasks))\n",
    "\n",
    "    # progress bar\n",
    "    ptrain = IntProgress()\n",
    "    pval = IntProgress()\n",
    "    display(ptrain)\n",
    "    display(pval)\n",
    "    ptrain.max = int(Xtrain.shape[0]/batch_size)\n",
    "    pval.max = int(Xvalid.shape[0]/batch_size)\n",
    "\n",
    "    spareness = model.spareness(thresh=0.05)\n",
    "    print(\"initial spareness: %s\" % sess.run(spareness))\n",
    "\n",
    "    # re-initialize\n",
    "    initialize_uninitialized(sess)\n",
    "\n",
    "    # reset due to adding a new task\n",
    "    patience_counter = 0\n",
    "    current_best_val_accu = 0\n",
    "\n",
    "    # optimize when the aggregated obj\n",
    "    while(patience_counter < early_stop_patience and epoch_counter < epoch):\n",
    "\n",
    "        def load_batches():\n",
    "            for i in range(int(Xtrain.shape[0]/batch_size)):\n",
    "                st = i*batch_size\n",
    "                ed = (i+1)*batch_size\n",
    "                batch = ia.Batch(images=Xtrain[st:ed,:,:,:], data=Ytrain[st:ed,:])\n",
    "                yield batch\n",
    "\n",
    "        batch_loader = ia.BatchLoader(load_batches)\n",
    "        bg_augmenter = ia.BackgroundAugmenter(batch_loader=batch_loader, augseq=transform, nb_workers=8)\n",
    "\n",
    "        # start training\n",
    "        stime = time.time()\n",
    "        bar_train = Bar('Training', max=int(Xtrain.shape[0]/batch_size), suffix='%(index)d/%(max)d - %(percent).1f%% - %(eta)ds')\n",
    "        bar_val =  Bar('Validation', max=int(Xvalid.shape[0]/batch_size), suffix='%(index)d/%(max)d - %(percent).1f%% - %(eta)ds')\n",
    "        train_loss, train_accu = 0.0, 0.0\n",
    "        while True:\n",
    "            batch = bg_augmenter.get_batch()\n",
    "            if batch is None:\n",
    "                print(\"Finished epoch.\")\n",
    "                break\n",
    "            x_images_aug = batch.images_aug\n",
    "            y_images = batch.data\n",
    "            loss, accu, _ = sess.run([obj, model.accu_dict[cur_task], train_op], feed_dict={model.x: x_images_aug,\n",
    "                            model.y: y_images,\n",
    "                            model.is_train: True})\n",
    "            bar_train.next()\n",
    "            train_loss += loss\n",
    "            train_accu += accu\n",
    "            ptrain.value +=1\n",
    "            ptrain.description = \"Training %s/%s\" % (ptrain.value, ptrain.max)\n",
    "        train_loss = train_loss/ptrain.value\n",
    "        train_accu = train_accu/ptrain.value\n",
    "        batch_loader.terminate()\n",
    "        bg_augmenter.terminate()\n",
    "\n",
    "        # # training an epoch\n",
    "        # for i in range(int(Xtrain.shape[0]/batch_size)):\n",
    "        #     st = i*batch_size\n",
    "        #     ed = (i+1)*batch_size\n",
    "\n",
    "        #     augX = transform.augment_images(Xtrain[st:ed,:,:,:])\n",
    "\n",
    "        #     sess.run([train_op], feed_dict={model.x: augX,\n",
    "        #                                     model.y: Ytrain[st:ed,:],\n",
    "        #                                     model.is_train: False})\n",
    "        #     ptrain.value +=1\n",
    "        #     ptrain.description = \"Training %s/%s\" % (i, ptrain.max)\n",
    "        #     bar_train.next()\n",
    "\n",
    "        # validation\n",
    "        val_loss = 0\n",
    "        val_accu = 0\n",
    "        for i in range(int(Xvalid.shape[0]/batch_size)):\n",
    "            st = i*batch_size\n",
    "            ed = (i+1)*batch_size\n",
    "            loss, accu = sess.run([obj, model.accu_dict[cur_task]],\n",
    "                                feed_dict={model.x: Xvalid[st:ed,:],\n",
    "                                            model.y: Yvalid[st:ed,:],\n",
    "                                            model.is_train: False})\n",
    "            val_loss += loss\n",
    "            val_accu += accu\n",
    "            pval.value += 1\n",
    "            pval.description = \"Testing %s/%s\" % (pval.value, pval.value)\n",
    "        val_loss = val_loss/pval.value\n",
    "        val_accu = val_accu/pval.value\n",
    "\n",
    "        print(\"\\nspareness: %s\" % sess.run(spareness))\n",
    "        # early stopping check\n",
    "        if (val_accu - current_best_val_accu) > min_delta:\n",
    "            current_best_val_accu = val_accu\n",
    "            patience_counter = 0\n",
    "\n",
    "            para_dict = sess.run(model.para_dict)\n",
    "            np.save(os.path.join(FLAG_save_dir, \"para_dict.npy\"), para_dict)\n",
    "            print(\"save in %s\" % os.path.join(FLAG_save_dir, \"para_dict.npy\"))\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        # shuffle Xtrain and Ytrain in the next epoch\n",
    "        idx = np.random.permutation(Xtrain.shape[0])\n",
    "        Xtrain, Ytrain = Xtrain[idx,:,:,:], Ytrain[idx,:]\n",
    "\n",
    "        # epoch end\n",
    "        # writer.add_summary(epoch_summary, epoch_counter)\n",
    "        epoch_counter += 1\n",
    "\n",
    "        ptrain.value = 0\n",
    "        pval.value = 0\n",
    "        bar_train.finish()\n",
    "        bar_val.finish()\n",
    "\n",
    "        print(\"Epoch %s (%s), %s sec >> train loss: %.4f, train accu: %.4f, val loss: %.4f, val accu at %s: %.4f\" % (epoch_counter, patience_counter, round(time.time()-stime,2), train_loss, train_accu, val_loss, cur_task, val_accu))\n",
    "        history.append([train_loss, train_accu, val_loss, val_accu ])\n",
    "        \n",
    "        if epoch_counter % 10 == 0:\n",
    "            import matplotlib.pyplot as plt\n",
    "            df = pd.DataFrame(history)\n",
    "            df.columns = ['train_loss', 'train_accu', 'val_loss', 'val_accu']\n",
    "            df[['train_loss', 'val_loss']].plot()\n",
    "            plt.savefig(os.path.join(FLAG_save_dir, 'loss.png'))\n",
    "            df[['train_accu', 'val_accu']].plot()\n",
    "            plt.savefig(os.path.join(FLAG_save_dir, 'accu.png'))\n",
    "            \n",
    "    saver.save(sess, checkpoint_path, global_step=epoch_counter)\n",
    "\n",
    "    #sp, rcut = gammaSparsifyVGG16(para_dict, thresh=0.02)\n",
    "    #np.save(os.path.join(FLAG_save_dir,\"sparse_dict.npy\"), sp)\n",
    "    #print(\"sparsify %s in %s\" % (np.round(1-rcut,3), os.path.join(FLAG_save_dir, \"sparse_dict.npy\")))\n",
    "\n",
    "    #writer.close()\n",
    "    #arr_spareness.append(1-rcut)\n",
    "    #np.save(os.path.join(FLAG_save_dir,\"sprocess.npy\"), arr_spareness)\n",
    "# FLAG_optimizer = opt_type\n",
    "# FLAG_lr = start_learning_rate\n",
    "# FLAG_batch_size = batch_size\n",
    "# FLAG_epoch_end = epoch_counter\n",
    "# FLAG_val_accu = current_best_val_accu\n",
    "\n",
    "# header = ''\n",
    "# row = ''\n",
    "# for key in sorted(vars(FLAG)):\n",
    "#     if header is '':\n",
    "#         header = key\n",
    "#         row = str(getattr(FLAG, key))\n",
    "#     else:\n",
    "#         header += \",\"+key\n",
    "#         row += \",\"+str(getattr(FLAG,key))\n",
    "# row += \"\\n\"\n",
    "# header += \"\\n\"\n",
    "# if os.path.exists(\"/home/cmchang/new_CP_CNN/model.csv\"):\n",
    "#     with open(\"/home/cmchang/new_CP_CNN/model.csv\", \"a\") as myfile:\n",
    "#         myfile.write(row)\n",
    "# else:\n",
    "#     with open(\"/home/cmchang/new_CP_CNN/model.csv\", \"w\") as myfile:\n",
    "#         myfile.write(header)\n",
    "#         myfile.write(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
