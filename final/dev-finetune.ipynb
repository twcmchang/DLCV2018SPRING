{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# %load model.py\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='1'\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "VGG_MEAN = [103.939, 116.779, 123.68] # [B, G, R]\n",
    "class VGG16:\n",
    "    def __init__(self, scope_name=\"VGG16\"):\n",
    "        \"\"\"\n",
    "        load pre-trained weights from path\n",
    "        :param vgg16_npy_path: file path of vgg16 pre-trained weights\n",
    "        \"\"\"\n",
    "        \n",
    "        self.scope_name = scope_name\n",
    "        \n",
    "        self.gamma_var = []\n",
    "        self.net_shape = []\n",
    "        \n",
    "        # operation dictionary\n",
    "        self.prob_dict = {}\n",
    "        self.loss_dict = {}\n",
    "        self.accu_dict = {}\n",
    "        self.feature_dict = {}\n",
    "\n",
    "        # parameter dictionary\n",
    "        self.para_dict = {}\n",
    "\n",
    "    def build(self, vgg16_npy_path, classes=10, shape=(32,32,3), prof_type=None, conv_pre_training=True, fc_pre_training=True):\n",
    "        \"\"\"\n",
    "        load variable from npy to build the VGG\n",
    "        :param rgb: rgb image [batch, height, width, 3] values scaled [0, 1]\n",
    "        \"\"\"\n",
    "        \n",
    "        # input information\n",
    "        self.H, self.W, self.C = shape\n",
    "        self.classes = classes\n",
    "        \n",
    "        start_time = time.time()\n",
    "        print(\"build model started\")\n",
    "\n",
    "        if prof_type is None:\n",
    "            self.prof_type = \"all-one\"\n",
    "        else:\n",
    "            self.prof_type = prof_type\n",
    "\n",
    "        # load pre-trained weights\n",
    "        if isinstance(vgg16_npy_path,dict):\n",
    "            self.data_dict = vgg16_npy_path\n",
    "            print(\"parameters loaded\")\n",
    "        else:\n",
    "            self.data_dict = np.load(vgg16_npy_path, encoding='latin1').item()\n",
    "            print(\"npy file loaded\")\n",
    "\n",
    "        # input placeholder\n",
    "        self.x = tf.placeholder(tf.float32, [None, self.H, self.W, self.C])\n",
    "        self.y = tf.placeholder(tf.float32, [None, self.classes])\n",
    "        self.is_train = tf.placeholder(tf.bool)\n",
    "        \n",
    "        self.x = tf.placeholder(tf.float32, [None, self.H, self.W, self.C])\n",
    "        self.is_train = tf.placeholder(tf.bool)\n",
    "        \n",
    "        # Convert RGB to BGR\n",
    "        red, green, blue = tf.split(axis=3, num_or_size_splits=3, value=self.x)\n",
    "\n",
    "        self.x = tf.concat(axis=3, values=[\n",
    "             blue - VGG_MEAN[0],\n",
    "            green - VGG_MEAN[1],\n",
    "              red - VGG_MEAN[2],\n",
    "        ])\n",
    "        assert self.x.get_shape().as_list()[1:] == [self.H, self.W, self.C]\n",
    "\n",
    "        dp={\n",
    "            'conv1_1':1.00,\n",
    "            'conv1_2':1.00,\n",
    "            'conv2_1':1.00,\n",
    "            'conv2_2':1.00,\n",
    "            'conv3_1':1.00,\n",
    "            'conv3_2':1.00,\n",
    "            'conv3_3':1.00,\n",
    "            'conv4_1':1.00,\n",
    "            'conv4_2':1.00,\n",
    "            'conv4_3':1.00,\n",
    "            'conv5_1':1.00,\n",
    "            'conv5_2':1.00,\n",
    "            'conv5_3':1.00\n",
    "            }\n",
    "\n",
    "        # declare and initialize the weights of VGG16\n",
    "        with tf.variable_scope(self.scope_name):\n",
    "            # weight decay\n",
    "            self._weight_decay = 0.0\n",
    "            for k, v in sorted(dp.items()):\n",
    "                (conv_filter, gamma, beta, bn_mean, bn_variance), conv_bias = self.get_conv_filter(name=k, new_bn=True), self.get_bias(name=k)\n",
    "                self.para_dict[k] = [conv_filter, conv_bias]\n",
    "                self.para_dict[k+\"_gamma\"] = gamma\n",
    "                self.para_dict[k+\"_beta\"] = beta\n",
    "                self.para_dict[k+\"_bn_mean\"] = bn_mean\n",
    "                self.para_dict[k+\"_bn_variance\"] = bn_variance\n",
    "                self.gamma_var.append(self.para_dict[k+\"_gamma\"])\n",
    "\n",
    "                # weight decay\n",
    "                self._weight_decay += tf.nn.l2_loss(conv_filter)+tf.nn.l2_loss(conv_bias)\n",
    "\n",
    "            if fc_pre_training:\n",
    "                fc_W, fc_b = self.get_fc_layer(name='fc_2'), self.get_bias(name='fc_2')\n",
    "                self.para_dict['fc_2'] = [fc_W, fc_b]\n",
    "\n",
    "                self._weight_decay += tf.nn.l2_loss(fc_W) + tf.nn.l2_loss(fc_b)\n",
    "            else:\n",
    "                # the last fully connected layers should be trained\n",
    "                # user specified fully connected layers\n",
    "                fc_W = self.get_fc_layer(name='fc_2', shape=(21504, self.classes))\n",
    "                fc_b = self.get_bias(name='fc_2', shape=(self.classes,))\n",
    "                self.para_dict['fc_2'] = [fc_W, fc_b]\n",
    "\n",
    "                self._weight_decay += tf.nn.l2_loss(fc_W) + tf.nn.l2_loss(fc_b)\n",
    "\n",
    "        print((\"build model finished: %ds\" % (time.time() - start_time)))\n",
    "\n",
    "    def sparsity_train(self, l1_gamma=0.001, l1_gamma_diff=0.001, decay=0.0005, keep_prob=0.0):\n",
    "        \n",
    "        self._keep_prob = keep_prob\n",
    "        start_time = time.time()\n",
    "        with tf.name_scope(\"var_dp\"):\n",
    "            conv1_1 = self.idp_conv_bn_layer( self.x, \"conv1_1\")\n",
    "            conv1_2 = self.idp_conv_bn_layer(conv1_1, \"conv1_2\")\n",
    "            pool1 = self.max_pool(conv1_2, 'pool1')\n",
    "\n",
    "            conv2_1 = self.idp_conv_bn_layer(  pool1, \"conv2_1\")\n",
    "            conv2_2 = self.idp_conv_bn_layer(conv2_1, \"conv2_2\")\n",
    "            pool2 = self.max_pool(conv2_2, 'pool2')\n",
    "\n",
    "            conv3_1 = self.idp_conv_bn_layer(  pool2, \"conv3_1\")\n",
    "            conv3_2 = self.idp_conv_bn_layer(conv3_1, \"conv3_2\")\n",
    "            conv3_3 = self.idp_conv_bn_layer(conv3_2, \"conv3_3\")\n",
    "            pool3 = self.max_pool(conv3_3, 'pool3')\n",
    "\n",
    "            conv4_1 = self.idp_conv_bn_layer(  pool3, \"conv4_1\")\n",
    "            conv4_2 = self.idp_conv_bn_layer(conv4_1, \"conv4_2\")\n",
    "            conv4_3 = self.idp_conv_bn_layer(conv4_2, \"conv4_3\")\n",
    "            pool4   = self.max_pool(conv4_3, 'pool4')\n",
    "\n",
    "            conv5_1 = self.idp_conv_bn_layer(  pool4, \"conv5_1\")\n",
    "            conv5_2 = self.idp_conv_bn_layer(conv5_1, \"conv5_2\")\n",
    "            conv5_3 = self.idp_conv_bn_layer(conv5_2, \"conv5_3\")\n",
    "            pool5 = self.max_pool(conv5_3, 'pool5')\n",
    "\n",
    "            pool5 = self.dropout_layer(pool5, keep_prob=self._keep_prob)\n",
    "\n",
    "            logits = self.fc_layer(pool5, 'fc_2')       \n",
    "            prob = tf.nn.softmax(logits, name=\"prob\")\n",
    "            \n",
    "            cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=self.y)\n",
    "            loss = tf.reduce_mean(cross_entropy)\n",
    "            accuracy = tf.reduce_mean(tf.cast(tf.equal(x=tf.argmax(logits, 1), y=tf.argmax(self.y, 1)),tf.float32))\n",
    "            \n",
    "            # gamma l1 regularization\n",
    "            l1_gamma_regularizer = tf.contrib.layers.l1_regularizer(scale=l1_gamma)\n",
    "            gamma_l1 = tf.contrib.layers.apply_regularization(l1_gamma_regularizer, self.gamma_var)\n",
    "\n",
    "            # gamma_diff l1 regularization\n",
    "            def non_increasing_constraint_axis_0(a):\n",
    "                return tf.nn.relu(a[1:]-a[:-1])\n",
    "            gamma_diff_var = [non_increasing_constraint_axis_0(x) for x in self.gamma_var]\n",
    "\n",
    "            l1_gamma_diff_regularizer = tf.contrib.layers.l1_regularizer(scale=l1_gamma_diff)\n",
    "            gamma_diff_l1 = tf.contrib.layers.apply_regularization(l1_gamma_diff_regularizer, gamma_diff_var)\n",
    "            \n",
    "            self.prob_dict[\"var_dp\"] = prob\n",
    "            self.loss_dict[\"var_dp\"] = loss + gamma_l1 + gamma_diff_l1 + self._weight_decay * decay\n",
    "            self.accu_dict[\"var_dp\"] = accuracy\n",
    "            \n",
    "            tf.summary.scalar(name=\"accu_var_dp\", tensor=accuracy)\n",
    "            tf.summary.scalar(name=\"loss_var_dp\", tensor=loss)\n",
    "        self.summary_op = tf.summary.merge_all()        \n",
    "        print((\"sparsity train operation setup: %ds\" % (time.time() - start_time)))\n",
    "    \n",
    "    def set_idp_operation(self, dp, decay=0.0002, keep_prob=1.0):\n",
    "        self._keep_prob = keep_prob\n",
    "        if type(dp) != list:\n",
    "            raise ValueError(\"when block_variational is False, dp must be a list.\")\n",
    "        self.dp = dp \n",
    "        print(\"DP under test:\", np.round(self.dp,2))\n",
    "        start_time = time.time()\n",
    "        # create operations at every dot product percentages\n",
    "        for dp_i in dp:\n",
    "            with tf.name_scope(str(int(dp_i*100))):\n",
    "                conv1_1 = self.idp_conv_bn_layer( self.x, \"conv1_1\", dp_i)\n",
    "                conv1_2 = self.idp_conv_bn_layer(conv1_1, \"conv1_2\", dp_i)\n",
    "                pool1 = self.max_pool(conv1_2, 'pool1')\n",
    "\n",
    "                if dp_i == 1.0:\n",
    "                    self.net_shape.append(conv1_1.get_shape())\n",
    "                    self.net_shape.append(pool1.get_shape())\n",
    "\n",
    "                conv2_1 = self.idp_conv_bn_layer(  pool1, \"conv2_1\", dp_i)\n",
    "                conv2_2 = self.idp_conv_bn_layer(conv2_1, \"conv2_2\", dp_i)\n",
    "                pool2 = self.max_pool(conv2_2, 'pool2')\n",
    "\n",
    "                if dp_i == 1.0:\n",
    "                    self.net_shape.append(conv2_1.get_shape())\n",
    "                    self.net_shape.append(pool2.get_shape())\n",
    "\n",
    "                conv3_1 = self.idp_conv_bn_layer(  pool2, \"conv3_1\", dp_i)\n",
    "                conv3_2 = self.idp_conv_bn_layer(conv3_1, \"conv3_2\", dp_i)\n",
    "                conv3_3 = self.idp_conv_bn_layer(conv3_2, \"conv3_3\", dp_i)\n",
    "                pool3 = self.max_pool(conv3_3, 'pool3')\n",
    "\n",
    "                if dp_i == 1.0:\n",
    "                    self.net_shape.append(conv3_1.get_shape())\n",
    "                    self.net_shape.append(conv3_2.get_shape())\n",
    "                    self.net_shape.append(pool3.get_shape())\n",
    "\n",
    "                conv4_1 = self.idp_conv_bn_layer(  pool3, \"conv4_1\", dp_i)\n",
    "                conv4_2 = self.idp_conv_bn_layer(conv4_1, \"conv4_2\", dp_i)\n",
    "                conv4_3 = self.idp_conv_bn_layer(conv4_2, \"conv4_3\", dp_i)\n",
    "                pool4 = self.max_pool(conv4_3, 'pool4')\n",
    "                \n",
    "                if dp_i == 1.0:\n",
    "                    self.net_shape.append(conv4_1.get_shape())\n",
    "                    self.net_shape.append(conv4_2.get_shape())\n",
    "                    self.net_shape.append(pool4.get_shape())\n",
    "\n",
    "                conv5_1 = self.idp_conv_bn_layer(  pool4, \"conv5_1\", dp_i)\n",
    "                conv5_2 = self.idp_conv_bn_layer(conv5_1, \"conv5_2\", dp_i)\n",
    "                conv5_3 = self.idp_conv_bn_layer(conv5_2, \"conv5_3\", dp_i)\n",
    "                pool5 = self.max_pool(conv5_3, 'pool5')\n",
    "\n",
    "                if dp_i == 1.0:\n",
    "                    self.net_shape.append(conv5_1.get_shape())\n",
    "                    self.net_shape.append(conv5_2.get_shape())\n",
    "                    self.net_shape.append(pool5.get_shape())\n",
    "\n",
    "#                 fc_1 = self.fc_layer(pool5, 'fc_1')\n",
    "#                 fc_1 = self.dropout_layer(fc_1, keep_prob=self._keep_prob)\n",
    "#                 fc_1 = tf.nn.relu(fc_1)\n",
    "                \n",
    "                logits = self.fc_layer(pool5, 'fc_2')\n",
    "                prob = tf.nn.softmax(logits, name=\"prob\")\n",
    "\n",
    "                cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=self.y)\n",
    "                loss = tf.reduce_mean(cross_entropy)\n",
    "                accuracy = tf.reduce_mean(tf.cast(tf.equal(x=tf.argmax(logits, 1), y=tf.argmax(self.y, 1)), dtype=tf.float32))\n",
    "\n",
    "                # self.feature_dict[str(int(dp_i*100))] = fc_1\n",
    "                self.prob_dict[str(int(dp_i*100))] = prob\n",
    "                self.loss_dict[str(int(dp_i*100))] = loss + self._weight_decay * decay\n",
    "                self.accu_dict[str(int(dp_i*100))] = accuracy\n",
    "\n",
    "                tf.summary.scalar(name=\"accu_at_\"+str(int(dp_i*100)), tensor=accuracy)\n",
    "                tf.summary.scalar(name=\"loss_at_\"+str(int(dp_i*100)), tensor=loss)\n",
    "        self.summary_op = tf.summary.merge_all()\n",
    "        print((\"Set dp operations finished: %ds\" % (time.time() - start_time)))\n",
    "\n",
    "    def spareness(self, thresh=0.05):\n",
    "        N_active, N_total = 0,0\n",
    "        for gamma in self.gamma_var:\n",
    "            m = tf.cast(tf.less(tf.abs(gamma), thresh), tf.float32)\n",
    "            n_active = tf.reduce_sum(m)\n",
    "            n_total  = tf.cast(tf.reduce_prod(tf.shape(m)), tf.float32)\n",
    "            N_active += n_active\n",
    "            N_total  += n_total\n",
    "        return N_active/N_total\n",
    "\n",
    "    def avg_pool(self, bottom, name):\n",
    "        return tf.nn.avg_pool(bottom, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name=name)\n",
    "\n",
    "    def max_pool(self, bottom, name):\n",
    "        return tf.nn.max_pool(bottom, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name=name)\n",
    "    \n",
    "    def dropout_layer(self, bottom, keep_prob):\n",
    "        if self.is_train == True:\n",
    "            return tf.nn.dropout(bottom, keep_prob=keep_prob)\n",
    "        else:\n",
    "            return bottom\n",
    "\n",
    "    def idp_conv_bn_layer(self, bottom, name, dp=1.0):\n",
    "        with tf.name_scope(name+str(int(dp*100))):\n",
    "            with tf.variable_scope(self.scope_name,reuse=True):\n",
    "                conv_filter = tf.get_variable(name=name+\"_W\")\n",
    "                conv_biases = tf.get_variable(name=name+\"_b\")\n",
    "                conv_gamma  = tf.get_variable(name=name+\"_gamma\")\n",
    "                moving_mean = tf.get_variable(name=name+'_bn_mean')\n",
    "                moving_variance = tf.get_variable(name=name+'_bn_variance')\n",
    "                beta = tf.get_variable(name=name+'_beta')\n",
    "            H,W,C,O = conv_filter.get_shape().as_list()\n",
    "            print(bottom.get_shape().as_list()) \n",
    "            if name is not 'conv1_1':\n",
    "                bottom = bottom[:,:,:,:int(C*dp)]\n",
    "                print(\"AFTER\",bottom.get_shape().as_list())\n",
    "                conv_filter = conv_filter[:,:,:int(C*dp),:]\n",
    "\n",
    "            # create a mask determined by the dot product percentage\n",
    "            n1 = int(O * dp)\n",
    "            n0 = O - n1\n",
    "            mask = tf.constant(value=np.append(np.ones(n1, dtype='float32'), np.zeros(n0, dtype='float32')), dtype=tf.float32)\n",
    "            conv_gamma = tf.multiply(conv_gamma, mask)\n",
    "            beta = tf.multiply(beta, mask)\n",
    "            \n",
    "            conv = tf.nn.conv2d(bottom, conv_filter, [1, 1, 1, 1], padding='SAME')\n",
    "            conv = tf.nn.bias_add(conv, conv_biases)\n",
    "\n",
    "            from tensorflow.python.training.moving_averages import assign_moving_average\n",
    "            def mean_var_with_update():\n",
    "                mean, variance = tf.nn.moments(conv, [0,1,2], name='moments')\n",
    "                with tf.control_dependencies([assign_moving_average(moving_mean, mean, 0.99),\n",
    "                                              assign_moving_average(moving_variance, variance, 0.99)]):\n",
    "                    return tf.identity(mean), tf.identity(variance)\n",
    "\n",
    "            mean, variance = tf.cond(self.is_train, mean_var_with_update, lambda:(moving_mean, moving_variance))\n",
    "\n",
    "            conv = tf.nn.batch_normalization(conv, mean, variance, beta, conv_gamma, 1e-05)\n",
    "            relu = tf.nn.relu(conv)\n",
    "            \n",
    "            return relu\n",
    "\n",
    "    def fc_layer(self, bottom, name):\n",
    "        with tf.name_scope(name):\n",
    "            shape = bottom.get_shape().as_list()\n",
    "            dim = 1\n",
    "            for d in shape[1:]:\n",
    "                dim *= d\n",
    "            x = tf.reshape(bottom, [-1, dim])\n",
    "            \n",
    "            with tf.variable_scope(self.scope_name,reuse=True):\n",
    "                weights = tf.get_variable(name=name+\"_W\")\n",
    "                biases = tf.get_variable(name=name+\"_b\")\n",
    "\n",
    "            # Fully connected layer. Note that the '+' operation automatically broadcasts the biases.\n",
    "            fc = tf.nn.bias_add(tf.matmul(x, weights), biases)\n",
    "            return fc\n",
    "\n",
    "    def get_conv_filter(self, name, new_bn=False, shape=None):\n",
    "        if shape is not None:\n",
    "            conv_filter = tf.get_variable(shape=shape, initializer=tf.truncated_normal_initializer(mean=0, stddev=0.1), name=name+\"_W\", dtype=tf.float32)\n",
    "        elif name in self.data_dict.keys():\n",
    "            conv_filter = tf.get_variable(initializer=self.data_dict[name][0], name=name+\"_W\")\n",
    "        else:\n",
    "            print(\"please specify a name in data_dict or specify a shape in use\")\n",
    "\n",
    "        H,W,C,O = conv_filter.get_shape().as_list()\n",
    "\n",
    "        if name+\"_gamma\" in self.data_dict.keys() and not new_bn: \n",
    "            gamma = tf.get_variable(initializer=self.data_dict[name+\"_gamma\"], name=name+\"_gamma\")\n",
    "        else:\n",
    "            gamma = tf.get_variable(initializer=self.get_profile(O, self.prof_type), name=name+\"_gamma\")\n",
    "\n",
    "        if name+\"_beta\" in self.data_dict.keys() and not new_bn:\n",
    "            beta = tf.get_variable(initializer=self.data_dict[name+\"_beta\"], name=name+\"_beta\")\n",
    "        else:\n",
    "            beta = tf.get_variable(shape=(O,), initializer=tf.zeros_initializer(), name=name+'_beta')\n",
    "\n",
    "        if name+\"_bn_mean\" in self.data_dict.keys() and not new_bn:\n",
    "            bn_mean = tf.get_variable(initializer=self.data_dict[name+\"_bn_mean\"], name=name+\"_bn_mean\")\n",
    "        else:\n",
    "            bn_mean = tf.get_variable(shape=(O,), initializer=tf.zeros_initializer(), name=name+'_bn_mean')\n",
    "\n",
    "        if name+\"_bn_variance\" in self.data_dict.keys() and not new_bn: \n",
    "            bn_variance = tf.get_variable(initializer=self.data_dict[name+\"_bn_variance\"], name=name+\"_bn_variance\")\n",
    "        else:\n",
    "            bn_variance = tf.get_variable(shape=(O,),initializer=tf.ones_initializer(), name=name+'_bn_variance')\n",
    "        \n",
    "        return conv_filter, gamma, beta, bn_mean, bn_variance\n",
    "\n",
    "    def get_fc_layer(self, name, shape=None):\n",
    "        if shape is not None:\n",
    "            return tf.get_variable(shape=shape, initializer=tf.truncated_normal_initializer(mean=0, stddev=0.1), name=name+\"_W\", dtype=tf.float32)\n",
    "        elif name in self.data_dict.keys():\n",
    "            return tf.get_variable(initializer=self.data_dict[name][0], name=name+\"_W\", dtype=tf.float32)\n",
    "        else:\n",
    "            print(\"please specify a name in data_dict or specify a shape in use\")\n",
    "            return None\n",
    "            \n",
    "    def get_bias(self, name, shape=None):\n",
    "        if shape is not None:\n",
    "            return tf.get_variable(shape=shape, initializer=tf.truncated_normal_initializer(mean=0, stddev=0.1), name=name+\"_b\", dtype=tf.float32)\n",
    "        elif name in self.data_dict.keys():\n",
    "            return tf.get_variable(initializer=self.data_dict[name][1], name=name+\"_b\", dtype=tf.float32)\n",
    "        else:\n",
    "            print(\"please specify a name in data_dict or specify a shape in use\")\n",
    "            return None\n",
    "\n",
    "    def get_profile(self, C, prof_type):\n",
    "        def half_exp(n, k=1, dtype='float32'):\n",
    "            n_ones = int(n/2)\n",
    "            n_other = n - n_ones\n",
    "            return np.append(np.ones(n_ones, dtype=dtype), np.exp((1-k)*np.arange(n_other), dtype=dtype))\n",
    "        if prof_type == \"linear\":\n",
    "            profile = np.linspace(2.0,0.0, num=C, endpoint=False, dtype='float32')\n",
    "        elif prof_type == \"all-one\":\n",
    "            profile = np.ones(C, dtype='float32')\n",
    "        elif prof_type == \"half-exp\":\n",
    "            profile = half_exp(C, 2.0)\n",
    "        elif prof_type == \"harmonic\":\n",
    "            profile = np.array(1.0/(np.arange(C)+1), dtype='float32')\n",
    "        else:\n",
    "            raise ValueError(\"prof_type must be \\\"all-one\\\", \\\"half-exp\\\", \\\"harmonic\\\" or \\\"linear\\\".\")\n",
    "        return profile\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import skimage.io as imageio\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from progress.bar import Bar\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('save/label_dict.pkl', 'rb') as f:\n",
    "    y_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HOME_DIR = \"/data/put_data/cmchang/DLCV_final/\"\n",
    "TRAIN_DIR = HOME_DIR+\"dlcv_final_2_dataset/train/\"\n",
    "VALID_DIR = HOME_DIR+\"dlcv_final_2_dataset/val/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = pd.read_csv(HOME_DIR+\"dlcv_final_2_dataset/train_id.txt\", header=None,sep=\" \", names=[\"img\", \"id\"])\n",
    "dvalid = pd.read_csv(HOME_DIR+\"dlcv_final_2_dataset/val_id.txt\", header=None,sep=\" \", names=[\"img\", \"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_list = list(TRAIN_DIR+dtrain.img)\n",
    "valid_list = list(VALID_DIR+dvalid.img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readImgList(file_list):\n",
    "    images = list()\n",
    "    for i, file in enumerate(file_list):\n",
    "        print(i, end=\"\\r\")\n",
    "        img = imageio.imread(file)\n",
    "        img = img.astype(int)\n",
    "        images.append(img)\n",
    "    return np.array(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transformLabel(id_list, y_dict):\n",
    "    label = list()\n",
    "    for uid in list(id_list):\n",
    "        label.append(y_dict[uid])\n",
    "    return np.array(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_encoding(class_numbers, num_classes):\n",
    "    return np.eye(num_classes, dtype=float)[class_numbers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_uninitialized(sess):\n",
    "    global_vars = tf.global_variables()\n",
    "    is_not_initialized = sess.run([tf.is_variable_initialized(var) for var in global_vars])\n",
    "    not_initialized_vars = [v for (v,f) in zip(global_vars, is_not_initialized) if not f]\n",
    "    if len(not_initialized_vars): \n",
    "            sess.run(tf.variables_initializer(not_initialized_vars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56474\r"
     ]
    }
   ],
   "source": [
    "Xtrain = readImgList(train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7210\r"
     ]
    }
   ],
   "source": [
    "Xvalid = readImgList(valid_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56475, 218, 178, 3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ytrain = transformLabel(list(dtrain.id), y_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yvalid = transformLabel(list(dvalid.id), y_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Ytrain = one_hot_encoding(ytrain, len(y_dict))\n",
    "Yvalid = one_hot_encoding(yvalid, len(y_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scope_name = \"Model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = VGG16(scope_name=scope_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FLAG_init_from = \"save_all-one_2/para_dict.npy\"\n",
    "FLAG_prof_type = \"linear\"\n",
    "FLAG_lambda_s = 2e-3\n",
    "FLAG_lambda_m = 1e-3\n",
    "FLAG_decay = 1e-5\n",
    "FLAG_lr = 2e-4\n",
    "FLAG_keep_prob = 1.0\n",
    "FLAG_save_dir = \"finetune_v3_from_save_all-one_2/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build model started\n",
      "npy file loaded\n",
      "build model finished: 2s\n"
     ]
    }
   ],
   "source": [
    "model.build(vgg16_npy_path=FLAG_init_from,\n",
    "            shape=Xtrain.shape[1:],\n",
    "            classes=len(y_dict),\n",
    "            prof_type=FLAG_prof_type,\n",
    "            conv_pre_training=True,\n",
    "            fc_pre_training=True,\n",
    "            new_bn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 218, 178, 3]\n",
      "[None, 218, 178, 64]\n",
      "AFTER [None, 218, 178, 64]\n",
      "[None, 109, 89, 64]\n",
      "AFTER [None, 109, 89, 64]\n",
      "[None, 109, 89, 128]\n",
      "AFTER [None, 109, 89, 128]\n",
      "[None, 55, 45, 128]\n",
      "AFTER [None, 55, 45, 128]\n",
      "[None, 55, 45, 256]\n",
      "AFTER [None, 55, 45, 256]\n",
      "[None, 55, 45, 256]\n",
      "AFTER [None, 55, 45, 256]\n",
      "[None, 28, 23, 256]\n",
      "AFTER [None, 28, 23, 256]\n",
      "[None, 28, 23, 512]\n",
      "AFTER [None, 28, 23, 512]\n",
      "[None, 28, 23, 512]\n",
      "AFTER [None, 28, 23, 512]\n",
      "[None, 14, 12, 512]\n",
      "AFTER [None, 14, 12, 512]\n",
      "[None, 14, 12, 512]\n",
      "AFTER [None, 14, 12, 512]\n",
      "[None, 14, 12, 512]\n",
      "AFTER [None, 14, 12, 512]\n",
      "sparsity train operation setup: 1s\n"
     ]
    }
   ],
   "source": [
    "model.sparsity_train(l1_gamma=FLAG_lambda_s, l1_gamma_diff=FLAG_lambda_m, decay=FLAG_decay, keep_prob=FLAG_keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import imgaug as ia\n",
    "from imgaug import augmenters as iaa\n",
    "sometimes = lambda aug: iaa.Sometimes(0.5, aug)\n",
    "transform = iaa.Sequential([\n",
    "    sometimes(iaa.Affine(translate_percent={\"x\": (-0.15, 0.15), \"y\": (-0.15, 0.15)})),\n",
    "    sometimes(iaa.Affine(scale={\"x\": (0.85, 1.15), \"y\":(0.85, 1.15)})),\n",
    "    sometimes(iaa.Affine(rotate=(-45, 45))),\n",
    "    sometimes(iaa.Fliplr(0.5))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== create directory =====\n",
      "['var_dp']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2df76bc4cd6b49c4997b17d4e5fd2cb5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11eda6a047004a54848d80566f838a1f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial spareness: 0.023200758\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.02935606\n",
      "save in finetune_v3_from_save_all-one_2/para_dict.npy\n",
      "Epoch 1 (0), 1757.09 sec >> train loss: 11.3799, train accu: 0.6780, val loss: 15.7223, val accu at var_dp: 0.2031\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.03787879\n",
      "save in finetune_v3_from_save_all-one_2/para_dict.npy\n",
      "Epoch 2 (0), 1482.95 sec >> train loss: 10.7967, train accu: 0.7067, val loss: 12.6855, val accu at var_dp: 0.4844\n",
      "Finished epoch.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 247, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 398, in _send_bytes\n",
      "    self._send(buf)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "spareness: 0.049952652\n",
      "Epoch 3 (1), 2525.36 sec >> train loss: 10.2411, train accu: 0.7557, val loss: 12.1805, val accu at var_dp: 0.4674\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.062973484\n",
      "Epoch 4 (2), 3072.88 sec >> train loss: 9.7490, train accu: 0.7835, val loss: 12.2740, val accu at var_dp: 0.4413\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.07694129\n",
      "save in finetune_v3_from_save_all-one_2/para_dict.npy\n",
      "Epoch 5 (0), 1504.04 sec >> train loss: 9.2607, train accu: 0.8095, val loss: 11.3590, val accu at var_dp: 0.5038\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.09019887\n",
      "Epoch 6 (1), 2233.04 sec >> train loss: 8.8277, train accu: 0.8223, val loss: 11.1932, val accu at var_dp: 0.4803\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.10440341\n",
      "save in finetune_v3_from_save_all-one_2/para_dict.npy\n",
      "Epoch 7 (0), 2102.07 sec >> train loss: 8.3726, train accu: 0.8439, val loss: 10.6124, val accu at var_dp: 0.5067\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.11813447\n",
      "save in finetune_v3_from_save_all-one_2/para_dict.npy\n",
      "Epoch 8 (0), 2362.74 sec >> train loss: 7.9752, train accu: 0.8528, val loss: 9.7692, val accu at var_dp: 0.5845\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.13068181\n",
      "Epoch 9 (1), 2274.01 sec >> train loss: 7.5703, train accu: 0.8674, val loss: 10.0258, val accu at var_dp: 0.5255\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.14133523\n",
      "save in finetune_v3_from_save_all-one_2/para_dict.npy\n",
      "Epoch 10 (0), 2592.06 sec >> train loss: 7.2003, train accu: 0.8760, val loss: 8.9185, val accu at var_dp: 0.6097\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.15198864\n",
      "Epoch 11 (1), 2309.3 sec >> train loss: 6.8160, train accu: 0.8898, val loss: 8.8149, val accu at var_dp: 0.5965\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.17092803\n",
      "save in finetune_v3_from_save_all-one_2/para_dict.npy\n",
      "Epoch 12 (0), 2135.79 sec >> train loss: 6.4705, train accu: 0.8989, val loss: 8.3677, val accu at var_dp: 0.6137\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.1796875\n",
      "save in finetune_v3_from_save_all-one_2/para_dict.npy\n",
      "Epoch 13 (0), 1841.46 sec >> train loss: 6.1535, train accu: 0.9011, val loss: 7.9912, val accu at var_dp: 0.6165\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.19436553\n",
      "Epoch 14 (1), 2136.84 sec >> train loss: 5.8322, train accu: 0.9106, val loss: 7.8322, val accu at var_dp: 0.6097\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.20785984\n",
      "Epoch 15 (2), 2095.59 sec >> train loss: 5.5334, train accu: 0.9151, val loss: 7.4762, val accu at var_dp: 0.6157\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.21732955\n",
      "save in finetune_v3_from_save_all-one_2/para_dict.npy\n",
      "Epoch 16 (0), 1941.07 sec >> train loss: 5.2638, train accu: 0.9174, val loss: 7.1828, val accu at var_dp: 0.6360\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.22916667\n",
      "save in finetune_v3_from_save_all-one_2/para_dict.npy\n",
      "Epoch 17 (0), 2151.54 sec >> train loss: 4.9984, train accu: 0.9213, val loss: 6.7269, val accu at var_dp: 0.6571\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.23887311\n",
      "save in finetune_v3_from_save_all-one_2/para_dict.npy\n",
      "Epoch 18 (0), 2028.57 sec >> train loss: 4.7385, train accu: 0.9286, val loss: 6.6075, val accu at var_dp: 0.6581\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.2504735\n",
      "save in finetune_v3_from_save_all-one_2/para_dict.npy\n",
      "Epoch 19 (0), 2087.06 sec >> train loss: 4.4866, train accu: 0.9343, val loss: 6.3074, val accu at var_dp: 0.6727\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.2606534\n",
      "Epoch 20 (1), 2046.4 sec >> train loss: 4.2646, train accu: 0.9354, val loss: 5.9984, val accu at var_dp: 0.6723\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.27201703\n",
      "Epoch 21 (2), 2102.06 sec >> train loss: 4.0409, train accu: 0.9421, val loss: 5.9111, val accu at var_dp: 0.6616\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.27864584\n",
      "save in finetune_v3_from_save_all-one_2/para_dict.npy\n",
      "Epoch 22 (0), 2115.17 sec >> train loss: 3.8608, train accu: 0.9365, val loss: 5.5863, val accu at var_dp: 0.6765\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.28787878\n",
      "Epoch 23 (1), 2022.99 sec >> train loss: 3.6489, train accu: 0.9450, val loss: 5.5227, val accu at var_dp: 0.6561\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.2985322\n",
      "save in finetune_v3_from_save_all-one_2/para_dict.npy\n",
      "Epoch 24 (0), 1607.56 sec >> train loss: 3.4745, train accu: 0.9459, val loss: 5.1674, val accu at var_dp: 0.6797\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.31273675\n",
      "Epoch 25 (1), 2154.49 sec >> train loss: 3.2937, train accu: 0.9497, val loss: 5.2541, val accu at var_dp: 0.6484\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.32291666\n",
      "Epoch 26 (2), 1983.6 sec >> train loss: 3.1343, train accu: 0.9489, val loss: 4.9185, val accu at var_dp: 0.6765\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.33214962\n",
      "Epoch 27 (3), 1836.23 sec >> train loss: 2.9776, train accu: 0.9515, val loss: 5.2261, val accu at var_dp: 0.6250\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.34232953\n",
      "Epoch 28 (4), 2272.2 sec >> train loss: 2.8293, train accu: 0.9542, val loss: 4.5901, val accu at var_dp: 0.6663\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.35629734\n",
      "Epoch 29 (5), 1939.62 sec >> train loss: 2.6853, train accu: 0.9566, val loss: 4.4332, val accu at var_dp: 0.6754\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.3650568\n",
      "Epoch 30 (6), 1936.51 sec >> train loss: 2.5429, train accu: 0.9620, val loss: 4.2464, val accu at var_dp: 0.6775\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.37381628\n",
      "Epoch 31 (7), 2218.72 sec >> train loss: 2.4275, train accu: 0.9605, val loss: 4.1845, val accu at var_dp: 0.6629\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.38636363\n",
      "Epoch 32 (8), 1922.93 sec >> train loss: 2.3186, train accu: 0.9588, val loss: 4.1092, val accu at var_dp: 0.6677\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.39607006\n",
      "save in finetune_v3_from_save_all-one_2/para_dict.npy\n",
      "Epoch 33 (0), 2055.05 sec >> train loss: 2.2011, train accu: 0.9628, val loss: 3.6516, val accu at var_dp: 0.7161\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.40980113\n",
      "Epoch 34 (1), 2188.35 sec >> train loss: 2.0960, train accu: 0.9643, val loss: 3.5468, val accu at var_dp: 0.7144\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.41832387\n",
      "save in finetune_v3_from_save_all-one_2/para_dict.npy\n",
      "Epoch 35 (0), 2126.05 sec >> train loss: 1.9922, train accu: 0.9675, val loss: 3.2913, val accu at var_dp: 0.7303\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.42566288\n",
      "Epoch 36 (1), 1937.13 sec >> train loss: 1.9041, train accu: 0.9673, val loss: 3.2630, val accu at var_dp: 0.7140\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.4346591\n",
      "Epoch 37 (2), 2176.54 sec >> train loss: 1.8170, train accu: 0.9677, val loss: 3.2308, val accu at var_dp: 0.7109\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.44602272\n",
      "Epoch 38 (3), 2012.12 sec >> train loss: 1.7254, train accu: 0.9719, val loss: 3.1840, val accu at var_dp: 0.7069\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.45572916\n",
      "Epoch 39 (4), 2129.28 sec >> train loss: 1.6571, train accu: 0.9700, val loss: 3.0243, val accu at var_dp: 0.7090\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.46496212\n",
      "Epoch 40 (5), 2221.49 sec >> train loss: 1.5790, train accu: 0.9717, val loss: 3.0530, val accu at var_dp: 0.6949\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.4744318\n",
      "save in finetune_v3_from_save_all-one_2/para_dict.npy\n",
      "Epoch 41 (0), 2174.56 sec >> train loss: 1.5110, train accu: 0.9731, val loss: 2.7600, val accu at var_dp: 0.7395\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.48366478\n",
      "Epoch 42 (1), 2194.7 sec >> train loss: 1.4452, train accu: 0.9748, val loss: 2.6913, val accu at var_dp: 0.7386\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.49455494\n",
      "save in finetune_v3_from_save_all-one_2/para_dict.npy\n",
      "Epoch 43 (0), 2240.39 sec >> train loss: 1.3894, train accu: 0.9754, val loss: 2.5982, val accu at var_dp: 0.7464\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.50284094\n",
      "Epoch 44 (1), 2188.18 sec >> train loss: 1.3349, train accu: 0.9742, val loss: 2.6434, val accu at var_dp: 0.7232\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.51420456\n",
      "Epoch 45 (2), 2190.21 sec >> train loss: 1.2888, train accu: 0.9745, val loss: 2.5685, val accu at var_dp: 0.7394\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.5286458\n",
      "Epoch 46 (3), 2280.88 sec >> train loss: 1.2413, train accu: 0.9754, val loss: 2.4549, val accu at var_dp: 0.7462\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.5428504\n",
      "save in finetune_v3_from_save_all-one_2/para_dict.npy\n",
      "Epoch 47 (0), 2210.19 sec >> train loss: 1.1893, train accu: 0.9777, val loss: 2.3872, val accu at var_dp: 0.7524\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.5537405\n",
      "Epoch 48 (1), 1259.73 sec >> train loss: 1.1551, train accu: 0.9761, val loss: 2.5498, val accu at var_dp: 0.7132\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.5629735\n",
      "Epoch 49 (2), 2191.69 sec >> train loss: 1.1167, train accu: 0.9771, val loss: 2.3086, val accu at var_dp: 0.7468\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.5745739\n",
      "Epoch 50 (3), 2179.3 sec >> train loss: 1.0762, train accu: 0.9785, val loss: 2.3065, val accu at var_dp: 0.7434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch.\n",
      "\n",
      "spareness: 0.5814394\n",
      "Epoch 51 (4), 1649.86 sec >> train loss: 1.0426, train accu: 0.9789, val loss: 2.3643, val accu at var_dp: 0.7309\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.59138256\n",
      "Epoch 52 (5), 2164.83 sec >> train loss: 1.0046, train accu: 0.9805, val loss: 2.2546, val accu at var_dp: 0.7444\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.5999053\n",
      "save in finetune_v3_from_save_all-one_2/para_dict.npy\n",
      "Epoch 53 (0), 2101.18 sec >> train loss: 0.9814, train accu: 0.9787, val loss: 2.1854, val accu at var_dp: 0.7525\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.6098485\n",
      "Epoch 54 (1), 1868.99 sec >> train loss: 0.9506, train accu: 0.9805, val loss: 2.2443, val accu at var_dp: 0.7363\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.6216856\n",
      "Epoch 55 (2), 2241.12 sec >> train loss: 0.9266, train accu: 0.9796, val loss: 2.1385, val accu at var_dp: 0.7482\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.63352275\n",
      "Epoch 56 (3), 2087.54 sec >> train loss: 0.9024, train accu: 0.9800, val loss: 2.1358, val accu at var_dp: 0.7489\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.648911\n",
      "Epoch 57 (4), 2119.89 sec >> train loss: 0.8777, train accu: 0.9810, val loss: 2.2557, val accu at var_dp: 0.7263\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.66169506\n",
      "save in finetune_v3_from_save_all-one_2/para_dict.npy\n",
      "Epoch 58 (0), 2215.08 sec >> train loss: 0.8577, train accu: 0.9805, val loss: 2.0466, val accu at var_dp: 0.7553\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.67732006\n",
      "save in finetune_v3_from_save_all-one_2/para_dict.npy\n",
      "Epoch 59 (0), 2161.81 sec >> train loss: 0.8350, train accu: 0.9817, val loss: 1.9765, val accu at var_dp: 0.7674\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.6910511\n",
      "Epoch 60 (1), 2041.63 sec >> train loss: 0.8165, train accu: 0.9809, val loss: 2.0185, val accu at var_dp: 0.7499\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.70549244\n",
      "Epoch 61 (2), 2233.79 sec >> train loss: 0.8001, train accu: 0.9811, val loss: 1.9887, val accu at var_dp: 0.7593\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.7232481\n",
      "Epoch 62 (3), 2149.04 sec >> train loss: 0.7852, train accu: 0.9807, val loss: 2.0523, val accu at var_dp: 0.7418\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.742661\n",
      "Epoch 63 (4), 2133.58 sec >> train loss: 0.7675, train accu: 0.9814, val loss: 1.8699, val accu at var_dp: 0.7667\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.75970644\n",
      "Epoch 64 (5), 2245.4 sec >> train loss: 0.7480, train accu: 0.9826, val loss: 1.9961, val accu at var_dp: 0.7471\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.7765151\n",
      "Epoch 65 (6), 1856.97 sec >> train loss: 0.7357, train accu: 0.9819, val loss: 1.8783, val accu at var_dp: 0.7603\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.7869318\n",
      "save in finetune_v3_from_save_all-one_2/para_dict.npy\n",
      "Epoch 66 (0), 1883.37 sec >> train loss: 0.7202, train accu: 0.9831, val loss: 1.8804, val accu at var_dp: 0.7688\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.79758525\n",
      "Epoch 67 (1), 2196.68 sec >> train loss: 0.7029, train accu: 0.9841, val loss: 1.9559, val accu at var_dp: 0.7427\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.8084754\n",
      "Epoch 68 (2), 1900.35 sec >> train loss: 0.6981, train accu: 0.9811, val loss: 1.9451, val accu at var_dp: 0.7414\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.81841856\n",
      "Epoch 69 (3), 2001.44 sec >> train loss: 0.6762, train accu: 0.9844, val loss: 1.8417, val accu at var_dp: 0.7613\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.8307292\n",
      "Epoch 70 (4), 2206.37 sec >> train loss: 0.6709, train accu: 0.9822, val loss: 1.8114, val accu at var_dp: 0.7666\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.8392519\n",
      "Epoch 71 (5), 1950.3 sec >> train loss: 0.6621, train accu: 0.9824, val loss: 1.8712, val accu at var_dp: 0.7575\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.8454072\n",
      "save in finetune_v3_from_save_all-one_2/para_dict.npy\n",
      "Epoch 72 (0), 2054.21 sec >> train loss: 0.6498, train accu: 0.9841, val loss: 1.7556, val accu at var_dp: 0.7716\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.85345644\n",
      "Epoch 73 (1), 2192.7 sec >> train loss: 0.6327, train accu: 0.9851, val loss: 1.9265, val accu at var_dp: 0.7419\n",
      "Finished epoch.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/multiprocessing/queues.py\", line 247, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 398, in _send_bytes\n",
      "    self._send(buf)\n",
      "  File \"/usr/lib/python3.5/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "spareness: 0.86174244\n",
      "Epoch 74 (2), 1985.76 sec >> train loss: 0.6276, train accu: 0.9845, val loss: 1.9210, val accu at var_dp: 0.7374\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.8655303\n",
      "Epoch 75 (3), 2161.05 sec >> train loss: 0.6271, train accu: 0.9819, val loss: 1.7908, val accu at var_dp: 0.7649\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.8733428\n",
      "Epoch 76 (4), 2223.72 sec >> train loss: 0.6077, train accu: 0.9854, val loss: 1.7762, val accu at var_dp: 0.7645\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.87736744\n",
      "Epoch 77 (5), 2077.18 sec >> train loss: 0.6046, train accu: 0.9842, val loss: 1.8439, val accu at var_dp: 0.7503\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.883286\n",
      "Epoch 78 (6), 2052.37 sec >> train loss: 0.5968, train accu: 0.9852, val loss: 1.8432, val accu at var_dp: 0.7644\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.88825756\n",
      "Epoch 79 (7), 2255.95 sec >> train loss: 0.5899, train accu: 0.9841, val loss: 2.0743, val accu at var_dp: 0.7125\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.89346594\n",
      "Epoch 80 (8), 2243.58 sec >> train loss: 0.5861, train accu: 0.9840, val loss: 1.8601, val accu at var_dp: 0.7448\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.8984375\n",
      "Epoch 81 (9), 2274.88 sec >> train loss: 0.5745, train accu: 0.9858, val loss: 1.9671, val accu at var_dp: 0.7342\n",
      "Finished epoch.\n",
      "\n",
      "spareness: 0.90127844\n",
      "Epoch 82 (10), 2368.34 sec >> train loss: 0.5707, train accu: 0.9842, val loss: 2.0427, val accu at var_dp: 0.7130\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'gammaSparsifyVGG16' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-6edf4a6033bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch_counter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m     \u001b[0msp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcut\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgammaSparsifyVGG16\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpara_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.02\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFLAG_save_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"sparse_dict.npy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sparsify %s in %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mrcut\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFLAG_save_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sparse_dict.npy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gammaSparsifyVGG16' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"===== create directory =====\")\n",
    "if not os.path.exists(FLAG_save_dir):\n",
    "    os.makedirs(FLAG_save_dir)\n",
    "\n",
    "arr_spareness = []\n",
    "\n",
    "# define tasks\n",
    "tasks = ['var_dp']\n",
    "print(tasks)\n",
    "\n",
    "# initial task\n",
    "cur_task = tasks[0]\n",
    "obj = model.loss_dict[tasks[0]]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "   # hyper parameters\n",
    "    batch_size = 64\n",
    "    epoch = 100\n",
    "    early_stop_patience = 10\n",
    "    min_delta = 0.0001\n",
    "    opt_type = 'adam'\n",
    "\n",
    "    # recorder\n",
    "    epoch_counter = 0\n",
    "    history = list()\n",
    "\n",
    "    # optimizer\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "    # Passing global_step to minimize() will increment it at each step.\n",
    "    if opt_type is 'sgd':\n",
    "        learning_rate = FLAG_lr # adam # 4e-3 #sgd\n",
    "        #half_cycle = 20000\n",
    "        #learning_rate = tf.train.exponential_decay(learning_rate, global_step, half_cycle, 0.5, staircase=True)\n",
    "        opt = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9, use_nesterov=True)\n",
    "    else:\n",
    "        learning_rate = FLAG_lr # adam # 4e-3 #sgd\n",
    "        #half_cycle = 10000\n",
    "        #learning_rate = tf.train.exponential_decay(learning_rate, global_step, half_cycle, 0.5, staircase=True)\n",
    "        opt = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.5)\n",
    "\n",
    "    checkpoint_path = os.path.join(FLAG_save_dir, 'model.ckpt')\n",
    "    tvars_trainable = tf.trainable_variables()\n",
    "\n",
    "    train_vars = list()\n",
    "    for var in tf.trainable_variables():\n",
    "        if model.scope_name in var.name:\n",
    "            train_vars.append(var)\n",
    "            \n",
    "    train_op = opt.minimize(obj, global_step=global_step, var_list=tvars_trainable)\n",
    "    \n",
    "    saver = tf.train.Saver(tf.global_variables(), max_to_keep=len(tasks))\n",
    "\n",
    "    # progress bar\n",
    "    ptrain = IntProgress()\n",
    "    pval = IntProgress()\n",
    "    display(ptrain)\n",
    "    display(pval)\n",
    "    ptrain.max = int(Xtrain.shape[0]/batch_size)\n",
    "    pval.max = int(Xvalid.shape[0]/batch_size)\n",
    "\n",
    "    spareness = model.spareness(thresh=0.05)\n",
    "    print(\"initial spareness: %s\" % sess.run(spareness))\n",
    "\n",
    "    # re-initialize\n",
    "    initialize_uninitialized(sess)\n",
    "\n",
    "    # reset due to adding a new task\n",
    "    patience_counter = 0\n",
    "    current_best_val_accu = 0\n",
    "\n",
    "    # optimize when the aggregated obj\n",
    "    while(patience_counter < early_stop_patience and epoch_counter < epoch):\n",
    "\n",
    "        def load_batches():\n",
    "            for i in range(int(Xtrain.shape[0]/batch_size)):\n",
    "                st = i*batch_size\n",
    "                ed = (i+1)*batch_size\n",
    "                batch = ia.Batch(images=Xtrain[st:ed,:,:,:], data=Ytrain[st:ed,:])\n",
    "                yield batch\n",
    "\n",
    "        batch_loader = ia.BatchLoader(load_batches)\n",
    "        bg_augmenter = ia.BackgroundAugmenter(batch_loader=batch_loader, augseq=transform, nb_workers=8)\n",
    "\n",
    "        # start training\n",
    "        stime = time.time()\n",
    "        bar_train = Bar('Training', max=int(Xtrain.shape[0]/batch_size), suffix='%(index)d/%(max)d - %(percent).1f%% - %(eta)ds')\n",
    "        bar_val =  Bar('Validation', max=int(Xvalid.shape[0]/batch_size), suffix='%(index)d/%(max)d - %(percent).1f%% - %(eta)ds')\n",
    "        train_loss, train_accu = 0.0, 0.0\n",
    "        while True:\n",
    "            batch = bg_augmenter.get_batch()\n",
    "            if batch is None:\n",
    "                print(\"Finished epoch.\")\n",
    "                break\n",
    "            x_images_aug = batch.images_aug\n",
    "            y_images = batch.data\n",
    "            loss, accu, _ = sess.run([obj, model.accu_dict[cur_task], train_op], feed_dict={model.x: x_images_aug,\n",
    "                            model.y: y_images,\n",
    "                            model.is_train: True})\n",
    "            bar_train.next()\n",
    "            train_loss += loss\n",
    "            train_accu += accu\n",
    "            ptrain.value +=1\n",
    "            ptrain.description = \"Training %s/%s\" % (ptrain.value, ptrain.max)\n",
    "        train_loss = train_loss/ptrain.value\n",
    "        train_accu = train_accu/ptrain.value\n",
    "        batch_loader.terminate()\n",
    "        bg_augmenter.terminate()\n",
    "\n",
    "        # validation\n",
    "        val_loss = 0\n",
    "        val_accu = 0\n",
    "        for i in range(int(Xvalid.shape[0]/batch_size)):\n",
    "            st = i*batch_size\n",
    "            ed = (i+1)*batch_size\n",
    "            loss, accu = sess.run([obj, model.accu_dict[cur_task]],\n",
    "                                feed_dict={model.x: Xvalid[st:ed,:],\n",
    "                                            model.y: Yvalid[st:ed,:],\n",
    "                                            model.is_train: False})\n",
    "            val_loss += loss\n",
    "            val_accu += accu\n",
    "            pval.value += 1\n",
    "            pval.description = \"Testing %s/%s\" % (pval.value, pval.value)\n",
    "        val_loss = val_loss/pval.value\n",
    "        val_accu = val_accu/pval.value\n",
    "\n",
    "        print(\"\\nspareness: %s\" % sess.run(spareness))\n",
    "        # early stopping check\n",
    "        if (val_accu - current_best_val_accu) > min_delta:\n",
    "            current_best_val_accu = val_accu\n",
    "            patience_counter = 0\n",
    "\n",
    "            para_dict = sess.run(model.para_dict)\n",
    "            np.save(os.path.join(FLAG_save_dir, \"para_dict.npy\"), para_dict)\n",
    "            print(\"save in %s\" % os.path.join(FLAG_save_dir, \"para_dict.npy\"))\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        # shuffle Xtrain and Ytrain in the next epoch\n",
    "        idx = np.random.permutation(Xtrain.shape[0])\n",
    "        Xtrain, Ytrain = Xtrain[idx,:,:,:], Ytrain[idx,:]\n",
    "\n",
    "        # epoch end\n",
    "        # writer.add_summary(epoch_summary, epoch_counter)\n",
    "        epoch_counter += 1\n",
    "\n",
    "        ptrain.value = 0\n",
    "        pval.value = 0\n",
    "        bar_train.finish()\n",
    "        bar_val.finish()\n",
    "\n",
    "        print(\"Epoch %s (%s), %s sec >> train loss: %.4f, train accu: %.4f, val loss: %.4f, val accu at %s: %.4f\" % (epoch_counter, patience_counter, round(time.time()-stime,2), train_loss, train_accu, val_loss, cur_task, val_accu))\n",
    "        history.append([train_loss, train_accu, val_loss, val_accu ])\n",
    "        \n",
    "        if epoch_counter % 10 == 0:\n",
    "            import matplotlib.pyplot as plt\n",
    "            df = pd.DataFrame(history)\n",
    "            df.columns = ['train_loss', 'train_accu', 'val_loss', 'val_accu']\n",
    "            df[['train_loss', 'val_loss']].plot()\n",
    "            plt.savefig(os.path.join(FLAG_save_dir, 'loss.png'))\n",
    "            df[['train_accu', 'val_accu']].plot()\n",
    "            plt.savefig(os.path.join(FLAG_save_dir, 'accu.png'))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
